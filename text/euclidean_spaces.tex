\subsection{Euclidean spaces}\label{subsec:euclidean_spaces}

\begin{definition}\label{def:euclidean_space}\mimprovised
  A \term{Euclidean space} of dimension \( n \) is the \hyperref[def:affine_space]{affine space \( \BbbR^n \)} equipped with the \hyperref[def:inner_product_space]{dot product} \( \inprod x y \coloneqq x^T y \), sometimes called the \term{Euclidean inner product}. We call \( \BbbR^2 \) the \term{Euclidean plane}.

  As described in \fullref{rem:structure_hierarchy}, this introduces a standard \hyperref[def:norm]{norm}, \hyperref[def:metric_space]{metric}, \hyperref[def:uniform_space]{uniformity} and \hyperref[def:topological_space]{topology}, which we call the \( n \)-dimensional \term{Euclidean norm} (resp. metric, uniformity or topology).

  Consider the \hyperref[def:sequence_space]{standard basis} \( e_1, \ldots, e_n \). We call the \hyperref[def:geometric_ray]{ray} at the origin with direction \( e_k \) the \( k \)-th \term{coordinate axis}.

  It is important to note that the meaning of the term \enquote{Euclidean space} varies in the literature --- see \fullref{rem:euclidean_space_etymology}.
\end{definition}

\begin{remark}\label{rem:euclidean_space_etymology}
  The term \enquote{Euclidean space} may have different meanings depending on the author. For example, \cite[sec. 24.1]{Тыртышников2004Лекции} defines Euclidean spaces as possibly infinite-dimensional real inner product spaces, while \cite[2.19]{Rudin1987RealAndComplex} restricts them to coordinate spaces with the dot product, as we do. Gallier in \cite[176]{Gallier2011}, on the other hand, even makes a distinction between Euclidean vector spaces and Euclidean affine spaces, and defines the former as \hyperref[def:inner_product_space]{real inner product spaces}.

  \enquote{The} Euclidean space may also refer to the ambient space used by Euclid in \cite{Fitzpatrick2008}.
\end{remark}

\begin{proposition}\label{thm:isometry_iff_affine_orthogonal_operator}\mcite[thm. 7.1]{Treil2017}
  The endofunction \( f: \BbbR^n \to \BbbR^n \) over the real inner product space \( \BbbR^n \) is an \hyperref[def:isometry]{isometry} if and only if it is an \hyperref[def:affine_operator]{affine operator} whose linear part \( T(x) \coloneqq f(x) - f(\vect 0) \) is \hyperref[def:unitary_operator]{orthogonal}.
\end{proposition}
\begin{proof}
  \SufficiencySubProof Suppose that \( f \) is an isometry.

  First note that
  \begin{equation*}
    \inprod { f(x) } { f(y) } = \inprod x y.
  \end{equation*}

  Indeed, we have
  \begin{equation*}
    \norm{ f(x) - f(y) }^2 = \norm{ f(x) }^2 + \norm{ f(y) }^2 - 2 \inprod { f(x) } { f(y) }
  \end{equation*}
  and
  \begin{equation*}
    \norm{ x - y }^2 = \norm{x}^2 + \norm{y}^2 - 2 \inprod x y.
  \end{equation*}

  Therefore,
  \begin{equation*}
    \inprod { f(x) } { f(y) }
    =
    \frac{ \norm{ f(x) - f(y) }^2 - \norm{ f(x) }^2 - \norm{ f(y) }^2 } 2
    =
    \frac{ \norm{ x - y }^2 - \norm{ x }^2 - \norm{ y }^2 } 2
    =
    \inprod x y.
  \end{equation*}

  Now we can show additivity of \( T \):
  \begin{balign*}
    &\phantom{{}={}}
    \norm{ T(x + y) - T(x) - T(y) }^2
    = \\ &=
    \norm{ f(x + y) - f(0) - f(x) + f(0) - f(y) + f(0) }^2
    = \\ &=
    \norm{ \parens{ f(x + y) - f(x) } - \parens{ f(y) - f(0) } }^2
    = \\ &=
    \norm{ f(x + y) - f(x) }^2 + \norm{ f(y) - f(0) }^2 - 2 \inprod{ f(x + y) - f(x) } { f(y) - f(0) }
    = \\ &=
    \norm{ x + y - x }^2 + \norm{y}^2 - 2 \inprod{x + y} y + 2 \inprod x y - 2 \inprod {x + y} 0 + 2 \inprod x 0
    = \\ &=
    2 \norm{y}^2 - 2 \inprod x y - 2 \norm{y}^2 + 2 \inprod x y.
  \end{balign*}

  The norm is zero, hence \( T(x + y) = T(x) + T(y) \).

  Similarly,
  \begin{balign*}
    &\phantom{{}={}}
    \norm{ T(\lambda x) - \lambda T(x) }^2
    = \\ &=
    \norm{ f(\lambda x) - f(0) - \lambda f(x) + \lambda f(0) }^2
    = \\ &=
    \norm{ f(\lambda x) - f(0) }^2 + \norm{ \lambda f(x) - \lambda f(0) }^2 - 2 \inprod{ f(\lambda x) - f(0) } { \lambda f(x) - \lambda f(0) }
    = \\ &=
    \norm{\lambda x}^2 + \lambda^2 \norm{x}^2 - 2 \lambda \inprod{ f(\lambda x) - f(0) } { f(x) - f(0) }
    = \\ &=
    2 \lambda^2 \norm{x}^2 - 2 \lambda^2 \inprod x x.
  \end{balign*}

  This norm is also zero, hence \( T(\lambda x) = \lambda T(x) \).

  Finally, we must show that \( T \) is a unitary operator:
  \begin{balign*}
    \inprod{ T(x) }{ T(y) }
    &=
    \inprod{ f(x) - f(0) }{ f(y) - f(0) }
    = \\ &=
    \inprod{ f(x) }{ f(y) } - \inprod{ f(x) }{ f(0) } - \inprod{ f(0) }{ f(y) } + \inprod{ f(0) }{ f(0) }
    = \\ &=
    \inprod x y.
  \end{balign*}

  \NecessitySubProof Suppose that \( f(x) = Tx + f_0 \) for some unitary operator \( T \) and some vector \( f_0 \). Then
  \begin{balign*}
    \norm{ f(x) - f(y) }^2
    &=
    \norm{ Tx - f_0 - Ty + f_0 }^2
    = \\ &=
    \norm{ T(x - y) }^2
    = \\ &=
    \inprod{ T(x - y) }{ T(x - y) }
    = \\ &=
    \inprod{ x - y }{ T^{-1} T(x - y) }
    = \\ &=
    \norm{ x - y }^2.
  \end{balign*}
\end{proof}

\begin{lemma}\label{thm:dot_product_and_outer_product}
  For any field \( \BbbK \) and vectors \( x \), \( y \) and \( z \) in \( \BbbK^n \), we have
  \begin{equation*}
    x^T y z = y z^T x.
  \end{equation*}
\end{lemma}
\begin{proof}
  The \( k \)-th coordinate of \( x^T y z \) is \( (x^T y) z_k \).

  The \( k \)-th coordinate of \( y z^T x \) is
  \begin{equation*}
    \sum_{i=1}^n (y_i z_k) x_i
    =
    z_k \sum_{i=1}^n y_i x_i
    =
    (x^T y) z_k.
  \end{equation*}
\end{proof}

\begin{definition}\label{def:rigid_motion}\mimprovised
  A \term{rigid motion} is an \hyperref[def:isometry]{isometry} in an \hyperref[def:euclidean_space]{Euclidean space}. \Fullref{thm:isometry_iff_affine_orthogonal_operator} provides an equivalent characterization: a rigid motion is an affine operator whose linear part is \hyperref[def:unitary_operator]{orthogonal}. Thus, given a rigid motion \( f: \BbbR^n \to \BbbR^n \), there exists an orthogonal operator \( T: \BbbR^n \to \BbbR^n \) and a directional vector \( d \) such that
  \begin{equation*}
    f(x) = Tx + d.
  \end{equation*}

  The following are common rigid motions:
  \begin{thmenum}
    \thmitem{def:rigid_motion/translation} The \term{translation} along the \term{direction} \( d \) is
    \begin{equation*}
      f(x) = x + d.
    \end{equation*}

    \begin{figure}[!ht]
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__translation__2d.pdf}
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__translation__3d.pdf}
      \hfill
      \hfill
      \caption{Translation of the unit square in \( \BbbR^2 \) and unit cube in \( \BbbR^3 \).}\label{fig:def:rigid_motion/translation}
    \end{figure}

    The term \enquote{translation} generalizes to an arbitrary \hyperref[def:magma]{magma} via
    \begin{equation*}
      f(x) \coloneqq v \cdot x.
    \end{equation*}

    \thmitem{def:rigid_motion/rotation} A \term{rotation} about the point \( O \) is an affine operator with fixed point \( O \) and an \hyperref[def:unitary_operator]{orthogonal} linear part with \hyperref[def:matrix_determinant]{determinant} \( 1 \).

    \begin{figure}[!ht]
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__rotation__2d.pdf}
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__rotation__3d.pdf}
      \hfill
      \hfill
      \caption{Rotation of the unit square in \( \BbbR^2 \) and unit cube in \( \BbbR^3 \).}\label{fig:def:rigid_motion/rotation}
    \end{figure}

    \thmitem{def:rigid_motion/householder_reflection} The \term{Householder reflection} through the \hyperref[def:geometric_ray]{ray} \( r \) with origin \( O \) and unit directional vector \( v \) is
    \begin{equation*}
      f(x) \coloneqq x - 2 \inprod {x - O} v v.
    \end{equation*}

    The vector \( v \) is interpreted as a \hyperref[def:normal_vector]{normal vector} of the \hyperref[def:affine_hyperplane]{affine hypersurface} \( O + \braket{ v }^\perp \).

    This can be expressed in matrix form as
    \begin{equation*}
      f(x)
      =
      x - (2 (x - O)^T v) \cdot v
      \reloset {\ref{thm:dot_product_and_outer_product}} =
      x - 2 v v^T (x - O)
      =
      2 v v^T O + (I_n - 2 v v^T) x.
    \end{equation*}

    \begin{figure}[!ht]
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__householder_reflection__2d.pdf}
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__householder_reflection__3d.pdf}
      \hfill
      \hfill
      \caption{Householder reflection of the unit square in \( \BbbR^2 \) and unit cube in \( \BbbR^3 \).}\label{fig:def:rigid_motion/householder_reflection}
    \end{figure}

    \thmitem{def:rigid_motion/point_reflection} Similarly, the \term{point reflection} through \( O \) is
    \begin{equation*}
      f(x) \coloneqq x - 2\vect{Ox} = 2O - x.
    \end{equation*}

    \begin{figure}[!ht]
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__point_reflection__2d.pdf}
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__point_reflection__3d.pdf}
      \hfill
      \hfill
      \caption{Point reflection of the unit square in \( \BbbR^2 \) and unit cube in \( \BbbR^3 \).}\label{fig:def:rigid_motion/point_reflection}
    \end{figure}
  \end{thmenum}
\end{definition}

\begin{proposition}\label{thm:lebesgue_measure_invariant_under_rigid_motions}
  The \hyperref[def:lebesgue_measure]{Lebesgue measure} of a set is invariant under \hyperref[def:rigid_motion]{rigid motions}.
\end{proposition}
\begin{proof}
  \todo{Prove.}
\end{proof}

\begin{definition}\label{def:argmin_argmax}
  Given a \hyperref[def:function]{plain function} \( f: A \to B \) from a \hyperref[def:set]{plain set} \( A \) to a \hyperref[def:partially_ordered_set]{partially ordered set} \( (B, \leq) \), if, for a unique value \( a_0 \in A \), we have
  \begin{equation*}
    f(a_0) = \min\set{ f(a) \given a \in A },
  \end{equation*}
  we denote this value \( a_0 \) via
  \begin{equation*}
    \argmin_{a \in A} f(a).
  \end{equation*}

  \hyperref[def:partially_ordered_set/duality]{Duality} the allows us to define
  \begin{equation*}
    \argmax_{a \in A} f(a).
  \end{equation*}
\end{definition}

\begin{definition}\label{def:orthogonal_projection}
  Let \( L \) be an affine subspace of \( \BbbR^n \). Let \( Oe_1 \cdots e_m \) be an orthogonal \hyperref[def:affine_coordinate_system]{affine coordinate system} of \( L \). The \term{orthogonal projection} or \term{shadow} onto \( L \) is the affine operator
  \begin{equation*}
    \begin{aligned}
      &\pi: \BbbR^n \to L, \\
      &\pi(x) \coloneqq O + \sum_{k=1}^m \inprod {\vect{Ox}} {e_k} \cdot e_k.
    \end{aligned}
  \end{equation*}

  The definition does not depend on the choice of coordinate system. \Fullref{thm:def:orthogonal_projection} lists important properties of orthogonal projections.

  \begin{figure}[!ht]
    \hfill
    \includegraphics[align=c]{output/def__orthogonal_projection.pdf}
    \hfill
    \hfill
    \caption{Orthogonal projection of the unit cube in \( \BbbR^3 \) onto an affine plane.}\label{fig:def:orthogonal_projection}
  \end{figure}
\end{definition}
\begin{defproof}
  Let \( O e_1\cdots e_m \) and \( P f_1 \cdots f_m \) be affine coordinate systems in \( L \). Then, for any \( x \) in \( \BbbR^n \),
  \begin{align*}
    \sum_{i=1}^m \inprod {\vect{Px}} {f_i} \cdot f_i
    &=
    \sum_{i=1}^m \inprod {\vect{Px}} {f_i} \cdot \parens*{ \sum_{j=1}^m \inprod {f_i} {e_j} \cdot e_j }
    = \\ &=
    \sum_{j=1}^m \parens*{ \sum_{i=1}^m \inprod {\vect{Px}} {f_i} \inprod {f_i} {e_j} } \cdot e_j
    = \\ &=
    \sum_{j=1}^m \inprod* {\vect{Px}} {\sum_{i=1}^m \inprod {f_i} {e_j} \cdot f_i} \cdot e_j
    = \\ &=
    \sum_{j=1}^m \inprod {\vect{Px}} {e_j} \cdot e_j.
  \end{align*}

  Furthermore,
  \begin{equation*}
    \vect{Px} = \vect{PO} + \vect{Ox}.
  \end{equation*}

  Therefore, since \( \vect{PO} \) is a vector from the direction \( \vect L \),
  \begin{align*}
    P + \sum_{i=1}^m \inprod {\vect{Px}} {f_i} \cdot f_i
    &=
    P + \sum_{j=1}^m \inprod {\vect{Px}} {e_j} \cdot e_j
    = \\ &=
    P + \sum_{j=1}^m \inprod {\vect{PO}} {e_j} \cdot e_j + \sum_{j=1}^m \inprod {\vect{Ox}} {e_j} \cdot e_j
    = \\ &=
    P + \vect{PO} + \sum_{j=1}^m \inprod {\vect{Ox}} {e_j} \cdot e_j.
  \end{align*}
\end{defproof}

\begin{proposition}\label{thm:def:orthogonal_projection}
  The \hyperref[def:orthogonal_projection]{orthogonal projection} \( \pi: \BbbR^n \to L \) has the following basic properties:

  \begin{thmenum}
    \thmitem{thm:def:orthogonal_projection/fixed} Every point in \( L \) is \hyperref[def:fixed_point]{fixed} under \( \pi \).

    \thmitem{thm:def:orthogonal_projection/normal} The vector \( \vect{x,\pi(x)} = \pi(x) - x \) is \hyperref[def:normal_vector]{normal} for \( L \), i.e. orthogonal to every vector in the \hyperref[def:affine_subspace]{direction} \( \vect L \).

    \thmitem{thm:def:orthogonal_projection/distance} The point \( \pi(x) \) is the unique point in \( L \) that minimizes the distance to \( x \). That is, that unique point in \( L \) such that
    \begin{equation*}
      \norm{ x - \pi(x) } = \min_{y \in L} \norm{ x - y }.
    \end{equation*}

    This gives an alternative characterization of \( \pi \) as
    \begin{equation}\label{eq:thm:def:orthogonal_projection/distance/argmin}
      \pi(x) \coloneqq \operatorname*{\hyperref[def:argmin_argmax]{argmin}}_{y \in L} \norm{ x - y }.
    \end{equation}
  \end{thmenum}
\end{proposition}
\begin{proof}
  Let \( O e_1 \cdots e_m \) be an orthonormal affine coordinate system in \( L \) and let \( e_{m+1}, \cdots, e_n \) be an expansion of \( e_1, \ldots, e_m \) to an orthonormal basis of \( \BbbR^n \). Let
  \begin{equation*}
    \pi(x) \coloneqq O + \sum_{k=1}^m \inprod {\vect{Ox}} {e_k} \cdot e_k.
  \end{equation*}

  \SubProofOf{thm:def:orthogonal_projection/fixed} For every point \( x \) in \( L \), \( \pi \) simply gives an expansion along the coordinate system \( O e_1 \cdots e_m \).

  \SubProofOf{thm:def:orthogonal_projection/normal} Let \( x \) be a point in \( L \) and let \( v \) be a vector in \( \vect L \). Then
  \begin{equation*}
    v = \sum_{k=1}^m \inprod v {e_k} \cdot e_k
  \end{equation*}
  and
  \begin{equation*}
    \inprod x v
    =
    \sum_{k=1}^m \inprod v {e_k} \cdot \inprod x {e_k}
    =
    \sum_{k=1}^m \inprod x {e_k} \cdot \inprod v {e_k}
    =
    \inprod {\pi(x)} v.
  \end{equation*}

  Therefore,
  \begin{equation*}
    \inprod {\pi(x) - x} v
    =
    \inprod {\pi(x)} v - \inprod x v
    =
    0.
  \end{equation*}

  \SubProofOf{thm:def:orthogonal_projection/distance} From \fullref{thm:def:orthogonal_projection/normal} it follows that, for \( y \in L \),
  \begin{equation*}
    \norm{ x - y }^2
    =
    \norm{ x - \pi(x) + \pi(x) - y }^2
    \reloset {\ref{thm:pythagoras_theorem}} =
    \norm{ x - \pi(x) }^2 + \norm{ \pi(x) - y }^2.
  \end{equation*}

  Since norms are nonnegative, the minimum along \( y \in L \) is obtained if \( \pi(x) = y \).
\end{proof}

\begin{definition}\label{def:distance_to_subspace}
  We define the \term{distance} \( \op{dist}(x, L) \) between a point \( x \) and an \hyperref[def:affine_subspace]{affine subspace} \( A \) as \( \norm{ x - \pi_L(x) } \).
\end{definition}

\begin{definition}\label{def:equidistant_points}
  We say that a set of points \( A \) is \enquote{equidistant} from \( x \) if every point \( y \) in \( A \) has equal distance \( \norm{y - x} \) from \( x \).
\end{definition}

\begin{proposition}\label{thm:segment_midpoint}
  Let \( x \) and \( y \) be distinct points. The convex combination \( \ifrac {x + y} 2 \) is the unique point of the \hyperref[def:line_segment]{segment} \( [x, y] \) that is \hyperref[def:equidistant_points]{equidistant} from both \( x \) and \( y \). We call it the \term{midpoint} of the segment.
\end{proposition}
\begin{proof}
  Let \( z = \lambda x + (1 - \lambda) y \) be a convex combination that is equidistant from \( x \) and from \( y \).

  Then
  \begin{equation*}
    \norm{ z - x }
    =
    \norm{ \lambda x + (1 - \lambda) y - x }
    =
    \norm{ (\lambda - 1) x + (1 - \lambda) y }
    =
    (1 - \lambda) \norm{ y - x }.
  \end{equation*}

  Analogously,
  \begin{equation*}
    \norm{ z - y }
    =
    \norm{ \lambda x + (1 - \lambda) y - y }
    =
    \norm{ \lambda x - \lambda y }
    =
    \lambda \norm{ x - y }.
  \end{equation*}

  From the equidistance assumption,
  \begin{equation*}
    (1 - \lambda) \norm{ y - x } = \lambda \norm{ x - y }.
  \end{equation*}

  Since the points \( x \) and \( y \) are distinct, \( \norm{ y - x } \neq 0 \), hence we can cancel it to obtain
  \begin{equation*}
    1 - \lambda = \lambda,
  \end{equation*}
  implying that \( \lambda = \ifrac 1 2 \).
\end{proof}
