\section{Polynomial factorization}\label{sec:polynomial_factorization}

\paragraph{Homogeneous polynomials}

\begin{definition}\label{def:homogeneous_polynomial}\mcite[139]{Jacobson1985AlgebraPart1}
  We say that a \hyperref[def:polynomial_algebra]{polynomial} is \term[bg=хомогенен (полином) (\cite[58]{ГеновМиховскиМоллов1991Алгебра}), ru=однородный (многочлен) (\cite[\S 145]{Фихтенгольц1968ОсновыТом1})]{homogeneous} of degree \( d \) all of its monomials have \hyperref[def:polynomial_degree]{total degree} \( d \).
\end{definition}

\begin{proposition}\label{thm:homogeneous_polynomial_iff_homogeneous_function}
  Fix a \hyperref[def:semiring]{(semi)ring} \( R \), an arbitrary \hyperref[def:algebra_over_semiring]{\( R \)-algebra} \( M \) and a \hyperref[def:polynomial_algebra]{polynomial algebra} \( R[\mscrX] \).

  If a \( p \in R[\mscrX] \) is \hyperref[def:homogeneous_polynomial]{homogeneous} of degree \( d \), then the \hyperref[con:evaluation_homomorphism]{evaluation} \( \Phi_M(p) \) is a \hyperref[def:homogenous_function]{homogeneous function} of degree \( d \). The converse holds if \( R \) is an \hyperref[def:integral_domain]{integral domain}.
\end{proposition}
\begin{proof}
  \SufficiencySubProof Let \( p \in R[\mscrX] \) be a homogeneous polynomial of degree \( d \) over \( \mscrX \). Fix a variable assignment \( e \) and a scalar \( r \).

  Let
  \begin{equation*}
    p(\mscrX) = \sum_{X_1 \ldots X_m} a_{X_1 \ldots X_m} X_1 \ldots X_m.
  \end{equation*}

  Then, by definition of \( \Phi_M \),
  \begin{equation*}
    \Phi_M(p)(X \mapsto r \cdot e(X))
    =
    \sum_{X_1 \ldots X_d} a_{X_1 \ldots X_d} \parens[\Big]{ r \cdot e(X_1) } \ldots \parens[\Big]{ r \cdot e(X_d) }
    =
    r^d \cdot \Phi_M(p)(e).
  \end{equation*}

  Therefore, \( \Phi_M \) is homogeneous of degree \( d \).

  \NecessitySubProof Suppose that \( R \) is an integral domain. Let \( p \in R[\mscrX] \) and suppose that the function \( \Phi_M(p) \) is homogeneous of degree \( d \).

  Then, for a nonzero scalar \( r \),
  \begin{equation*}
    \Phi_M(p)(X \mapsto r \cdot e(X))
    =
    r^d \cdot \sum_{X_1 \ldots X_m} r^{m - d} \cdot a_{X_1 \ldots X_m} \cdot X_1 \cdots X_m.
  \end{equation*}

  Since \( R \) is entire, \( r^{m - d} \cdot a_{X_1 \ldots X_m} \) is nonzero if the coefficient is nonzero. Then \( \Phi_M(p)(X \mapsto r \cdot e(X)) \) does not equal \( r^d \cdot \Phi_M(p)(e) \) unless \( m = d \) for every monomial \( X_1 \cdots X_m \) with nonzero coefficients. Since we have assumed that equality holds, \( p \) must be a homogeneous polynomial of degree \( d \).
\end{proof}

\begin{proposition}\label{thm:degree_of_multivariate_polynomial_product}
  The \hyperref[def:polynomial_degree]{total degree} of the product of polynomials over an \hyperref[def:integral_domain]{integral domain} is the sum of their total degrees.
\end{proposition}
\begin{proof}
  Follows from \fullref{thm:polynomial_degree_arithmetic/product} by induction on the number of variables.
\end{proof}

\begin{proposition}\label{thm:divisors_of_homogeneous_polynomial}
  The divisors of a \hyperref[def:homogeneous_polynomial]{homogeneous polynomial} of are homogeneous.
\end{proposition}
\begin{proof}
  Let \( p(X_1, \ldots, X_n) \) be a homogeneous polynomial and let
  \begin{equation*}
    P(X_1, \ldots, X_n) = f(X_1, \ldots, X_n) \cdot g(X_1, \ldots, X_n).
  \end{equation*}

  Let \( d^- \) and \( d^+ \) be the minimal and maximal total degree of monomials in \( f \), and let \( e^- \) and \( e^+ \) be the corresponding degrees in \( g \).

  \Fullref{thm:degree_of_multivariate_polynomial_product} applied to individual terms implies that \( p \) has a monomial of degree \( d^- + e^- \) and a monomial of degree \( d^+ + e^+ \). By homogeneity,
  \begin{equation*}
    d = d^- + e^- = d^+ + e^+,
  \end{equation*}
  thus
  \begin{equation*}
    d^+ - d^- = - (e^+ - e^-).
  \end{equation*}

  Since both sides are nonnegative, we conclude that they are both \( 0 \). Then \( f \) is homogeneous of degree \( d^+ \) and \( g \) is homogeneous of degree \( e^+ \).

  This concludes the proof.
\end{proof}

\begin{proposition}\label{thm:homogeneous_polynomial_constant}
  In an \hyperref[def:integral_domain]{integral domain} \( D \), the \hyperref[def:homogeneous_polynomial]{homogeneous} polynomial \( p(X_1, \ldots, X_n, Y) \) is \hyperref[def:domain_divisibility/irreducible]{irreducible} in \( D[X_1, \ldots, X_n, Y] \) if and only if \( p(X_1, \ldots, X_n, 1) \) is irreducible in \( D[X_1, \ldots, X_n] \).
\end{proposition}
\begin{proof}
  Suppose that \( p(X_1, \ldots, X_n, Y) \) is a homogeneous polynomial of degree \( d \) over the domain \( D \). Define
  \begin{equation*}
    q(X_1, \ldots, X_n) \coloneqq p(X_1, \ldots, X_n, 1).
  \end{equation*}

  The latter is a polynomial in \( D[X_1, \ldots, X_n] \) and a scalar in \( D[X_1, \ldots, X_n][Y] \). \Fullref{thm:def:domain_divisibility/irreducible_in_polynomial_ring} thus implies that \( q \) is irreducible in \( D[X_1, \ldots, X_n, Y] \) if and only if it is irreducible in \( D[X_1, \ldots, X_n] \), hence it is sufficient to consider irreducibility in \( D[X_1, \ldots, X_n, Y] \).

  \SufficiencySubProof Suppose that \( p \) is irreducible. We will show that \( q \) also is.

  Suppose that
  \begin{equation*}
    q(X_1, \ldots, X_n) = f(X_1, \ldots, X_n) \cdot g(X_1, \ldots, X_n).
  \end{equation*}

  Consider the field of algebraic functions
  \begin{equation*}
    D(X_1, \ldots, X_n, Y).
  \end{equation*}

  \Fullref{thm:homogeneous_polynomial_iff_homogeneous_function} implies that
  \begin{equation*}
    p(X_1, \ldots, X_n, Y) = Y^d \cdot q\parens[\Big]{ \frac {X_1} Y, \cdots, \frac {X_n} Y }.
  \end{equation*}

  Then
  \begin{equation*}
    p(X_1, \ldots, X_n, Y) = Y^d \cdot f\parens[\Big]{ \frac {X_1} Y, \cdots, \frac {X_n} Y } \cdot g\parens[\Big]{ \frac {X_1} Y, \cdots, \frac {X_n} Y }.
  \end{equation*}

  Then
  \begin{equation*}
    \widehat{f}(X_1, \ldots, X_n, Y) \coloneqq Y^{\deg f} \cdot f\parens[\Big]{ \frac {X_1} Y, \cdots, \frac {X_n} Y }
  \end{equation*}
  is a polynomial in \( D[X_1, \ldots, X_n, Y] \) (and not a more general rational function). Thus,
  \begin{equation*}
    \widehat{f}(X_1, \ldots, X_n, 1) = f(X_1, \ldots, X_n).
  \end{equation*}

  Define \( \widehat{g} \) similarly.

  Therefore,
  \begin{equation*}
    p(X_1, \ldots, X_n, Y) = \widehat f(X_1, \ldots, X_n, Y) \cdot \widehat g(X_1, \ldots, X_n, Y).
  \end{equation*}

  Since \( p \) is irreducible, we conclude that \( \widehat{f} \) or \( \widehat{g} \) (or possibly both) are invertible. Without loss of generality, suppose that \( \widehat{f} \) is invertible. Then \fullref{thm:def:polynomial_algebra/invertible} implies that it is an invertible constant polynomial, and thus \( f \) is also an invertible constant.

  Generalizing on \( f \) and \( g \), we conclude that \( q(X_1, \ldots, X_n) = p(X_1, \ldots, X_n, 1) \) is irreducible.

  \NecessitySubProof Suppose that \( q \) is irreducible. We will show that \( p \) also is.

  If
  \begin{equation*}
    p(X_1, \ldots, X_n, Y) = f(X_1, \ldots, X_n, Y) \cdot g(X_1, \ldots, X_n, Y),
  \end{equation*}
  then
  \begin{equation*}
    p(X_1, \ldots, X_n, 1) = f(X_1, \ldots, X_n, 1) \cdot g(X_1, \ldots, X_n, 1).
  \end{equation*}

  Since \( q \) is irreducible, \( f(X_1, \ldots, X_n, 1) \) or \( g(X_1, \ldots, X_n, 1) \) (or both) is invertible.

  Without loss of generality, suppose that \( f(X_1, \ldots, X_n, 1) \) is invertible, thus, in particular, a constant polynomial. \Fullref{thm:divisors_of_homogeneous_polynomial} implies that \( f \) is homogeneous, thus
  \begin{equation*}
    f(X_1, \ldots, X_n, Y) = Y^{\deg f} \underbrace{f\parens[\Big]{ \frac {X_1} Y, \cdots, \frac {X_n} Y, 1 }}_{\T{invertible constant}}
  \end{equation*}
  is also an invertible constant polynomial.

  Generalizing on \( f \) and \( g \), we conclude that \( p \) is irreducible.
\end{proof}

\paragraph{Symmetric polynomials}

\begin{definition}\label{def:symmetric_polynomial}\mcite[139]{Тыртышников2017Алгебра}
  We say that a polynomial \( p(X_1, \ldots, X_n) \) over the \hyperref[def:semiring]{(semi)ring} \( R \) is \term[bg=симетричен (полином) (\cite[58]{ГеновМиховскиМоллов1991Алгебра}), ru=симметрический (многочлен), en=symmetric (polynomial) (\cite[190]{Lang2002Algebra})]{symmetric} if, for any \hyperref[def:algebra_over_semiring]{\( R \)-algebra} \( M \), the \hyperref[con:evaluation_homomorphism]{evaluation} \( \Phi_M(p) \) is a \hyperref[def:symmetric_function]{symmetric function}, i.e. \( \Phi_M(p) \) is invariant under permutations of \( X_1, \ldots, X_n \).
\end{definition}

\begin{definition}\label{def:elementary_symmetric_polynomial}\mcite[139]{Тыртышников2017Алгебра}
  We define the \term[ru=элементарный симметрический многочлен, en=elementary symmetric polynomial (\cite[190]{Lang2002Algebra})]{elementary symmetric polynomial} \( \sigma_k \) in \( X_1, \ldots, X_n \) as
  \begin{equation}\label{eq:def:elementary_symmetric_polynomial}
    \sigma_k(X_1, \ldots, X_n) \coloneqq \sum_{i_1 < \cdots < i_k} X_{i_1} \cdots X_{i_k}.
  \end{equation}
\end{definition}

\begin{lemma}\label{thm:symmetric_polynomial_recurrence}
  For the \hyperref[def:elementary_symmetric_polynomial]{elementary symmetric polynomials}, we have the following recurrence:
  \begin{equation}\label{eq:thm:symmetric_polynomial_recurrence}
    \sigma_k(X_1, \ldots, X_n) = \sigma_{k+1}(X_1, \ldots, X_{n-1}) + X_n \sigma_k(X_1, \ldots, X_{n-1})
  \end{equation}
\end{lemma}
\begin{proof}
  We have
  \begin{balign*}
    &\phantom{{}={}}
    \sigma_{k+1}(X_1, \ldots, X_{n-1}) + X_n \sigma_k(X_1, \ldots, X_{n-1})
    = \\ &=
    \sum_{\substack{i_1 < \cdots < i_{k+1} \\ i_{k+1} \leq n - 1}} X_{i_1} \cdots X_{i_{k+1}} + X_n \cdot \sum_{\substack{i_1 < \cdots < i_{k+1} \\ i_{k+1} \leq n - 1}} X_{i_1} \cdots X_{i_k}
    = \\ &=
    \sum_{i_1 < \cdots < i_{k+1}} X_{i_1} \cdots X_{i_{k+1}}
    = \\ &=
    \sigma_k(X_1, \ldots, X_n)
  \end{balign*}
\end{proof}

\begin{theorem}[Vieta's formulas]\label{thm:vietas_formulas}\mcite[thm. 3.11.1]{Тыртышников2017Алгебра}
  Suppose that, over some \hyperref[def:integral_domain]{integral domain}, the polynomial
  \begin{equation*}
    p(X) = \sum_{k=0}^n a_k X_k
  \end{equation*}
  \hyperref[def:polynomial_splits_into_linear_factors]{splits into linear factors}, and let \( \alpha_1, \ldots, \alpha_n \) be an enumeration of its roots.

  Then the non-leading coefficients \( a_0, \ldots, a_{n-1} \) can be expressed via the \hyperref[def:elementary_symmetric_polynomial]{elementary symmetric polynomials} applied to the roots as follows:
  \begin{equation}\label{eq:thm:vietas_formulas}
    a_k = a_n \cdot (-1)^{n-k} \cdot \sigma_{n-k}(\alpha_1, \ldots, \alpha_n).
  \end{equation}
\end{theorem}
\begin{proof}
  \Fullref{thm:polynomial_into_linear_factors} implies that
  \begin{equation*}
    p(X) = a_n \cdot \prod_{k=1}^n (X - \alpha_k).
  \end{equation*}

  We will use induction on \( n \). The case \( n = 0 \) is vacuous. Suppose that the theorem holds for polynomials of degree \( n - 1 \). Then we can apply the inductive hypothesis to
  \begin{equation*}
    q(X) \coloneqq a_n \cdot \prod_{k=1}^{n-1} (X - \alpha_k).
  \end{equation*}

  Let \( b_0, \ldots, b_{n-1} \) be the coefficients of \( q(X) \). The leading coefficient \( b_{n-1} \) of \( q(X) \) is \( a_n \). By the inductive hypothesis, for \( k = 0, \ldots, n - 2 \),
  \begin{equation*}
    b_k = a_n \cdot (-1)^{n-1-k} \cdot \sigma_{n-1-k}(\alpha_1, \ldots, \alpha_{n-1}).
  \end{equation*}

  Since \( p(X) = q(X) \cdot (X - \alpha_n) \), by the definition of convolution product, for \( k = 0, \ldots, n - 1 \),
  \begin{equation*}
    a_k = b_{k+1} - \alpha_n \cdot b_k.
  \end{equation*}

  Thus,
  \begin{balign*}
    a_k
    &=
    a_n \cdot \parens[\Big]{ (-1)^{n-k} \sigma_{n-k}(\alpha_1, \ldots, \alpha_{n-1}) - \alpha_n \cdot (-1)^{n-1-k} \cdot \sigma_{n-1-k}(\alpha_1, \ldots, \alpha_{n-1}) }
    = \\ &=
    a_n \cdot (-1)^{n-k} \cdot \parens[\Big]{ \sigma_{n-k}(\alpha_1, \ldots, \alpha_{n-1}) + \alpha_n \sigma_{n-1-k}(\alpha_1, \ldots, \alpha_{n-1}) }
    \reloset {\eqref{eq:thm:symmetric_polynomial_recurrence}} = \\ &=
    a_n \cdot (-1)^{n-k} \cdot \sigma_{n-k}(\alpha_1, \ldots, \alpha_n).
  \end{balign*}
\end{proof}

\paragraph{Resultants and discriminants}

\begin{definition}\label{def:sylvester_matrix}\mcite[136]{Тыртышников2017Алгебра}
  We define the \term[ru=матрица Сильвестра, en=Sylvester matrix (\cite[def. 3.6.2]{CoxLittleOShea2015AlgGeometry})]{Sylvester matrix} of the non-constant polynomials
  \begin{align*}
    p(X) = \sum_{k=0}^n a_k X^k
    &&\T{and}&&
    q(X) = \sum_{k=0}^m b_k X^k
  \end{align*}
  as the  of the \( (n + m) \times (n + m) \) matrix
  \begin{equation*}
    S(f, g) \coloneqq
    \begin{pmatrix}
      a_n    & a_{n-1} & \cdots  & \cdots  & a_0     &        &        &        \\
             & a_n     & a_{n-1} & \cdots  & \cdots  & a_0    &        &        \\
             &         & \cdots  & \cdots  &         &        & \cdots &        \\
             &         &         & a_n     & a_{n-1} & \cdots & \cdots & a_0    \\
      b_m    & b_{m-1} & \cdots  & \cdots  & b_0     &        &        &        \\
             & b_m     & b_{m-1} & \cdots  & \cdots  & b_0    &        &        \\
             &         & \cdots  & \cdots  &         &        & \cdots &        \\
             &         &         & b_n     & b_{m-1} & \cdots & \cdots & b_0
    \end{pmatrix}
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:common_root_implies_sylvester_matrix_singular}
  If two non-constant polynomials over an \hyperref[def:integral_domain]{integral domain} have a common root, their \hyperref[def:sylvester_matrix]{Sylvester matrix} is \hyperref[def:inverse_matrix]{singular}.
\end{proposition}
\begin{comments}
  \item The converse holds in a \hyperref[def:factorial_domain]{factorial domain} --- see \fullref{thm:sylvester_matrix_singular_implies_common_factor}.
\end{comments}
\begin{proof}
  Fix an integral domain \( D \) and two non-constant polynomials
  \begin{align*}
    p(X) = \sum_{k=0}^n a_k X^k
    &&\T{and}&&
    q(X) = \sum_{k=0}^m b_k X^k
  \end{align*}

  Then
  \begin{equation}\label{eq:thm:common_root_iff_sylvester_matrix_singular/proof}
    S(p, q)
    \cdot
    \begin{pmatrix}
      X^{n+m-1} \\
      X^{n+m-2} \\
      \vdots    \\
      X         \\
      1
    \end{pmatrix}
    =
    \begin{pmatrix}
      X^{m-1} \cdot p(X) \\
      \vdots             \\
      p(X)               \\
      X^{n-1} \cdot q(X) \\
      \vdots             \\
      q(X)
    \end{pmatrix}
  \end{equation}

  Suppose that \( \alpha \) is a common root of \( p(X) \) and \( q(X) \). Then \( p(\alpha) = q(\alpha) = 0 \), thus the column vector
  \begin{equation*}
    \begin{pmatrix}
      \alpha^{n+m-1} &
      \alpha^{n+m-2} &
      \cdots         &
      \alpha         &
      1
    \end{pmatrix}^T
  \end{equation*}
  is in the kernel of \( S(p, q) \). \Fullref{thm:matrix_invertibility_via_kernel} implies that \( S(p, q) \) is singular.
\end{proof}

\begin{lemma}\label{thm:linear_combination_implies_common_factor}
  Fix a \hyperref[def:factorial_domain]{factorial domain} \( D \) and two non-constant polynomials
  \begin{align*}
    p(X) = \sum_{k=0}^n a_k X^k
    &&\T{and}&&
    q(X) = \sum_{k=0}^m b_k X^k.
  \end{align*}

  If there exist nonzero polynomials \( f(X) \) of degree at most \( n - 1 \) and \( g(X) \) of degree at most \( m - 1 \), and if
  \begin{equation*}
    f(X) \cdot p(X) = g(X) \cdot q(X),
  \end{equation*}
  then \( p(X) \) and \( q(X) \) have a common irreducible factor.
\end{lemma}
\begin{proof}
  Since \( D \) is factorial, \fullref{thm:polynomial_ring_over_factorial} implies that \( D[X] \) is also factorial. Then factorizations exist. Consider the irreducible factorization
  \begin{equation*}
    f(X) \cdot p(X) = \alpha r_1(X) \cdots r_s(X).
  \end{equation*}

  We will use induction on \( s \) to show that at least one of these factors divides both \( p(X) \) and \( q(X) \).
  \begin{itemize}
    \item Since \( p(X) \) is non-constant, it is non-invertible in \( D[X] \), and \fullref{thm:def:factorial_domain/invertible} implies that it has at least one irreducible factor. If \( s = 1 \), this is the only factor of \( p(X) \), and similarly for \( q(X) \). This concludes the base case.

    \item Suppose that the statement holds for factorizations with \( s - 1 \) factors. Let \( r_i(X) \) be a factor of \( p(X) \). If it is also a factor of \( q(X) \), we are done. Otherwise, \( r_i(X) \) is a factor of \( g(X) \), and we have
    \begin{equation*}
      f(X) \cdot \frac {p(X)} {r_i(X)} = \alpha r_1(X) \cdots r_{i-1}(X) \cdot r_{i+1}(X) \cdots r_s(X) = \frac {g(X)} {r_i(X)} \cdot q(X).
    \end{equation*}

    Here \( p(X) / r_i(X) \) is a polynomial of degree at most \( n - 1 \), while \( g(X) / r_i(X) \) is a polynomial of degree at most \( n - 2 \). Thus, \( n \geq 2 \), and \( p(X) / r_i(X) \) is non-constant. We can thus apply the inductive hypothesis to conclude that \( p(X) / r_i(X) \) and \( q(X) \) have a common irreducible factor.
  \end{itemize}

  This completes the proof.
\end{proof}

\begin{proposition}\label{thm:sylvester_matrix_singular_implies_common_factor}
  If the \hyperref[def:sylvester_matrix]{Sylvester matrix} of some polynomials over a \hyperref[def:factorial_domain]{factorial domain} is \hyperref[def:inverse_matrix]{singular}, they have a common irreducible factor.
\end{proposition}
\begin{proof}
  Fix a factorial domain \( D \) and two non-constant polynomials
  \begin{align*}
    p(X) = \sum_{k=0}^n a_k X^k
    &&\T{and}&&
    q(X) = \sum_{k=0}^m b_k X^k.
  \end{align*}

  Suppose that \( S(p, q) \) is singular. \Fullref{thm:matrix_invertible_iff_transpose_invertible} implies that \( S(p, q)^T \) is also singular. Let
  \begin{equation*}
    \begin{pmatrix}
      \alpha_1 &
      \cdots   &
      \alpha_m &
      \beta_1  &
      \cdots   &
      \beta_n
    \end{pmatrix}^T
  \end{equation*}
  be a non-zero vector in the kernel of \( S(p, q)^T \).

  Consider the product
  \begin{equation*}
    \begin{pmatrix}
      X^{n+m-1} \\
      X^{n+m-2} \\
      \vdots    \\
      X         \\
      1
    \end{pmatrix}^T
    \cdot
    S(p, q)^T
    \cdot
    \begin{pmatrix}
      \alpha_1 \\
      \vdots   \\
      \alpha_m \\
      \beta_1  \\
      \vdots   \\
      \beta_n
    \end{pmatrix}
    \reloset {\eqref{eq:thm:common_root_iff_sylvester_matrix_singular/proof}} =
    \begin{pmatrix}
      X^{m-1} \cdot p(X) \\
      \vdots             \\
      p(X)               \\
      X^{n-1} \cdot q(X) \\
      \vdots             \\
      q(X)
    \end{pmatrix}^T
    \cdot
    \begin{pmatrix}
      \alpha_1 \\
      \vdots   \\
      \alpha_m \\
      \beta_1  \\
      \vdots   \\
      \beta_n
    \end{pmatrix}.
  \end{equation*}

  We have
  \begin{equation*}
    \underbrace{\sum_{k=1}^m \alpha_k \cdot X^{m-k}}_{f(X)} \cdot p(X)
    +
    \underbrace{\sum_{k=1}^n \beta_k \cdot X^{n-k}}_{-g(X)} \cdot q(X)
    =
    0.
  \end{equation*}

  Then we can apply \fullref{thm:linear_combination_implies_common_factor} to conclude that \( p(X) \) and \( q(X) \) have a common irreducible factor.
\end{proof}

\begin{definition}\label{def:resultant}\mcite[136]{Тыртышников2017Алгебра}
  We define the \term[ru=результанта, en=resultant (\cite[def. 3.6.2]{CoxLittleOShea2015AlgGeometry})]{resultant} of the non-constant univariate polynomials \( p(X) \) and \( q(X) \) as the \hyperref[def:determinant]{determinant}
  \begin{equation*}
    R(p, q) \coloneqq \det S(p, q)
  \end{equation*}
  of the corresponding \hyperref[def:sylvester_matrix]{Sylvester matrix}.
\end{definition}
\begin{comments}
  \item A useful characterization of resultants is given in \fullref{thm:resultant_as_product}.
\end{comments}

\begin{proposition}\label{thm:resultant_invertibility}
  In a \hyperref[def:factorial_domain]{factorial domain}, the \hyperref[def:resultant]{resultant} of two polynomials is non-invertible if and only if they have a common irreducible factor.
\end{proposition}
\begin{proof}
  Follows from \fullref{thm:common_root_implies_sylvester_matrix_singular} in one direction and \fullref{thm:sylvester_matrix_singular_implies_common_factor} in the other.
\end{proof}

\begin{proposition}\label{thm:resultant_of_linear_polynomial}
  The \hyperref[def:resultant]{resultant} of the non-constant polynomial
  \begin{equation*}
    p(X) = \sum_{k=0}^n a_k X^k
  \end{equation*}
  and the linear polynomial
  \begin{equation*}
    q(X) = b_0 + b_1 X
  \end{equation*}
  is
  \begin{equation}\label{eq:thm:resultant_of_linear_polynomial}
    R(p, q) = \sum_{k=0}^n (-1)^k a_{n-k} \cdot b_1^k \cdot b_0^{n-k}.
  \end{equation}
\end{proposition}
\begin{proof}
  \hyperref[def:sylvester_matrix]{Sylvester matrix}
  \begin{equation*}
    S(p, q) =
    \begin{pmatrix}
      a_n    & a_{n-1} & a_{n-2} & \cdots  & a_1 & a_0 \\
      b_m    & b_0     &         & \cdots  &     &     \\
             & b_m     & b_0     & \cdots  &     &     \\
             &         & \cdots  & \cdots  &     &     \\
             &         &         & \cdots  & b_0 &     \\
             &         &         & \cdots  & b_m & b_0
    \end{pmatrix}
  \end{equation*}

  We will show \eqref{eq:thm:resultant_of_linear_polynomial} via induction on \( n \). The case \( n = 2 \) follows from \fullref{thm:2x2_determinant}, so suppose that \eqref{eq:thm:resultant_of_linear_polynomial} holds. Consider the polynomial
  \begin{equation*}
    f(X) = \sum_{k=0}^{n+1} a_k X^k.
  \end{equation*}

  \Fullref{thm:laplace_expansion} implies that
  \begin{equation*}
    R(f, q)
    =
    a_{n+1} \cdot \underbrace{\det
    \begin{pmatrix}
      b_0     &         & \cdots  &     &     \\
      b_1     & b_0     & \cdots  &     &     \\
              & \cdots  & \cdots  &     &     \\
              &         & \cdots  & b_0 &     \\
              &         & \cdots  & b_1 & b_0
    \end{pmatrix}
    }_{b_0^{n+1}}
    -
    b_m \cdot \underbrace{\det
    \begin{pmatrix}
      a_n     & a_{n-1} & \cdots  & a_1 & a_0 \\
      b_1     & b_0     & \cdots  &     &     \\
              & \cdots  & \cdots  &     &     \\
              &         & \cdots  & b_0 &     \\
              &         & \cdots  & b_1 & b_0
    \end{pmatrix}}_{R(p, q)},
  \end{equation*}
  where \( p(X) = f(X) - a_{n+1} X^{n+1} \).

  From the inductive hypothesis it follows that
  \begin{equation*}
    R(f, q) = a_{n+1} b_0^{n+1} - b_1 \cdot \sum_{k=0}^n (-1)^k a_{n-k} \cdot b_1^k \cdot b_0^{n-k},
  \end{equation*}
  as desired.

  This concludes the proof.
\end{proof}

\begin{proposition}\label{thm:2x2_block_antidiagonal_determinant}
  We have the following \hyperref[def:block_matrix]{block matrix} \hyperref[def:determinant]{determinant} identity:
  \begin{equation}\label{eq:thm:2x2_block_determinant}
    \det
    \begin{pmatrix}
      0_{m \times n} & B              \\
      C              & 0_{n \times m}
    \end{pmatrix}
    =
    (-1)^{mn} \cdot \det(B) \cdot \det(C),
  \end{equation}
  where \( n > 0 \) and \( m > 0 \).
\end{proposition}
\begin{proof}
  We will use \fullref{thm:laplace_expansion}. Denote by \( c_{i,j} \) the \( (i, j) \)-th entry of \( C \). Then we can expand
  \begin{equation*}
    \det
    \begin{pmatrix}
      0_{m \times n} & B              \\
      C              & 0_{n \times m}
    \end{pmatrix}
    =
    \det
    \parens*
      {
        \begin{array}{c | c}
          0_{m \times n} & B              \\
          \hline
          \begin{matrix}
             c_{1,1} & \cdots & c_{n,1} \\
             \vdots  & \ddots & \vdots  \\
             c_{1,n} & \cdots & c_{n,n}
          \end{matrix} & 0_{n \times m}
        \end{array}
      }
  \end{equation*}
  along the first column to obtain
  \begin{equation*}
    \sum_{k=1}^n (-1)^{(m + k) + 1} \cdot c_{k,1} \cdot \det
    \begin{pmatrix}
      0_{m \times (n-1)} & B                  \\
      C_{k,1}            & 0_{(n-1) \times m}
    \end{pmatrix}
  \end{equation*}

  We will use induction on \( n \) to show \eqref{eq:thm:2x2_block_determinant}. In the base case \( n = 1 \) we have
  \begin{equation*}
    \det
    \begin{pmatrix}
      0_{m \times 1} & B              \\
      C              & 0_{1 \times m}
    \end{pmatrix}
    =
    (-1)^{(m + 1) + 1} \cdot \underbrace{c_{1,1}}_{\det C} \cdot \det B.
  \end{equation*}

  For the inductive step, suppose that the proposition holds for \( n - 1 \). Then
  \begin{balign*}
    \det \begin{pmatrix}
      0_{m \times n} & B              \\
      C              & 0_{n \times m}
    \end{pmatrix}
    &=
    \sum_{k=1}^n (-1)^{(m + k) + 1} \cdot c_{k,1} \cdot \det
    \begin{pmatrix}
      0_{m \times (n-1)} & B                  \\
      C_{k,1}            & 0_{(n-1) \times m}
    \end{pmatrix}
    = \\ &=
    \sum_{k=1}^n (-1)^{(m + k) + 1} \cdot c_{k,1} \cdot \det B \cdot (-1)^{m(n-1)} \cdot \det C_{k,1}
    = \\ &=
    (-1)^{mn} \cdot \det B \cdot \sum_{k=1}^n (-1)^{k+1} \cdot c_{k,1} \cdot \det C_{k,1}.
    = \\ &=
    (-1)^{mn} \cdot \det B \cdot \det C,
  \end{balign*}
  as desired.
\end{proof}

\begin{proposition}\label{thm:resultant_as_product}\mcite[prop. 3.10.2]{Тыртышников2017Алгебра}
  Fix two non-constant polynomials
  \begin{align*}
    p(X) = \sum_{k=0}^n a_k X^k
    &&\T{and}&&
    q(X) = \sum_{k=0}^m b_k X^k
  \end{align*}
  over an \hyperref[def:integral_domain]{integral domain} and suppose both \hyperref[def:polynomial_splits_into_linear_factors]{splits into linear factors}.

  Let \( \alpha_1, \ldots, \alpha_n \) and \( \beta_1, \ldots, \beta_m \) be enumerations of the roots of \( p(X) \) and \( q(X) \). Then for their \hyperref[def:resultant]{resultant} we have
  \begin{equation}\label{eq:thm:resultant_as_product}
    R(p, q) = a_n^m \cdot b_m^n \cdot \prod_{i=1}^n \prod_{j=1}^m (\alpha_i - \beta_j).
  \end{equation}
\end{proposition}
\begin{proof}
  Let
  \begin{equation*}
    W(x_1, \ldots, x_s) =
    \begin{pmatrix}
      x_1^{s-1} & x_2^{s-1} & \cdots & x_s^{s-1} \\
      x_1^{s-2} & x_2^{s-2} & \cdots & x_s^{s-2} \\
      \vdots    & \vdots    & \ddots & \vdots    \\
      x_1^1     & x_2^1     & \cdots & x_s^1     \\
      x_1^0     & x_2^0     & \cdots & x_s^0
    \end{pmatrix}
  \end{equation*}

  Due to its similarity with the \hyperref[ex:vandermonde_matrix]{Vandermonde matrix}, we can analogously deduce its determinant\fnote{The Vandermonde matrix has \( x_j - x_i \) in its determinant instead of \( x_i - x_j \).}:
  \begin{equation}\label{eq:thm:resultant_as_product/proof/w_determinant}
    \det W(x_1, \ldots, x_s) = \prod_{i < j} (x_i - x_j).
  \end{equation}

  Consider the matrices
  \begin{align*}
    Z        &\coloneqq W(\alpha_1, \ldots, \alpha_n, \beta_1, \ldots, \beta_m), \\
    Z_\alpha &\coloneqq W(\alpha_1, \ldots, \alpha_n), \\
    Z_\beta  &\coloneqq W(\beta_1, \ldots, \beta_m), \\
    D_p      &\coloneqq \op{diag}(p(\beta_1), \cdots, p(\beta_m)), \\
    D_q      &\coloneqq \op{diag}(q(\alpha_1), \cdots, q(\alpha_n)).
  \end{align*}

  Then
  \begin{equation}\label{eq:thm:resultant_as_product/proof/matrix_product}
    S(p, q) \cdot Z =
    \parens*
      {
        \begin{array}{c | c}
          0_{m \times n} & Z_\beta D_p   \\
          \hline
          Z_\alpha D_q   & 0_{n \times m}
        \end{array}
      }
  \end{equation}

  Indeed, denoting by \( c_{i,j} \) the \( (i, j) \)-th entry of the product \( S(p, q) \cdot Z \), we have
  \begin{equation*}
    c_{i,j} = \begin{cases}
      \alpha_j^{m-i} \cdot p(\alpha_j),         &1 \leq j \leq n \T{and} 1 \leq i \leq m, \\
      \beta_{j-n}^{m-i} \cdot p(\beta_{j-n}),   &n + 1 \leq j \leq n + m \T{and} 1 \leq i \leq m, \\
      \alpha_j^{n+m-i} \cdot q(\alpha_j),       &1 \leq j \leq m \T{and} m + 1 \leq i \leq n + m, \\
      \beta_{j-n}^{n+m-i} \cdot q(\beta_{j-n}), &n + 1 \leq j \leq n + m \T{and} m + 1 \leq i \leq n + m.
    \end{cases}
  \end{equation*}

  On the other hand, \( Z_\beta D_p \) is simpler:
  \begin{equation*}
    \begin{pmatrix}
      \beta_1^{m-1} \cdot p(\beta_1) & p(\beta_2) \cdot \beta_2^{m-1} & \cdots & \beta_m^{m-1} \cdot p(\beta_m) \\
      \beta_1^{m-2} \cdot p(\beta_1) & p(\beta_2) \cdot \beta_2^{m-2} & \cdots & \beta_m^{m-2} \cdot p(\beta_m) \\
      \vdots                         & \vdots                         & \ddots & \vdots                         \\
      \beta_1 \cdot p(\beta_1)       & p(\beta_2) \cdot \beta_2^{m-1} & \cdots & \beta_m \cdot p(\beta_m)       \\
      p(\beta_1)                     & p(\beta_2)                     & \cdots & p(\beta_m)
    \end{pmatrix}.
  \end{equation*}

  This demonstrates equality in \eqref{eq:thm:resultant_as_product/proof/matrix_product}.

  We can now use \eqref{eq:thm:resultant_as_product/proof/w_determinant} to determine the determinant of \( S(p, q) \cdot Z \):
  \begin{equation*}
    R(p, q) \cdot \det Z
    =
    R(p, q) \cdot \prod_{i=1}^n \prod_{j=1}^n (\alpha_i - \beta_j) \cdot \prod_{i_1 < i_2} (\alpha_{i_1} - \alpha_{i_2}) \cdot \prod_{j_1 < j_2} (\beta_{j_1} - \beta_{j_2}).
  \end{equation*}

  The determinant of \( Z_\beta D_p \) is
  \begin{equation*}
    \det(Z_\beta) \cdot \det(D_p)
    =
    \prod_{j_1 < j_2 \leq m} (\beta_{j_1} - \beta_{j_2}) \cdot \prod_{j=1}^m p(\beta_j)
    =
    \prod_{j_1 < j_2 \leq m} (\beta_{j_1} - \beta_{j_2}) \cdot a_n^m \prod_{j=1}^m \prod_{i=1}^n \underbrace{(\beta_j - \alpha_i)}_{(-1)(\alpha_i - \beta_j)},
  \end{equation*}
  where we have used that
  \begin{equation*}
    p(X) = a_n \prod_{i=1}^n (X - \alpha_i).
  \end{equation*}

  We can analogously obtain the determinant of \( Z_\alpha D_q \). From \fullref{thm:2x2_block_antidiagonal_determinant} it follows that the determinant of the right side of \eqref{eq:thm:resultant_as_product/proof/matrix_product} is
  \begin{equation*}
    (-1)^{nm} \cdot \det(Z_\beta D_p) \cdot \det(Z_\alpha D_q)
  \end{equation*}
  which we can expand to
  \begin{equation*}
    a_n^m \cdot b_m^n \cdot \prod_{i=1}^n \prod_{j=1}^m (\alpha_i - \beta_j) \cdot \det Z.
  \end{equation*}

  The determinant of the left side of \eqref{eq:thm:resultant_as_product/proof/matrix_product} is
  \begin{equation*}
    R(p, q) \cdot \det Z.
  \end{equation*}

  Cancelling \( \det Z \), we obtain \eqref{eq:thm:resultant_as_product}.
\end{proof}

\begin{definition}\label{def:discriminant}\mcite[prop. 8.5]{Lang2002Algebra}
  We define the \term[ru=дискриминант (\cite[141]{Винберг2014Алгебра}), en=discriminant (\cite[223]{Rotman2015AlgebraVol1})]{discriminant} of a non-constant polynomial
  \begin{equation*}
    p(X) = \sum_{k=0}^n a_k X_k
  \end{equation*}
  as
  \begin{equation*}
    D(p) \coloneqq \frac {(-1)^{n(n-1)/2}} {a_n} \cdot R(p, p'),
  \end{equation*}
  where \( R(p, p') \) is the \hyperref[def:resultant]{resultant} of \( p(X) \) and its \hyperref[def:algebraic_derivative]{algebraic derivative} \( p'(X) \).
\end{definition}
\begin{comments}
  \item Discriminants are often defined via the roots of \( p(X) \) and later this property is proven to be equivalent, for example by
  \incite[227]{Кострикин2000АлгебраТом1},
  \incite[258]{Jacobson1985AlgebraPart1},
  \incite[192]{Lang2002Algebra} and
  \incite[223]{Rotman2015AlgebraVol1}.

  We prefer the given definition because it does not explicitly use roots, the existence of which is a strong assumption and requires relying on \hyperref[def:splitting_field]{splitting fields}.

  We state the other definition as a characterization in \fullref{thm:discriminant_as_product}.
\end{comments}

\begin{lemma}\label{thm:derivative_at_polynomial_root}
  For the \hyperref[def:algebraic_derivative]{algebraic derivative} of
  \begin{equation*}
    p(X) = \prod_{k=1}^n (X - \alpha_k)
  \end{equation*}
  we have, for every \( m = 1, \ldots, n \),
  \begin{equation}\label{eq:thm:derivative_at_polynomial_root}
    p'(\alpha_m) = \prod_{k \neq m} (\alpha_m - \alpha_k).
  \end{equation}
\end{lemma}
\begin{proof}
  We will use induction on \( n \). The case \( n = 0 \) is vacuous, so suppose that the lemma holds for \( n - 1 \). \Fullref{thm:def:algebraic_derivative/product} implies that
  \begin{equation}\label{eq:thm:derivative_at_polynomial_root/product_derivative}
    p'(X) = \parens[\Big]{ \prod_{k=1}^{n-1} (X - \alpha_k) }' \cdot (X - \alpha_n) + \prod_{k=1}^{n-1} (X - \alpha_k) \cdot \underbrace{(X - \alpha_n)'}_{1}.
  \end{equation}

  \begin{itemize}
    \item If \( m = n \), the first term of \eqref{eq:thm:derivative_at_polynomial_root/product_derivative} vanishes and we are left with
    \begin{equation*}
      p'(\alpha_n) = \prod_{k=1}^{n-1} (\alpha_m - \alpha_k),
    \end{equation*}
    as desired.

    \item If \( m < n \), by the inductive hypothesis
    \begin{equation*}
      p'(\alpha_m) = \parens[\Big]{ \prod_{\substack{k \neq m \\ k \neq n}} (\alpha_m - \alpha_k) } \cdot (\alpha_m - \alpha_n) + \underbrace{\prod_{k=1}^{n-1} (\alpha_m - \alpha_k)}_{0}.
    \end{equation*}
  \end{itemize}
\end{proof}

\begin{proposition}\label{thm:discriminant_as_product}
  Fix an \hyperref[def:algebraically_closed_field]{\hi{algebraically closed}} \hyperref[def:field]{field} \( \BbbK \) and a polynomial
  \begin{equation*}
    p(X) = \sum_{k=0}^n a_k X^k.
  \end{equation*}

  Let \( \alpha_1, \ldots, \alpha_n \) be an enumeration of its roots. Then for its \hyperref[def:determinant]{determinant} we have
  \begin{equation}\label{eq:thm:resultant_as_product}
    D(p) = a_n^{2n - 2} \prod_{i < j} (\alpha_i - \alpha_j)^2.
  \end{equation}
\end{proposition}
\begin{proof}
  Let \( \beta_1, \ldots, \beta_{n-1} \) be the roots of \( p' \).

  \Fullref{thm:resultant_as_product} implies that
  \begin{equation}\label{eq:thm:discriminant_as_product/proof/resultant}
    D(p) = \frac {(-1)^{n(n-1)/2}} {a_n} \cdot a_n^{n-1} \cdot (n a_n)^n \cdot \prod_{i=1}^n \prod_{j=1}^{n-1} (\alpha_i - \beta_j).
  \end{equation}

  Note that
  \begin{equation*}
    p'(\alpha_i) = n a_n \prod_{j=1}^{n-1} (\alpha_i - \beta_j),
  \end{equation*}
  however from \fullref{thm:derivative_at_polynomial_root} it follows that
  \begin{equation*}
    p'(\alpha_i) = a_n \prod_{j \neq i} (\alpha_i - \alpha_j).
  \end{equation*}

  Therefore, we can simplify \eqref{eq:thm:discriminant_as_product/proof/resultant} to
  \begin{equation*}
    D(p) = (-1)^{n(n-1)/2} \cdot a_n^{2n-2} \prod_{i=1}^n \prod_{j \neq i} (\alpha_i - \alpha_j).
  \end{equation*}

  It remains to use
  \begin{equation*}
    (\alpha_i - \alpha_j) = (-1)(\alpha_j - \alpha_i)
  \end{equation*}
  whenever \( j > i \) to conclude \eqref{eq:thm:discriminant_as_product}.
\end{proof}

\begin{corollary}\label{thm:discriminant_invertibility}
  Fix a polynomial \( p(X) \) over an \hyperref[def:algebraically_closed_field]{algebraically closed field}. The \hyperref[def:discriminant]{discriminant} \( D(p) \) is zero if and only if \( p(X) \) has repeated roots.
\end{corollary}
\begin{proof}
  \SubProof{Proof 1} \Fullref{thm:resultant_invertibility} implies that the discriminant \( D(p) \) is nonzero if and only if \( p(X) \) has no irreducible factor in common with its algebraic derivative. Since the field is algebraically closed, every irreducible factor is linear, thus corresponding to a root.

  \SubProof{Proof 2} \Fullref{thm:discriminant_as_product} implies that \( D(p) \) is zero if and only if it has repeated roots.
\end{proof}

\paragraph{Quadratic polynomials}

\begin{lemma}\label{thm:quadratic_polynomial_discriminant}
  The \hyperref[def:discriminant]{discriminant} of the quadratic polynomial \( p(X) = a X^2 + b X + c \) is
  \begin{equation}\label{eq:thm:quadratic_polynomial_discriminant}
    D(p) = b^2 - 4ac
  \end{equation}
\end{lemma}
\begin{proof}
  We have
  \begin{align*}
    D(p)
    &=
    \frac {(-1)} a \cdot \begin{pmatrix}
      a  & b  & c  \\
      2a & b  &    \\
         & 2a & b
    \end{pmatrix}
    \reloset {\eqref{eq:thm:3x3_determinant}} = \\ &=
    \frac {(-1)} a \cdot \parens[\Big]{ ab^2 + 0 + 4a^2c - 0 - 0 - 2ab^2 }
    = \\ &=
    b^2 - 4ac.
  \end{align*}
\end{proof}

\begin{proposition}\label{thm:quadratic_polynomial_roots}
  Fix a \hyperref[def:monic_polynomial]{monic} quadratic univariate polynomial \( p(X) = aX^2 + b X + c \) over a \hyperref[def:field]{field} \( \BbbK \).

  If \( \alpha \) and \( \beta \) are roots of \( p(X) \), then \hyperref[def:discriminant]{discriminant} \( b^2 - 4ac \) is the square of \( s = a(\alpha + \beta) \) and
  \begin{align*}
    2a\alpha = -b + s
    &&\T{and}&&
    2a\beta = -b - s.
  \end{align*}
\end{proposition}
\begin{proof}
  Suppose that \( \alpha \) and \( \beta \) are the roots of \( p(X) \). \Fullref{thm:vietas_formulas} implies that
  \begin{equation*}
    b =  -a \cdot \sigma_1(\alpha, \beta) = -a(\alpha + \beta)
  \end{equation*}
  and
  \begin{equation*}
    c = a \cdot \sigma_2(\alpha, \beta) = a\alpha\beta.
  \end{equation*}

  The discriminant is
  \begin{equation*}
    D(p) = b^2 - 4ac = a^2(\alpha + \beta)^2 - 4a^2 \alpha\beta = a^2 (\alpha^2 + 2\alpha\beta + v^2) - 4a^2 \alpha\beta = a^2 (\alpha - \beta)^2.
  \end{equation*}

  Denote \( a(\alpha - \beta) \) by \( s \). Then
  \begin{equation*}
    2a\alpha = a(\alpha + \beta) + a(\alpha - \beta) = -b + s
  \end{equation*}
  and
  \begin{equation*}
    2a\beta = a(\alpha + \beta) - a(\alpha - \beta) = -b - s
  \end{equation*}
\end{proof}

\paragraph{Irreducible polynomials}

\begin{proposition}\label{thm:axx_byy_irreducible}
  Fix a \hyperref[def:totally_ordered_set]{totally} \hyperref[def:ordered_ring]{ordered} \hyperref[def:field]{field} \( \BbbK \).

  The polynomial \( p(X, Y) = a X^2 + b Y^2 \), where \( a \) and \( b \) are either both \hyperref[def:ordered_semiring_positivity]{positive} or \hyperref[def:ordered_semiring_positivity]{negative}, is irreducible in \( \BbbK[X, Y] \).
\end{proposition}
\begin{proof}
  Fix some decomposition \( p(X, Y) = f(X, Y) \cdot g(X, Y) \).

  Aiming at a contradiction, suppose that both are not invertible. Then both have positive degree. \Fullref{thm:divisors_of_homogeneous_polynomial} implies that both \( f(X, Y) \) and \( g(X, Y) \) are homogeneous, and \fullref{thm:polynomial_degree_arithmetic/product} leaves only the possibility that both are linear homogeneous.

  Let
  \begin{align*}
    f(X, Y) &= c X + d Y + e \\
    g(X, Y) &= f X + g Y + h.
  \end{align*}

  We have
  \begin{equation*}
    f(X, Y) \cdot g(X, Y) = c f X^2 + c g X Y + c h X + d f X Y + d g Y^2 + d h Y + e f X + e g Y + e h.
  \end{equation*}

  In order for there to be no mixed monomials, we must have \( c g = - d f \). Furthermore, \( c \), \( d \), \( f \) and \( g \) are nonzero because otherwise either \( a = c f \) or \( b = d g \) would be zero.

  Since \( a = c f \), it follows that \( f = a / c \), and similarly \( g = b / d \). Then
  \begin{equation*}
    c g = c \cdot \frac b d = - d \cdot \frac a c = - d f.
  \end{equation*}

  Multiplying by \( cd \), we obtain
  \begin{equation}\label{eq:thm:axx_byy_irreducible/proof/contradiction}
    c^2 \cdot b = - d^2 \cdot a.
  \end{equation}

  We have assumed that \( a \) and \( b \) have matching signs. \Fullref{thm:ordered_ring_power} implies that both \( c^2 \) and \( d^2 \) are positive. Then \fullref{thm:def:signum} implies that \( c^2 \cdot b \) and \( d^2 \cdot a \) also have matching signs.

  But then they cannot satisfy \eqref{eq:thm:axx_byy_irreducible/proof/contradiction}. The obtained contradiction shows that \( p(X, Y) \) is irreducible.
\end{proof}

\begin{corollary}\label{thm:ordered_field_not_algebraically_closed}
  A \hyperref[def:totally_ordered_set]{totally} \hyperref[def:ordered_semiring]{ordered} \hyperref[def:field]{field} cannot be \hyperref[def:algebraically_closed_field]{algebraically closed}.
\end{corollary}
\begin{proof}
  \Fullref{thm:axx_byy_irreducible} implies that \( X^2 + Y^2 \) is an irreducible polynomial, and \fullref{thm:homogeneous_polynomial_constant} imply that \( X^2 + 1 \) is also irreducible.
\end{proof}

\begin{proposition}\label{thm:axx_byy_czz_irreducible}
  The polynomial \( p(X, Y, Z) = a X^2 + b Y^2 + c Z^2 \), where \( a \), \( b \) and \( c \) are nonzero scalars from an arbitrary \hyperref[def:field]{field} \( \BbbK \), is irreducible in \( \BbbK[X, Y, Z] \).
\end{proposition}
\begin{proof}
  As in \fullref{thm:axx_byy_irreducible}, suppose that \( p(X, Y, Z) \) is a product of the linear polynomials
  \begin{align*}
    q(X, Y, Z) &= d X + e Y + f Z + g, \\
    r(X, Y, Z) &= h X + i Y + j Z + k.
  \end{align*}

  Then \( q(X, Y, Z) \cdot r(X, Y, Z) \) is
  \begin{balign*}
    &\phantom{{}+{}}
    d h X^2 + d i X Y + d j X Z + d k X
    + \\ &+
    e h X Y + e i Y^2 + e j Y Z + e k Y
    + \\ &+
    f h X Z + f i Y Z + f j Z^2 + f k Z
    + \\ &+
    g h X + g i Y + g j Z + g k.
  \end{balign*}

  Since \( a = dh \), \( b = ei \) and \( c = fj \) are nonzero, it follows that the corresponding scalars are nonzero. Furthermore, we must have
  \begin{align*}
    (d i + e h) X Y &= 0, \\
    (d j + f h) X Z &= 0, \\
    (e j + f i) Y Z &= 0,
  \end{align*}
  that is,
  \begin{align*}
    d i &= - e h, \\
    d j &= - f h, \\
    e j &= - f i.
  \end{align*}

  We can divide the first two equalities to obtain \( i / j = e / f \), i.e. \( ej = fi \). But the third equality states that \( ej = -fi \). Hence, \( ej \) and \( fi \) can both only be zero. But we know that \( e \), \( f \), \( i \) and \( j \) are all nonzero, hence \( ej \) and \( fi \) must also be nonzero.

  The obtained contradiction shows that the polynomial \( p(X, Y, Z) = a X^2 + b Y^2 + c Z^2 \) is irreducible over any field.
\end{proof}

\begin{proposition}\label{thm:axz_byy_irreducible}
  The polynomial \( p(X, Y, Z) = a XZ + b Y^2 \), where \( a \) and \( b \) are nonzero scalars from an arbitrary \hyperref[def:field]{field} \( \BbbK \), is irreducible in \( \BbbK[X, Y, Z] \).
\end{proposition}
\begin{proof}
  As in \fullref{thm:axx_byy_czz_irreducible}, suppose that the homogeneous polynomial \( p(X, Y, Z) \) is a product of the linear polynomials
  \begin{align*}
    q(X, Y, Z) &= d X + e Y + f Z + g, \\
    r(X, Y, Z) &= h X + i Y + j Z + k.
  \end{align*}

  Unlike in \fullref{thm:axx_byy_czz_irreducible}, we need \( dh \) and \( fj \), the coefficients of \( X^2 \) and \( Z^2 \), to be zero. The coefficient \( dj + fh \) of \( XZ \) must be nonzero, however. Then either \( d \) and \( j \) are nonzero and \( h = f = 0 \), or vice versa.

  The coefficients \( ei \) of \( Y^2 \) is also nonzero, hence \( e \neq 0 \) and \( i \neq 0 \).

  \begin{itemize}
    \item If \( h = f = 0 \), then, since the coefficient \( di + eh \) of \( XY \) must be zero, it follows that \( i = 0 \).

    \item If \( d = j = 0 \), then, since the coefficient \( ej + fi \) of \( YZ \) must be zero, it again follows that \( i = 0 \).
  \end{itemize}

  In both cases we obtain a contradiction. Therefore, \( p(X, Y, Z) \) is irreducible.
\end{proof}
