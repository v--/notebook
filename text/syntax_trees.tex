\section{Syntax trees}\label{sec:syntax_trees}

\paragraph{Parse trees}

\begin{definition}\label{def:parse_tree}\mcite[\S 2.2.3]{AhoEtAl2006Compilers}
  A \term[ru=дерево вывода (\cite[81]{Гладкий1973ГрамматикиИЯзыки})]{parse tree} or \term{concrete syntax tree} for the \hyperref[def:chomsky_hierarchy/context_free]{context-free grammar} \( G = (V, \Sigma, \to, S) \) is a \hyperref[def:labeled_tree]{labeled tree} \( T \) with labels from \( V \cup \Sigma \) such that:
  \begin{thmenum}
    \thmitem{def:parse_tree/root} The root has label \( S \).
    \thmitem{def:parse_tree/leaves} If a leaf is labeled by a nonterminal \( N \), there exists a rule \( N \to \bnfes \).
    \thmitem{def:parse_tree/children} There is a rule
    \begin{equation*}
      x \to y_1 \cdots y_n,
    \end{equation*}
    where \( x \) is the label of the root of \( T \) and \( y_1, \ldots, y_n \) --- of the immediate children of \( T \).
  \end{thmenum}
\end{definition}
\begin{comments}
  \item We associate with each parse tree a string as shown in \fullref{def:parse_tree_string}.
  \item Although we generally follow the definition from \cite[\S 2.2.3]{AhoEtAl2006Compilers}, we disallow parse trees for strings containing nonterminals, and thus we do not need to specially handle \( \bnfes \)-labeled nodes.
\end{comments}

\begin{remark}\label{rem:parse_tree_roots}
  In syntactic definitions and proofs, for example in relation to \hyperref[con:evaluation]{evaluation}, we usually consider \hyperref[def:formal_grammar/schema]{grammar schemas} and each subtree has a root with a different label. In this case, we work with parse trees for different grammars over the same grammar schema (differing by the start symbol). We also allow singleton trees consisting of a single terminal or \( \bnfes \), acknowledging that these are not technically parse trees.
\end{remark}

\begin{definition}\label{def:parse_tree_string}\mcite[\S 2.2.3]{AhoEtAl2006Compilers}
  We say that the \hyperref[def:parse_tree]{parse tree} \( T \) \term{yields} the string \( x_1 \cdots x_n \) if, for \( k = 1, \ldots, n \), the \( k \)-th member in the \hyperref[def:ordered_tree_enumeration]{pre-order enumeration} of the terminal-labeled nodes of \( T \) has label \( x_k \).
\end{definition}
\begin{comments}
  \item We implicitly associate this string with the parse tree.
\end{comments}

\begin{example}\label{ex:def:parse_tree}
  We give several examples of \hyperref[def:parse_tree]{parse trees}.

  \begin{thmenum}
    \thmitem{ex:def:parse_tree/an} Consider the grammar
    \begin{equation*}
      \begin{aligned}
        S &\to A \mid \bnfes, \\
        A &\to aA \mid a.
      \end{aligned}
    \end{equation*}
    from \fullref{ex:def:chomsky_hierarchy/an}.

    It describes the language \( \mscrL = \set{ a^n \given n \geq 0 } \) discussed in \fullref{ex:def:formal_language/an}. The only possible parse tree for the string \( aaa \) is
    \begin{equation*}\label{eq:ex:def:parse_tree/an}
      \begin{aligned}
        \includegraphics[page=1]{output/ex__def__parse_tree}
      \end{aligned}
    \end{equation*}

    If we instead use the \enquote{simpler} grammar
    \begin{equation*}
      S \to aS \mid \bnfes,
    \end{equation*}
    then we would have an \( \bnfes \)-labeled node in the parse tree:
    \begin{equation*}
      \includegraphics[page=2]{output/ex__def__parse_tree}
    \end{equation*}

    \thmitem{ex:def:parse_tree/anbn} Consider the grammar
    \begin{equation*}
      \begin{aligned}
        S &\to A \mid \bnfes, \\
        A &\to aAb \mid ab
      \end{aligned}
    \end{equation*}
    from \fullref{ex:def:chomsky_hierarchy/anbn} describing \( \mscrL = \set{ a^n b^n \given n \geq 0 } \) from \fullref{ex:def:formal_language/anbn}.

    The only possible parse tree for the string \( aaabbb \) is
    \begin{equation*}
      \includegraphics[page=3]{output/ex__def__parse_tree}
    \end{equation*}

    \thmitem{ex:def:parse_tree/arithmetic} We continue the binary natural number grammar example from \fullref{ex:natural_number_arithmetic_grammar/schema} with the grammar \eqref{eq:ex:natural_number_arithmetic_grammar/schema/shorthand}.

    The only possible parse tree for the expression \( (\syn1 \syn0 \syntimes (\syn1 \synplus \syn1 \syn0)) \) is
    \begin{equation*}
      \includegraphics[page=4]{output/ex__def__parse_tree}
    \end{equation*}

    We will discuss abstract syntax trees in \fullref{con:abstract_syntax_tree}, which are notably tidier.

    We can \hyperref[con:evaluation]{evaluate} this parse tree by substituting every binary number string (such as \( \syn1 \syn0 \)) with its numeric value in \( \BbbN \) and then recursively applying the corresponding expressions. Evaluating the above parse tree gives the same result, the decimal number \( 6 \), as evaluating \( \syn1 \syn 1 \syn0 \). More formal details on this evaluation are discussed in \fullref{ex:natural_number_arithmetic_grammar/evaluation}.

    Now consider instead the same grammar without parentheses, that is, replace the expression expansion rule
    \begin{equation*}
      E \to (E O E)
    \end{equation*}
    with
    \begin{equation*}
      E \to E O E.
    \end{equation*}

    The corresponding expression \( \syn1 \syn0 \synplus \syn1 \syntimes \syn1 \syn0 \) has more than one parse tree:
    \begin{equation*}
      \begin{aligned}
        \includegraphics[page=5]{output/ex__def__parse_tree}
        \qquad\qquad
        \includegraphics[page=6]{output/ex__def__parse_tree}
      \end{aligned}
    \end{equation*}

    The first tree corresponds to \( (10 \syntimes (1 \synplus 10)) \), which evaluates to \( 6 \), while the second one corresponds to \( ((\syn1 \syn0 \syntimes \syn1) \synplus 10) \), which evaluates to \( 4 \).\fnote{\( 10 \) in binary notation is \( 2 \) is decimal notation, \( 100 \) in binary is \( 4 \) in decimal and \( 110 \) in binary is \( 6 \) in decimal.}

    Thus, removing parentheses makes the grammar ambiguous in the sense of \fullref{def:grammar_ambiguity}. We will show in \fullref{ex:natural_number_arithmetic_grammar/unambiguous} that the parenthesized grammar is unambiguous.
  \end{thmenum}
\end{example}

\begin{algorithm}[Brute force parsing]\label{alg:brute_force_parsing}
  Fix a \hyperref[def:chomsky_hierarchy/context_free]{context-free grammar} \( G = (V, \Sigma, \to, S) \). For each nonterminal \( A \), we will introduce a \hyperref[def:function]{set-valued map} \( P_A(w) \), whose values are parse trees of \( w \).

  To avoid unbounded recursion, we will also define the auxiliary set-valued map \( P'_A(w, U) \), where \( U \) is a set of nonterminal-string pairs \( (B, u) \) \enquote{already traversed} during the recursion. This will allow us to define
  \begin{equation*}
    P_A(w)
    \coloneqq
    P'_A(w, \varnothing)
  \end{equation*}

  Suppose that we are given the rule \( A \to v_1 \cdots v_m \), where each \( v_k \) is a single symbol (either a terminal or nonterminal). The algorithm relies on partitioning \( w \) into \( m \) parts. Denote by \( S(w, m) \) the set of all partitions of \( w \) into \( m \) parts, that is, all \( m \)-tuples \( \vect w = (w_1, \ldots, w_m) \) such that
  \begin{equation*}
    w = \underbrace{ r_1 \cdots r_{l_1} }_{w_1} \underbrace{ r_{l_1 + 1} \cdots r_{l_1 + l_2} }_{w_2} \cdots \underbrace{ r_{l_1 + \cdots + l_{m-1} + 1} \cdots r_{l_1 + \cdots + l_m} }_{w_m},
  \end{equation*}
  where \( l_k \coloneqq \len(w_k) \).

  We will introduce a second auxiliary set-valued map
  \begin{equation*}
    P^\dprime_A(A \to v_1 \cdots v_m, \vect w, U),
  \end{equation*}
  which gives the parse trees of \( w \) corresponding to a concrete rule and partition. This will allow us to define
  \begin{equation*}
    P'_A(w, U)
    \coloneqq
    \bigcup\set[\Big]{ P^\dprime_A(A \to v_1 \cdots v_m, \vect w, U) \given A \to v_1 \cdots v_m \T{and} \vect w \in S(w, m) }.
  \end{equation*}

  \begin{thmenum}
    \thmitem{alg:brute_force_parsing/tree_set} For each \( v_k \) in the rule \( A \to v_1 \cdots v_m \), construct a set \( \mscrT_k \) of parse trees as follows:
    \begin{itemize}
      \item If \( v_k \) is a terminal, let \( \mscrT_k \) be a set with one element --- \hyperref[def:canonical_singleton_tree]{canonical singleton tree} with label \( w_k \).

      \item If \( v_k \) is a nonterminal, let
      \begin{equation*}
        \mscrT_k \coloneqq \begin{cases}
          \varnothing,                                         &(A, w_k) \in U, \\
          P'_{v_k}\parens[\Big]{ w_k, U \cup \set{ (A, w) } }, &\T{otherwise.}
        \end{cases}
      \end{equation*}
    \end{itemize}

    \thmitem{alg:brute_force_parsing/combine} Combine the above sets \( \mscrT_1, \cdots, \mscrT_m \) as follows:
    \begin{itemize}
      \item If \( m = n = 0 \), i.e. if \( A \to \bnfes \) and \( w = \bnfes \), let
      \begin{equation*}
        P^\dprime_A\parens[\Big]{ A \to v_1 \cdots v_m, \vect w, U }
      \end{equation*}
      consist of the \hyperref[def:canonical_singleton_tree]{canonical singleton tree} with label \( A \).

      \item Otherwise, let
      \begin{equation*}
        P^\dprime_A\parens[\Big]{ A \to v_1 \cdots v_m, \vect w, U }
      \end{equation*}
      consist of all \hyperref[def:ordered_tree_grafting_product]{grafted trees} of the form
      \begin{equation*}
        \begin{aligned}
          \includegraphics[page=1]{output/alg__brute_force_parsing}
        \end{aligned}
      \end{equation*}
      where \( (T_1, \ldots, T_m) \) is a tuple in the Cartesian product \( \mscrT_1 \times \cdots \times \mscrT_m \).
    \end{itemize}
  \end{thmenum}
\end{algorithm}
\begin{comments}
  \item This is a variation of Stephen Unger's algorithm described in \cite{Unger1968Parser}.
  \item This algorithm can be found as \identifier{grammars.brute_force_parse.parse} in \cite{notebook:code}.
  \item We can avoid the auxiliary parameter \( U \) if the grammar has no \hyperref[def:epsilon_free_grammar]{\( \bnfes \) rules} and no \hyperref[def:renaming_rule]{renaming rules}, but otherwise we can easily get stuck in a loop like \( A \Rightarrow B \Rightarrow A \) (if \( A \to B \) and \( B \to A \)) or \( A \Rightarrow AB \Rightarrow A \) (if \( B \to \bnfes \)).
\end{comments}

\paragraph{Leftmost and rightmost derivations}

\begin{definition}\label{def:leftmost_derivation}\mcite[53]{Salomaa1973FormalLanguages}
  In a context-free grammar, we say that the \hyperref[def:formal_grammar/derivation]{derivation}
  \begin{equation*}
    S \Rightarrow w_1 \Rightarrow \cdots \Rightarrow w_m = w
  \end{equation*}
  is \term{leftmost} if, given any step \( w_{k-1} = p_k A_k s_k \Rightarrow p_k v_k s_k = w_k \), the prefix \( p_k \) contains only terminals.

  We define \term{rightmost} by instead requiring that the suffix \( s_k \) contains only terminals.
\end{definition}

\begin{example}\label{ex:natural_number_arithmetic_grammar/derivation}
  We will consider derivations in the binary natural number grammar \eqref{eq:ex:natural_number_arithmetic_grammar/schema/shorthand} from \fullref{ex:natural_number_arithmetic_grammar/schema}.

  We have shown in \fullref{ex:def:parse_tree/arithmetic} that removing parentheses results in multiple parse trees yielding the same string.

  Similarly, using parentheses results in a single leftmost derivation:
  \begin{equation*}
    \begin{aligned}
      \includegraphics[page=1]{output/ex__natural_number_arithmetic_grammar__derivation}
    \end{aligned}
  \end{equation*}

  Removing parentheses allows for multiple leftmost derivations:
  \begin{equation*}
    \begin{aligned}
      \includegraphics[page=2]{output/ex__natural_number_arithmetic_grammar__derivation}
    \end{aligned}
  \end{equation*}
\end{example}

\begin{proposition}\label{thm:leftmost_derivation_existence}
  Every string in a context-free grammar's generated language has at least one \hyperref[def:leftmost_derivation]{leftmost derivation}.
\end{proposition}
\begin{proof}
  A leftmost derivation can be achieved via reordering of the derivation steps.
\end{proof}

\begin{algorithm}[Derivation to parse tree]\label{alg:derivation_to_parse_tree}
  Fix a context-free grammar \( G = (V, \Sigma, \to, S) \) and a derivation
  \begin{equation*}
    S = w_0 \Rightarrow w_1 \Rightarrow \cdots \Rightarrow w_m = w
  \end{equation*}
  of the terminal-only string \( w = r_1 \cdots r_n \).

  We will construct a parse tree recursively. At step \( k \), where \( k = 0, \ldots, m \), the leaves of the tree \( T_k \), enumerated by \hyperref[def:ordered_tree_enumeration]{pre-ordering}, should be the symbols of the string \( w_k \). Proceed as follows:
  \begin{thmenum}
    \thmitem{alg:derivation_to_parse_tree/initial} Let \( T_0 \) be the singleton tree with label \( S \).

    \thmitem{alg:derivation_to_parse_tree/step} At step \( k > 0 \), having already built \( T_{k-1} \) for \( w_{k-1} \), let \( v_0 \) be the first (with respect to pre-order enumeration) node labeled by \( A \) in \( T_{k-1} \).

    Then the tree \( T_{k-1} \) has the form
    \begin{equation*}
      \includegraphics[page=1]{output/alg__parse_tree_from_derivation}
    \end{equation*}
    where \( w_{k-1} = r_1 \cdots r_l \) are the symbols of \( w_{k-1} \), and \( j_0 \) is the index of the first instance of \( A \) in \( w_{k-1} \).

    \begin{itemize}
      \item If \( l = 0 \), i.e. if \( w_i = \bnfes \), define \( T_k \coloneqq T_{k-1} \).

      \item Otherwise, there exists a rule \( A \to s_1 \cdots s_p \) producing \( w_i \) from \( w_{k-1} \), and we instead define \( T_k \) by \hyperref[def:ordered_tree_grafting]{grafting} \( s_1, \ldots, s_p \) onto \( v_0 \):
      \begin{equation*}
        \begin{aligned}
          \includegraphics[page=2]{output/alg__parse_tree_from_derivation}
        \end{aligned}
      \end{equation*}
    \end{itemize}
  \end{thmenum}

  \thmitem{alg:derivation_to_parse_tree/finish} The tree \( T_m \) is the desired parse tree for \( w \).
\end{algorithm}
\begin{comments}
  \item This algorithm can be found as \identifier{grammars.parse_tree.parse_tree_to_derivation} in \cite{notebook:code}.
\end{comments}

\begin{algorithm}[Parse tree to leftmost derivation]\label{alg:parse_tree_to_leftmost_derivation}
  Let \( T \) be a parse tree for \( w = r_1 \cdots r_n \). We will explicitly construct a \hyperref[def:leftmost_derivation]{leftmost derivation} for \( w \).

  We proceed as follows:
  \begin{thmenum}
    \thmitem{alg:parse_tree_to_leftmost_derivation/initial} Let \( v_0, v_1, \ldots, v_m \) be the \hyperref[def:ordered_tree_enumeration]{pre-order enumeration} of the nodes of \( T \) labeled by nonterminals. Denote by \( l_k \) the label of \( v_k \).

    Define \( w_0 \coloneqq l_0 \). By assumption, the root label \( l_0 \) of \( T \) is \( S \).

    \thmitem{alg:parse_tree_to_leftmost_derivation/step} At step \( 0 < k \leq m \), the nonterminal \( l_{k-1} \) must occur in \( w_{k-1} \).

    Let \( s_1, \ldots, s_p \) be the labels of the children of \( v_k \). \Fullref{def:parse_tree/children} ensures that there exists a production rule
    \begin{equation*}
      l_{k-1} \to s_1 \cdots s_p.
    \end{equation*}

    Given \( w_{k-1} = a l_{k-1} b \), where \( a \) and \( b \) are strings and \( a \) does not contain nonterminals, we define
    \begin{equation*}
      w_k \coloneqq a s_1 \cdots s_p b.
    \end{equation*}

    \thmitem{alg:parse_tree_to_leftmost_derivation/finish} At step \( k = m + 1 \), there are no more terminals in \( w_m \). Furthermore, the symbols of \( w_m \) are by construction a pre-order enumeration of \( T \), thus \( w_m = w \).

    Therefore, we have the derivation
    \begin{equation*}
      S = w_0 \Rightarrow w_1 \Rightarrow \cdots \Rightarrow w_m = w.
    \end{equation*}
  \end{thmenum}
\end{algorithm}
\begin{comments}
  \item This algorithm can be found as \identifier{grammars.parse_tree.parse_tree_to_derivation} in \cite{notebook:code}.
\end{comments}

\begin{proposition}\label{thm:derivations_and_parse_trees}
  In a \hyperref[def:chomsky_hierarchy/context_free]{context-free grammar}, there is a bijective correspondence between \hyperref[def:parse_tree]{parse trees} and \hyperref[def:leftmost_derivation]{leftmost derivations} of terminal-only strings.
\end{proposition}
\begin{proof}
  \Fullref{alg:derivation_to_parse_tree} allows us to construct a parse tree from any derivation. Multiple derivations can lead to the same parse tree, but our choice of nonterminal in \fullref{alg:derivation_to_parse_tree/step} ensures that a leftmost derivation leads to a unique parse tree.

  Conversely, \fullref{alg:parse_tree_to_leftmost_derivation} allows us to construct a leftmost derivation from any parse tree. Uniqueness is ensured by choosing at \fullref{alg:parse_tree_to_leftmost_derivation/step} the smallest possible index.
\end{proof}

\begin{corollary}\label{thm:parse_tree_existence}
  A string over a context-free grammar's terminal symbols is derivable if and only if there exists a parse tree for it.
\end{corollary}
\begin{proof}
  Follows from \fullref{thm:derivations_and_parse_trees} and \fullref{thm:leftmost_derivation_existence}.
\end{proof}

\paragraph{Grammar ambiguity}

\begin{definition}\label{def:grammar_ambiguity}\mcite[54]{Salomaa1973FormalLanguages}
  We say that a string generated by a \hyperref[def:chomsky_hierarchy/context_free]{context-free grammar} is \term{ambiguous} if any of the following equivalent conditions hold:
  \begin{thmenum}
    \thmitem{def:grammar_ambiguity/tree} It has multiple \hyperref[def:parse_tree]{parse trees}.
    \thmitem{def:grammar_ambiguity/leftmost} It has multiple \hyperref[def:leftmost_derivation]{leftmost derivations}.
    \thmitem{def:grammar_ambiguity/rightmost} It has multiple \hyperref[def:leftmost_derivation]{right derivations}.
  \end{thmenum}

  We say that the grammar itself is \term{ambiguous} if it generates at least one ambiguous string, and \term{unambiguous} otherwise.
\end{definition}
\begin{comments}
  \item \Fullref{thm:derivations_and_parse_trees} ensures that every generated string has at least one parse tree.
  \item \Fullref{thm:leftmost_derivation_existence} ensures that every generated string has at least one leftmost derivation.
\end{comments}
\begin{proof}
  \EquivalenceSubProof{def:grammar_ambiguity/tree}{def:grammar_ambiguity/leftmost} Follows from \fullref{thm:derivations_and_parse_trees}.
  \EquivalenceSubProof{def:grammar_ambiguity/leftmost}{def:grammar_ambiguity/rightmost} Trivial.
\end{proof}

\begin{example}\label{ex:natural_number_arithmetic_grammar/unambiguous}
  We will show that the binary natural number grammar \eqref{eq:ex:natural_number_arithmetic_grammar/schema/shorthand} from \fullref{ex:natural_number_arithmetic_grammar/schema} is \hyperref[ex:natural_number_arithmetic_grammar/unambiguous]{unambiguous}.

  Let \( w \) be a string in \( \mscrL(G) \). We will build a derivation tree for \( w \) via \fullref{thm:omega_recursion} on \( \len(w) \):

  \begin{itemize}
    \item If \( \len(w) = 1 \), then \( w \) is either \enquote{\( \syn0 \)} or \enquote{\( \syn1 \)}, and the possible parse trees are
    \begin{equation*}
      \begin{aligned}
        \includegraphics[page=1]{output/ex__natural_number_arithmetic_grammar__unambiguous}
        \qquad\qquad
        \includegraphics[page=2]{output/ex__natural_number_arithmetic_grammar__unambiguous}
      \end{aligned}
    \end{equation*}

    \item Let \( w = r_1 \cdots r_n \) be a string of length \( n \) and suppose that every string shorter than \( w \) is unambiguous.

    \begin{itemize}
      \item If \( r_1 \) is \enquote{\( ( \)}, then \( w \) is an expression. Hence, \( r_n \) is \enquote{\( ) \)} and there exists some index \( k \) such that \( r_k \) is either \enquote{\( \synplus \)} or \enquote{\( \syntimes \)}. The inductive hypothesis applies to \( r_2 \cdots r_{k-1} \) and  \( r_{k\synplus1} \cdots r_{n-1} \), and gives us parse trees \( T_1 \) and \( T_2 \) whose roots are labeled with \( E \). Then the following is the unique parse tree of \( w \):
      \begin{equation*}
        \begin{aligned}
          \includegraphics[page=3]{output/ex__natural_number_arithmetic_grammar__unambiguous}
        \end{aligned}
      \end{equation*}

      \item Otherwise, \( w \) is a binary numerical string. Given a parse tree of \( r_1 \cdots r_{n-2} \), since the grammar with start symbol \( N \) is right linear, the parse tree for \( w \) requires simply adding the dotted edges
      \begin{equation*}
        \begin{aligned}
          \includegraphics[page=4]{output/ex__natural_number_arithmetic_grammar__unambiguous}
        \end{aligned}
      \end{equation*}
    \end{itemize}
  \end{itemize}
\end{example}

\begin{proposition}\label{thm:regular_grammars_are_unambiguous}
  All \hyperref[def:chomsky_hierarchy/regular]{regular grammars} are \hyperref[def:grammar_ambiguity]{unambiguous}.
\end{proposition}
\begin{proof}
  The right side of any rule contains at most one nonterminal, so there is only one possible derivation for every string.
\end{proof}

\paragraph{Evaluation}

\begin{concept}\label{con:evaluation}
  We have briefly discussed \enquote{evaluation} in \fullref{con:syntax_semantics_duality}. We will now describe it in more detail.

  Fix a grammar \( G = (V, \Sigma, \to, S) \) with nonterminals \( A_1, \cdots, A_n \). Then an example evaluation function has the form
  \begin{equation*}
    E(w) \coloneqq \begin{cases}
      \cdots, &w \T{is generated from} S, \\
      \cdots, &w \T{is generated from} A_1, \\
              &\vdots,
    \end{cases}
  \end{equation*}

  \term{Pattern matching} is a term used for partial definition of \( E \) based on the structure of \( w \). Pattern matching depends on knowing the possible parse trees for \( w \). We use parse trees implicitly without explicitly mentioning it. \Fullref{ex:natural_number_arithmetic_grammar/evaluation} provides justification for not using parse trees directly.

  We use the convention from \fullref{rem:parse_tree_roots} regarding parse tree for different starting nodes over the same grammar schema. Of course, more granular rules are possible based on the structure of strings --- see \fullref{ex:natural_number_arithmetic_grammar/evaluation} for an example.
\end{concept}
\begin{comments}
  \item If the grammar is \hyperref[def:grammar_ambiguity]{ambiguous}, \( E \) may not be \hyperref[def:function]{single-valued} if we are not careful.
  \item We allow different starting nonterminals because, if \( F \) applies itself recursively, it must be able to process strings generated in different ways.
\end{comments}

\begin{example}\label{ex:natural_number_arithmetic_grammar/evaluation}
  We will define \hyperref[con:evaluation]{evaluation} for the binary natural number grammar \eqref{eq:ex:natural_number_arithmetic_grammar/schema/shorthand} from \fullref{ex:natural_number_arithmetic_grammar/schema}. Let
  \begin{equation*}
    F(w) \coloneqq \begin{cases}
      0,                  &w = \syn0, \\
      1,                  &w = \syn1, \\
      2 \times F(w'),     &w = w' \syn0, \\
      2 \times F(w') + 1, &w = w' \syn1, \\
      F(u) + F(v),        &w = (u \synplus v), \\
      F(u) \times F(v),   &w = (u \syntimes v),
    \end{cases}
  \end{equation*}
  where we have utilized the dot convention from \fullref{rem:object_language_dots/terminals} to unambiguously utilize metalingual numbers and operations.

  In this example, we do pattern matching on the string \( w \). This pattern matching depends on knowing the possible parse trees and knowing that the grammar is unambiguous. We use parse trees implicitly without recognizing it.

  Using parse trees directly would be much more tedious to describe --- for example, defining summation, we would need to state that trees of the form
  \begin{equation*}
    \begin{aligned}
      \includegraphics[page=1]{output/ex__natural_number_arithmetic_grammar__evaluation}
    \end{aligned}
  \end{equation*}
  evaluate as \( F(T_1) + F(T_2) \).
\end{example}

\paragraph{Abstract syntax trees}

\begin{concept}\label{con:abstract_syntax_tree}
  \hyperref[def:parse_tree]{Parse trees} focus on how strings are built from symbols. Take the following parse tree from \fullref{ex:def:parse_tree/arithmetic}:
  \begin{equation}\label{eq:con:abstract_syntax_tree/base}
    \begin{aligned}
      \includegraphics[page=1]{output/rem__abstract_syntax_tree}
    \end{aligned}
  \end{equation}

  Most of the information in this tree is not necessary for \hyperref[con:evaluation]{evaluating} it. An \term[en=abstract syntax tree (\cite[41]{AhoEtAl2006Compilers})]{abstract syntax tree} (AST) is instead an \hyperref[def:labeled_tree]{labeled tree} that contains information that is necessary for evaluation, but not for yielding the source string. The possible abstract syntax trees for a language differ depending on their use, as we will show, and are often produced from parse trees (which we called \enquote{concrete syntax trees} in \fullref{def:parse_tree}). The only restriction we put on abstract syntax trees is that they should be finite labeled trees in a bijective correspondence with parse trees.

  First, we can vastly simplify the tree by splitting the grammar \eqref{eq:ex:natural_number_arithmetic_grammar/schema/shorthand} into two parts:
  \begin{thmenum}
    \thmitem{con:abstract_syntax_tree/lexical} The \term{lexical part}
    \begin{equation*}
      \begin{aligned}
        N &\to \syn0 \mid \syn1 \mid \syn1 B \\
        B &\to \syn0 \mid B \syn0 \mid \syn1 \mid B \syn1 \\
        O &\to \synplus \mid \syntimes
      \end{aligned}
    \end{equation*}

    Any strings produced via the lexical rules are called \term[ru=лексемы (\cite[329]{Гладкий1973ГрамматикиИЯзыки})]{lexemes}. The goal is to isolate atoms of a language from arbitrarily complicated recursive structures. This distinction is important for evaluation and becomes apparent in more complicated use cases like first-order formula substitution in \fullref{def:first_order_substitution}.

    We rarely need parse trees for lexical rules. The lexical grammar is often \hyperref[def:chomsky_hierarchy/regular]{regular}.

    \thmitem{con:abstract_syntax_tree/syntactic} The \term{syntactic part}
    \begin{equation*}
      E \to N \mid (E O E)
    \end{equation*}

    Syntactic rules regard lexemes as base symbols, although this is often not possible formally. In this example, \( N \) produces infinitely many strings, and by definition a grammar's terminals are only finitely many.

    Compared to lexical rules, syntactic rules benefit from parse trees because they highlight the recursive structure of a string.
  \end{thmenum}

  One simplification we can do to \eqref{eq:con:abstract_syntax_tree/base} is to collapse lexical rules and regard lexemes as strings:
  \begin{equation}\label{eq:con:abstract_syntax_tree/syntactic}
    \begin{aligned}
      \includegraphics[page=2]{output/rem__abstract_syntax_tree}
    \end{aligned}
  \end{equation}

  Another thing we can simplify is to remove auxiliary symbols like parentheses\fnote{Whitespace and comments in programming languages are also often useless in parse trees during evaluation, but they are useful for code refactoring tools, error reporting and metaprogramming.}. They are only necessary in order to the grammar to be unambiguous, but once we already have a parse tree, they become meaningless. Removing the parentheses in \eqref{eq:con:abstract_syntax_tree/syntactic} leads to
  \begin{equation}\label{eq:con:abstract_syntax_tree/no_parens}
    \begin{aligned}
      \includegraphics[page=3]{output/rem__abstract_syntax_tree}
    \end{aligned}
  \end{equation}

  We can also remove unnecessarily long chains of nonterminals from \eqref{eq:con:abstract_syntax_tree/no_parens} where that would not be ambiguous (i.e. collapse \( E \to E \to \syn1 \syn0 \) to \( E \to \syn1 \syn0 \)):
  \begin{equation}\label{eq:con:abstract_syntax_tree/collapsed}
    \begin{aligned}
      \includegraphics[page=4]{output/rem__abstract_syntax_tree}
    \end{aligned}
  \end{equation}

  We can now do a simplification based on our intended semantics. Every node in \eqref{eq:con:abstract_syntax_tree/collapsed} is labeled via either a number lexeme, operation symbol or \( E \). We know the operations are binary, and we can use the symbol of the operation instead of \( E \) as a label, removing the need to keep a separate node for the operation symbol. This leads to
  \begin{equation}\label{eq:con:abstract_syntax_tree/final}
    \begin{aligned}
      \includegraphics[page=5]{output/rem__abstract_syntax_tree}
    \end{aligned}
  \end{equation}

  This last tree has only 5 nodes, compared to the original tree \eqref{eq:con:abstract_syntax_tree/base} with 21 nodes. Yet, we can recover \eqref{eq:con:abstract_syntax_tree/base} from \eqref{eq:con:abstract_syntax_tree/final}.
\end{concept}

\paragraph{Paired delimiters}

\begin{definition}\label{def:paired_delimiters}\mimprovised
  Fix a \hyperref[def:formal_language/language]{formal language} \( \mscrL \) over the \hyperref[def:formal_language/alphabet]{alphabet} \( \mscrA \). Fix also symbols \( l \) and \( r \) in \( \mscrA \), which we will call \term{paired delimiters}. We call \( l \) and \( r \) the \term{left} and \term{right} delimiters, or, in analogy with parentheses and brackets, \term{opening} and \term{closing} delimiters.

  We say that the delimiters \( l \) and \( r \) are \term{balanced} in the string \( w \) if it has an equal amount of occurrences of \( l \) and \( r \), and every prefix of \( w \) has at least as many occurrences of \( l \) as there are of \( r \).

  If every string in \( \mscrL \) has balanced delimiters, we say that \( \mscrL \) itself has balanced delimiters.
\end{definition}
\begin{comments}
  \item The term \enquote{paired delimiter} is based on \identifier{DeclarePairedDelimiter} command from the \identifier{mathtools} \LaTeX{} package. The command is used in this monograph to define convenience macros for different kinds of parentheses and brackets.

  \item The condition for balanced delimiters is based on the discussion of \hyperref[def:dyck_language]{Dyck languages} by \incite[example 6.6.6]{Stanley2023EnumerativeCombinatoricsVol2}.

  \item It is not formally necessary for \( l \) and \( r \) to be distinct --- for example, \hyperref[def:absolute_value]{absolute values} and \hyperref[def:norm]{norms} have identical opening and closing delimiters.
\end{comments}

\begin{example}\label{ex:def:paired_delimiters}
  We list examples of \hyperref[def:paired_delimiters]{paired delimiters}:
  \begin{thmenum}
    \thmitem{ex:def:paired_delimiters/none} Of course, if neither delimiter occurs in a word, it vacuously has balanced delimiters.

    \thmitem{ex:def:paired_delimiters/anbn} Consider the language
    \begin{equation*}
      \mscrL \coloneqq \set{ a^n b^n \given n \geq 0 }
    \end{equation*}
    from \fullref{ex:def:formal_language/anbn}.

    Regarded as delimiters, \( a \) and \( b \) are balanced in \( \mscrL \). Indeed, for every prefix \( p \) of \( a^n b^n \), there exists some \( m \leq n \) such that either \( p = a^m \) or \( p = a^n b^m \). In both cases \( p \) has at least as many occurrences of \( a \) as there are of \( b \).

    \thmitem{ex:def:paired_delimiters/lambda} \hyperref[def:lambda_term]{\( \synlambda \)-terms} have balanced parentheses.

    For example, in the term \( (\synx \synx) \), every nonempty prefix has one opening parenthesis, and only the entire word has a closing parenthesis. Thus, the word has balanced parentheses.

    This is straightforward to verify in more complicated terms like
    \begin{equation*}
      \ref{eq:ex:def:lambda_term/combinator/big_omega} = ((\qabs \synx (\synx \synx)) (\qabs \synx (\synx \synx))).
    \end{equation*}

    \thmitem{ex:def:paired_delimiters/unbalanced} On the other hand, the word \( )( \) has unbalanced delimiters because the prefix \( ) \) has more closing than opening parentheses.
  \end{thmenum}
\end{example}

\begin{proposition}\label{thm:paired_delimiters_not_regular}
  If a formal language with \hi{distinct} \hyperref[def:paired_delimiters]{balanced delimiters} has, for every positive integer \( p \), a word whose first \( p \) symbols are left delimiters, it is not a \hyperref[def:chomsky_hierarchy/regular]{regular language}.
\end{proposition}
\begin{proof}
  Fix such a language \( \mscrL \) and a positive integer \( p \). By assumption, there exists a word \( w \) in \( \mscrL \) whose first \( p \) symbols are left delimiters.

  Let \( w = x y z \) be a decomposition of \( w \) such that \( y \) is nonempty and \( \len(xy) \leq p \). Then \( xy \) consist only of left delimiters.

  For any integer \( n \geq 2 \), the word \( x y^n z \) then has more left than right delimiters. Thus, \( x y^n z \) has unbalanced delimiters, and hence does not belong to \( \mscrL \).

  By the contraposition to \fullref{thm:regular_pumping_lemma}, \( \mscrL \) is not regular.
\end{proof}

\begin{example}\label{ex:thm:paired_delimiters_not_regular}
  In \fullref{thm:paired_delimiters_not_regular} the condition for arbitrarily many left delimiters is important.

  Consider, for example, the language
  \begin{equation*}
    \mscrL \coloneqq \set{ (ab)^n \given n \geq 0 },
  \end{equation*}
  where we regard \( a \) and \( b \) as paired delimiters.

  We can decompose the word \( abab \) as \( x = y = ab \) and \( z = \bnfes \), and then \( xy^n z \) is again in \( \mscrL \) for any positive integer \( n \).

  We can also decompose it as \( x = a \), \( y = ba \) and \( z = b \), and again \( xy^n z \) is in \( \mscrL \).

  Any longer word in \( \mscrL \) has \( abab \) as a prefix, thus we cannot obtain a contradiction with \fullref{thm:regular_pumping_lemma}.
\end{example}

\begin{definition}\label{def:dyck_language}\mcite[example 6.6.6]{Stanley2023EnumerativeCombinatoricsVol2}
  The \term[ru=язык Дика (\cite[213]{Гладкий1973ГрамматикиИЯзыки})]{Dyck language} is the language \hyperref[def:formal_grammar/language]{generated} by the \hyperref[def:formal_grammar]{grammar}
  \begin{bnf*}
    \bnfprod{word} {\bnftsq{(} \bnfsp \bnfpn{word} \bnfsp \bnftsq{)} \bnfor \bnfpn{word} \bnfsp \bnfpn{word} \bnfor \varepsilon}
  \end{bnf*}

  We call strings in this language \term{Dyck words}.
\end{definition}

\begin{proposition}\label{thm:dyck_language_balanced}
  \hyperref[def:dyck_language]{Dyck words} have \hyperref[def:paired_delimiters]{balanced parentheses}.
\end{proposition}
\begin{proof}
  Let \( w \) be a word in the Dyck language. We will use \fullref{thm:induction_on_rooted_trees} on the \hyperref[def:parse_tree]{parse tree} of \( w \).

  \begin{itemize}
    \item If \( w \) is the empty word, its parentheses are vacuously balanced.
    \item If \( w = (w') \) and \( w' \) has balanced parentheses, then, for every prefix \( p \) of \( w \), either
    \begin{itemize}
      \item \( p = ( \), in which case \( p \) has more left parentheses than right.
      \item \( p = (p' \), where \( p' \) is a prefix of \( w' \), in which case again \( p \) has more parentheses than right.
      \item \( p = w = (w') \), then \( p \) has as an equal amount of left and right parentheses.
    \end{itemize}

    \item If \( w = w_1 w_2 \) and both \( w_1 \) and \( w_2 \) have balanced parentheses, then, for every prefix \( p \) of \( w \), either
    \begin{itemize}
      \item \( p = p_1 \), where \( p_1 \) is a prefix of \( w_1 \).
      \item \( p = w_1 p_2 \), where \( p_2 \) is a prefix of \( w_2 \).
    \end{itemize}

    In both cases, \( p \) has at least as many left parentheses as right parentheses.
  \end{itemize}

  In all cases, \( w \) has balanced parentheses.
\end{proof}

\begin{proposition}\label{thm:dyck_language_alternative_grammar}
  The \hyperref[def:dyck_language]{Dyck language} can also be \hyperref[def:formal_grammar/language]{generated} by the following \hyperref[def:formal_grammar]{grammar}:
  \textup
    {
      \begin{bnf*}
        \bnfprod{word} {\bnftsq{(} \bnfsp \bnfpn{word} \bnfsp \bnftsq{)} \bnfsp \bnfpn{word} \bnfor \varepsilon}
      \end{bnf*}
    }
\end{proposition}
\begin{proof}
  Denote by \( W \) the Dyck language generated by the grammar in \fullref{def:dyck_language} and by \( V \) --- the language generated by this proposition's grammar.

  \SubProof{Proof that \( W \subseteq V \)} We will first use induction on the length of the Dyck word \( w \) from \( W \) to show that it belongs to \( V \).

  \begin{itemize}
    \item The case \( w = \bnfes \) is obvious.

    \item If \( w = (w') \), then we can apply the inductive hypothesis to \( w' \). Then \( w' \) belongs to \( V \), and thus so does \( w = (w')\bnfes \).

    \item If \( w = w_1 w_2 \), where \( w_1 \) and \( w_2 \) are nonempty substrings, we can apply the inductive hypothesis to conclude that they both belong to \( V \).

    Then \( w_1 = (v_1) v_2 \) for some words \( v_1 \) and \( v_2 \) from \( V \). The string \( v_2 w_2 \) is still shorter than \( w \), so the inductive hypothesis applies, and we conclude that it belongs to \( V \).

    It follows that \( w = (v_1) v_2 w_2 \) is generated by this proposition's grammar, i.e. it belongs to \( V \).
  \end{itemize}

  \SubProof{Proof that \( V \subseteq W \)} Let \( v \) be a string in \( V \). We will use \fullref{thm:induction_on_rooted_trees} to show that \( v \) belongs to \( W \).

  \begin{itemize}
    \item Again, the case \( v = \bnfes \) is obvious.

    \item If \( v = (v_1) v_2 \), where the inductive hypothesis holds for \( v_1 \) and \( v_2 \), then \( (v_1) \) belongs to \( W \), and its concatenation with \( v_2 \) thus also belongs to \( W \).
  \end{itemize}
\end{proof}

\begin{corollary}\label{thm:dyck_words_and_binary_trees}
  \begin{figure}[!ht]
    \begin{subcaptionblock}{0.19\textwidth}
      \centering
      \includegraphics[page=1]{output/thm__dyck_words_and_binary_trees}
      \caption{\texttt{((()))}.}
    \end{subcaptionblock}
    \begin{subcaptionblock}{0.19\textwidth}
      \centering
      \includegraphics[page=2]{output/thm__dyck_words_and_binary_trees}
      \caption{\texttt{(()())}.}
    \end{subcaptionblock}
    \begin{subcaptionblock}{0.19\textwidth}
      \centering
      \includegraphics[page=3]{output/thm__dyck_words_and_binary_trees}
      \caption{\texttt{()(())}.}
    \end{subcaptionblock}
    \begin{subcaptionblock}{0.19\textwidth}
      \centering
      \includegraphics[page=4]{output/thm__dyck_words_and_binary_trees}
      \caption{\texttt{(())()}.}
    \end{subcaptionblock}
    \begin{subcaptionblock}{0.19\textwidth}
      \centering
      \includegraphics[page=5]{output/thm__dyck_words_and_binary_trees}
      \caption{\texttt{()()()}.}
    \end{subcaptionblock}
    \caption{A visualization of \fullref{thm:dyck_words_and_binary_trees}: \hyperref[def:n_ary_tree]{full binary trees} and their corresponding \hyperref[def:dyck_language]{Dyck words}.}\label{fig:thm:dyck_words_and_binary_trees}
  \end{figure}

  \hyperref[def:n_ary_tree]{Full binary trees} and \hyperref[def:dyck_language]{Dyck words} are related as follows:
  \begin{thmenum}
    \thmitem{thm:dyck_words_and_binary_trees/tree_to_word} For every full binary tree \( T \), we define a Dyck word \( w_T \) as follows:
    \begin{equation}\label{eq:thm:dyck_words_and_binary_trees/tree_to_word}
      w_T \coloneqq \begin{cases}
        \bnfes,             &T \T{has no children}, \\
        ( w_{T_1} ) w_{T_2} &T_1 \T{and} T_2 \T{are children of} T.
      \end{cases}
    \end{equation}

    \thmitem{thm:dyck_words_and_binary_trees/word_to_tree} Conversely, for every Dyck word \( w \), we can define a tree \( T_w \) as follows:
    \begin{thmenum}
      \thmitem{thm:dyck_words_and_binary_trees/word_to_tree/empty} If \( w = \bnfes \), we define \( T_w \) as the \hyperref[def:canonical_singleton_tree]{canonical singleton tree}.

      \thmitem{thm:dyck_words_and_binary_trees/word_to_tree/parens} If \( w = (w_1) w_2 \), assuming we have already built \( T_{w_1} \) and \( T_{w_2} \), we define \( T_w \) by \hyperref[def:ordered_tree_grafting_product]{grafting} \( T_{w_1} \) and \( T_{w_2} \) to a new root.
    \end{thmenum}

    \thmitem{thm:dyck_words_and_binary_trees/inverse} The above procedures are mutually inverse and define a bijective correspondence.
  \end{thmenum}
\end{corollary}
\begin{proof}
  \Fullref{thm:dyck_language_alternative_grammar} implies that \eqref{eq:thm:dyck_words_and_binary_trees/tree_to_word} describes precisely all Dyck words. This process is clearly reversible.
\end{proof}
