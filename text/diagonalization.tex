\subsection{Diagonalization}\label{subsec:diagonalization}

In this subsection, we restrict ourselves to fields rather than arbitrary rings.

\begin{definition}\label{def:eigenpair}\mcite[def. 4.17]{Rudin1991Functional}
  Let \( T: V \to V \) be a \hyperref[def:semimodule/homomorphism]{linear endomorphism} over the \hyperref[def:vector_space]{vector space} \( V \) over \( \BbbK \).

  An \term{eigenpair} \( (\lambda, x) \) consists of an \term{eigenvalue} \( \lambda \in \BbbK \) and a \hi{nonzero} \term{eigenvector} \( x \in V \) such that
  \begin{equation*}
    Tx = \lambda x.
  \end{equation*}

  We say that \( \lambda \) is an eigenvalue of \( T \) if it is part of at least one eigenpair; analogously, we say that \( x \) is an eigenvector if it is part of at least one eigenpair.

  In function spaces, the term \enquote{eigenfunction} is sometimes used instead of \enquote{eigenvector}.
\end{definition}

\begin{example}\label{ex:def:eigenpair}
  We list some examples of \hyperref[def:eigenpair]{eigenpairs}:
  \begin{thmenum}
    \thmitem{ex:def:eigenpair/zero} The zero matrix over \( \BbbK \) of degree \( n \) has the entire vector space \( \BbbK^n \), without the zero vector, as its eigenvectors corresponding to the eigenvalue \( 0 \).

    \thmitem{ex:def:eigenpair/identity} The identity matrix \( I_n \) over \( \BbbK \) of degree \( n \) also has \( \BbbK^n \setminus \set{ \vect 0 } \) as its eigenvectors, however they correspond to the eigenvalue \( 1 \) rather than \( 0 \).

    \thmitem{ex:def:eigenpair/2112} Consider the matrix
    \begin{equation*}
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}.
    \end{equation*}

    The following two are eigenpairs:
    \begin{align*}
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
      \begin{pmatrix}
        1 \\ 1
      \end{pmatrix}
      =
      3
      \begin{pmatrix}
        1 \\ 1
      \end{pmatrix}
      &&
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
      \begin{pmatrix}
        1 \\ -1
      \end{pmatrix}
      =
      1
      \begin{pmatrix}
        1 \\ -1
      \end{pmatrix}
    \end{align*}

    As we shall show in \fullref{ex:def:eigenspace/2112}, these are not the only eigenpairs for \( A \).

    \thmitem{ex:def:eigenpair/exponent} In suitable function spaces like \( C^\infty(\BbbR) \), the function \( e^{\lambda x} \) is en eigenvector of the \hyperref[def:differentiability]{differentiation} operator \( D_x \) corresponding to the eigenvalue \( \lambda \) because
    \begin{equation*}
      D_x e^{\lambda x} = \lambda e^{\lambda x}.
    \end{equation*}

    In particular, the case \( \lambda = 0 \) corresponds to the constant function \( e^0 = 1 \), hence
    \begin{equation*}
      D_x 1 = 0 \cdot 1.
    \end{equation*}
  \end{thmenum}
\end{example}

\begin{remark}\label{rem:eigenpairs_via_invertibility}
  The \hyperref[def:eigenpair]{eigenpair equation}
  \begin{equation*}
    Tx = \lambda x
  \end{equation*}
  can be rewritten as
  \begin{equation*}
    (T - \lambda \cdot \id) x = \vect 0.
  \end{equation*}

  We can regard the above as a \hyperref[rem:system_of_equations]{system of equations}. By \fullref{thm:homogeneous_linear_equations_solutions}, there exists a nonzero solution, i.e. an eigenvector corresponding to \( \lambda \), if and only if the map
  \begin{equation*}
    T - \lambda \cdot \id
  \end{equation*}
  is an isomorphism.

  If \( V \) is finite-dimensional, by \fullref{thm:square_matrix_left_invertible_iff_right_invertible}, this map is injective if and only if it is surjective. Otherwise, it may fail to be either injective or surjective.
\end{remark}

\begin{definition}\label{def:eigenspace}\mcite[sec. 31.2]{Тыртышников2007}
  The \term{eigenspace} of an \hyperref[def:eigenpair]{eigenvalue} of \( T: V \to V \) is the set of all corresponding \hyperref[def:eigenpair]{eigenvectors}, along with the zero vector. It is a vector subspace of \( V \) as a consequence of \fullref{thm:degrees_of_freedom}. The \hyperref[thm:vector_space_dimension]{dimension} of the eigenspace is called the \term{geometric multiplicity} of the eigenvalue.
\end{definition}

\begin{example}\label{ex:def:eigenspace}
  We list some examples of \hyperref[def:eigenspace]{eigenspaces}:
  \begin{thmenum}
    \thmitem{ex:def:eigenspace/zero} We discussed in \fullref{ex:def:eigenpair/zero} how, with respect to the zero matrix, the entire space is the eigenspace of zero. Hence, the geometric multiplicity of zero is \( n \).

    \thmitem{ex:def:eigenspace/identity} Similarly, with respect to the identity matrix, the entire space is the eigenspace of \( 1 \).

    \thmitem{ex:def:eigenspace/2112} We discussed in \fullref{ex:def:eigenpair/2112} how the vector \( (1, 1) \) corresponds to the eigenvalue \( 3 \). The entire eigenspace is
    \begin{equation*}
      \set{ (t, t) \given t \in \BbbK }.
    \end{equation*}

    The eigenspace of \( 1 \) is
    \begin{equation*}
      \set{ (-t, t) \given t \in \BbbK }.
    \end{equation*}

    \thmitem{ex:def:eigenspace/1101} The eigenspace of \( 1 \) with respect to
    \begin{equation*}
      \begin{pmatrix}
        1 & 1 \\
        0 & 1
      \end{pmatrix}
    \end{equation*}
    is
    \begin{equation*}
      \set{ (t, 0) \given t \in \BbbK }.
    \end{equation*}

    We will show in \fullref{ex:def:characteristic_polynomial/1101} that this is the only eigenvalue of the matrix.

    \thmitem{ex:def:eigenspace/exponent} The eigenspace of \( \lambda \) with respect to differentiation is
    \begin{equation*}
      \set{ t e^\lambda \given t \in \BbbK }.
    \end{equation*}

    Hence, every scalar is an eigenvalue with geometric multiplicity \( 1 \).
  \end{thmenum}
\end{example}

\begin{definition}\label{def:characteristic_polynomial}\mimprovised
  Let \( T: V \to V \) be a linear endomorphism over the \( n \)-dimensional vector space \( V \) over \( \BbbK \). Regard \( T \) as an endomorphism over the module \( \BbbK[\Lambda]^n \) over the \hyperref[def:polynomial_algebra]{polynomial ring} \( \BbbK[\Lambda] \). The \term{characteristic polynomial} of \( T \) is defined as
  \begin{equation*}
    p_T(\Lambda) \coloneqq \det(T - \Lambda \id).
  \end{equation*}

  The \term{algebraic multiplicity} of \( \lambda \in \BbbK \) is the \hyperref[def:polynomial_root]{root multiplicity} of \( \lambda \) in \( p_A \).
\end{definition}

\begin{proposition}\label{thm:eigenvalues_and_characteristic_polynomials}
  The \hyperref[def:eigenpair]{eigenvalues} of a square matrix are precisely the \hyperref[def:polynomial_root]{roots} of its \hyperref[def:characteristic_polynomial]{characteristic polynomial}.
\end{proposition}
\begin{proof}
  Follows from \fullref{rem:eigenpairs_via_invertibility}.
\end{proof}

\begin{remark}\label{rem:characteristic_polynomial}
  We are, for the most part, only interested in the roots of a \hyperref[def:characteristic_polynomial]{characteristic polynomial}. For this reason, it is sometimes alternatively defined as
  \begin{equation*}
    \det(\Lambda \id - T).
  \end{equation*}

  This definition is used, for example, in \cite[74]{Knapp2016BasicAlgebra}. This ensures that the polynomial is \hyperref[def:monic_polynomial]{monic}.

  We prefer
  \begin{equation*}
    \det(T - \Lambda \id)
  \end{equation*}
  because it is the more popular definition and because it directly generalizes the determinant of \( A \).
\end{remark}

\begin{example}\label{ex:def:characteristic_polynomial}
  We list some examples of \hyperref[def:characteristic_polynomial]{characteristic polynomials}:
  \begin{thmenum}
    \thmitem{ex:def:characteristic_polynomial/zero} The characteristic polynomial of the \( n \times n \) zero matrix is \( (-\Lambda)^n \). Its only root is \( 0 \). Both the geometric and the algebraic multiplicities of \( 0 \) are \( n \).

    \thmitem{ex:def:characteristic_polynomial/identity} Similarly, the characteristic polynomial of the identity matrix \( I_n \) is \( (1 - \Lambda)^n \). Hence, the only eigenvalue of \( I_n \) is \( 1 \) and both of its multiplicities are \( n \).

    \thmitem{ex:def:characteristic_polynomial/2112} We continue \fullref{ex:def:eigenspace/2112}. The matrix
    \begin{equation*}
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
    \end{equation*}
    has characteristic polynomial
    \begin{equation*}
      (2 - \Lambda)^2 - 1 = \Lambda^2 - 4\Lambda + 3.
    \end{equation*}

    Its roots are
    \begin{equation*}
      \frac {4 \pm \sqrt{16 - 12}} 2 = 2 \pm 1.
    \end{equation*}

    Both roots have geometric and algebraic multiplicities \( 1 \).

    \thmitem{ex:def:characteristic_polynomial/1101} The matrix
    \begin{equation*}
      \begin{pmatrix}
        1 & 1 \\
        0 & 1
      \end{pmatrix}
    \end{equation*}
    has characteristic polynomial
    \begin{equation*}
      (1 - \Lambda)^2.
    \end{equation*}

    The only eigenvalue is hence \( 1 \), and it has algebraic multiplicity \( 2 \). We discussed in \fullref{ex:def:eigenspace/1101} that the corresponding geometric multiplicity is \( 1 \).
  \end{thmenum}
\end{example}

\begin{proposition}\label{thm:geometric_vs_algebraic_multiplicity}\mcite{MathSE:geometric_multiplicity_is_bounded_by_algebraic}
  The \hyperref[def:characteristic_polynomial]{geometric multiplicity} of an eigenvalue does not exceed its \hyperref[def:characteristic_polynomial]{algebraic multiplicity}.
\end{proposition}
\begin{proof}
  Suppose that the geometric multiplicity of \( \lambda \) with respect to \( T \) is \( m \). Then there exist linearly independent eigenvalues \( x_1, \ldots, x_m \) such that, for \( k = 1, \ldots, m \),
  \begin{equation*}
    T x_k = \lambda x_k.
  \end{equation*}

  \Fullref{thm:def:vector_space/expansion} allows us to expand this to a basis of \( \BbbK^n \) via some vectors \( x_{m+1}, \ldots, x_n \). With respect to this basis, the operator \( T \) has the matrix
  \begin{equation*}
    A = \begin{pmatrix}
      \lambda I_k & B \\
      0           & C
    \end{pmatrix},
  \end{equation*}
  for suitable matrices \( B \) and \( C \).

  The characteristic polynomial of this matrix is
  \begin{equation*}
    p_A(\Lambda) = (\lambda - \Lambda)^k p_C(\Lambda).
  \end{equation*}

  Therefore, the algebraic multiplicity of \( A \) is at least \( k \).
\end{proof}

\begin{definition}\label{rem:point_spectrum}\mcite[10.32]{Rudin1991Functional}
  The set of all \hyperref[def:eigenpair]{eigenvalues} of a linear endomorphism is called its \term{point spectrum}.
\end{definition}

\begin{lemma}\label{thm:eigenvectors_are_linearly_independent}
  Eigenvectors corresponding to different eigenvalues are linearly independent.
\end{lemma}
\begin{proof}
  Suppose that for some endomorphism \( T: V \to V \) we have distinct eigenvalues \( \lambda \) and \( \mu \), let \( e \) be an eigenvector corresponding to \( \lambda \) and \( f \) --- to \( \mu \).

  Let \( t e + r f = 0_V \) for some scalars \( t \) and \( r \). Then
  \begin{equation*}
    0_V = A(t e + r f) = t A e + r A f = t \lambda e + r \mu f.
  \end{equation*}

  Also, we can multiply \( t e + r f = 0_V \) by \( \lambda \) to obtain
  \begin{equation*}
    0_V = t \lambda e + r \lambda f.
  \end{equation*}

  We can now subtract the two to obtain
  \begin{equation*}
    0_V = r (\mu - \lambda) f.
  \end{equation*}

  Since \( \mu \neq \lambda \) and since \( f \) is by definition nonzero, we have \( r = 0 \). We similarly conclude that \( t = 0 \).

  Therefore, \( e \) and \( f \) are linearly independent.
\end{proof}

\begin{proposition}\label{thm:operator_diagonalizability}
  Let \( T: V \to V \) be a linear endomorphism over the vector space \( V \) over \( \BbbK \) of dimension \( n \). The following are equivalent:
  \begin{thmenum}
    \thmitem{thm:operator_diagonalizability/basis} The space \( V \) has a basis \( e_1, \ldots, e_n \) of eigenvectors of \( T \).

    \thmitem{thm:operator_diagonalizability/diagonal} There exists a basis \( e_1, \ldots, e_n \) of \( V \) such that \( T \) has a diagonal matrix with respect to this basis.

    The \( k \)-th diagonal entry of this matrix is then an eigenvalue of \( T \) for which \( e_k \) is an eigenvector.

    \thmitem{thm:operator_diagonalizability/multiplicities} There exist eigenvalues \( \lambda_1, \ldots, \lambda_m \) whose algebraic and geometric multiplicities coincide and sum to \( n \).

    \thmitem{thm:operator_diagonalizability/direct_sum} There exist eigenvalues \( \lambda_1, \ldots, \lambda_m \) such that
    \begin{equation*}
      V \cong \ker(T - \lambda_1) \oplus \cdots \oplus \ker(T - \lambda_m).
    \end{equation*}
  \end{thmenum}
\end{proposition}
\begin{proof}
  \ImplicationSubProof{thm:operator_diagonalizability/basis}{thm:operator_diagonalizability/diagonal} Suppose that there exists a basis \( e_1, \ldots, e_n \) of \( V \) such that \( T e_k = \lambda_k e_k \) for some eigenvalues \( \lambda_1, \ldots, \lambda_n \). Let
  \begin{equation*}
    A \coloneqq \op{diag}(\lambda_1, \ldots, \lambda_n).
  \end{equation*}

  Given an arbitrary vector \( x \) from \( V \), we have
  \begin{equation*}
    x = \sum_{k=1}^n \pi_{e_k}(x) \cdot e_k.
  \end{equation*}

  Define
  \begin{equation*}
    \begin{aligned}
      &\Phi: V \to \BbbK^n \\
      &\Phi(x) \coloneqq \begin{pmatrix}
        \pi_{e_1}(x) \\
        \vdots \\
        \pi_{e_n}(x) \\
      \end{pmatrix}.
    \end{aligned}
  \end{equation*}

  Then
  \begin{multline*}
    \Phi(T x)
    =
    \Phi\parens*{ T \parens*{ \sum_{k=1}^n \pi_{e_k}(x) \cdot e_k } }
    =
    \Phi\parens*{ \sum_{k=1}^n \pi_{e_k}(x) \cdot T e_k }
    = \\ =
    \Phi\parens*{ \sum_{k=1}^n \lambda_k \cdot \pi_{e_k}(x) \cdot e_k }
    =
    \begin{pmatrix}
      \lambda_1 \cdot \pi_{e_1}(x) \\
      \vdots \\
      \lambda_n \cdot \pi_{e_n}(x)
    \end{pmatrix}
    =
    A \cdot \Phi(x)
  \end{multline*}

  \ImplicationSubProof{thm:operator_diagonalizability/diagonal}{thm:operator_diagonalizability/basis} Conversely, suppose that
  \begin{equation*}
    A \coloneqq \op{diag}(\lambda_1, \ldots, \lambda_n)
  \end{equation*}
  is the matrix of \( T \) with respect to the basis \( e_1, \ldots, e_n \), i.e.
  \begin{equation*}
    \Phi(T x) = A \cdot \Phi(x).
  \end{equation*}

  Then
  \begin{equation*}
    A \cdot \Phi(e_k) = a_{k,\anon*} = \begin{pmatrix} 0 & \lambda_k & \end{pmatrix} = \lambda_k \Phi(e_k)
  \end{equation*}
  and
  \begin{equation*}
    T e_k = \Phi^{-1}(A \cdot \Phi(e_k)) = \lambda_k e_k.
  \end{equation*}

  Therefore, \( e_k \) is an eigenvector of \( T \) corresponding to \( \lambda_k \).

  \ImplicationSubProof{thm:operator_diagonalizability/basis}{thm:operator_diagonalizability/multiplicities} Suppose that there exists a basis \( e_1, \ldots, e_n \) of \( V \) such that \( Te_k = \lambda_k e_k \) for some eigenvalues \( \lambda_1, \ldots, \lambda_n \).

  If \( \lambda_{i_1} = \cdots = \lambda_{i_m} \) and all other eigenvalues are distinct from \( \lambda_{i_1} \), then
  \begin{equation*}
    \ker(T - \lambda_{i_1} \id) = \linspan\set{ e_{i_1}, \cdots, e_{i_m} }.
  \end{equation*}

  Thus, the algebraic and geometric multiplicities coincide. The sum of multiplicities over all distinct eigenvalues is obviously \( n \).

  \ImplicationSubProof{thm:operator_diagonalizability/multiplicities}{thm:operator_diagonalizability/direct_sum} Suppose that \( \lambda_1, \ldots, \lambda_m \) are the distinct eigenvalues of \( T \) and that their algebraic and geometric multiplicities coincide and sum to \( n \).

  For \( i \neq j \), the eigenvectors corresponding to \( \lambda_i \) are linearly independent from those corresponding to \( \lambda_j \) by \fullref{thm:eigenvectors_are_linearly_independent}. Thus, the union of some bases of \( \ker(T - \lambda_k) \) for all \( k = 1, \ldots, m \) is a basis of \( V \).

  By \fullref{thm:basis_of_direct_sum}, the union is also a basis of the direct sum
  \begin{equation*}
    \ker(T - \lambda_1) \oplus \cdots \oplus \ker(T - \lambda_m).
  \end{equation*}

  Therefore, this is an \hyperref[def:semimodule_direct_sum]{internal direct sum} giving \( V \).

  \ImplicationSubProof{thm:operator_diagonalizability/direct_sum}{thm:operator_diagonalizability/basis} Suppose that
  \begin{equation*}
    V \cong \ker(T - \lambda_1) \oplus \cdots \oplus \ker(T - \lambda_m).
  \end{equation*}

  Construct a basis of \( V \) by taking bases of the eigenspaces \( \ker(T - \lambda_k \id) \) together. This will be a basis of eigenvectors by \fullref{thm:basis_of_direct_sum}.
\end{proof}

\begin{definition}\label{def:diagonalizable_matrix}\mimprovised
  A square matrix is called \term{diagonalizable} if, as a linear endomorphism, it satisfies any of the conditions from \fullref{thm:operator_diagonalizability}.

  The condition \fullref{thm:operator_diagonalizability/diagonal} for the \( n \times n \) matrix \( A \) is then equivalent to the existence of a \hyperref[rem:change_of_basis]{change of basis} matrix \( P \) and a \hyperref[def:matrix_diagonal]{diagonal matrix} \( D \) such that
  \begin{equation*}
    A = P^{-1} D P.
  \end{equation*}

  It is thus necessary and sufficient for \( A \) to be \hyperref[def:similar_matrices]{similar} to a diagonal matrix. This is often taken as \enquote{the} definition of diagonalizability, however it hides the relationship with the theory of eigenvalues and eigenspaces presented in \fullref{thm:operator_diagonalizability}. That is, the \( k \)-th diagonal entry of \( D \) is an eigenvalue of \( A \) for which the \( k \)-th column of \( P \) is an eigenvector.
\end{definition}

\begin{definition}\label{def:adjoint_operator}\mcite[sec. 38.1]{Тыртышников2007}
  We say that the linear operator \( T^*: V \to U \) between \hyperref[def:inner_product_space]{inner product spaces} is \term{adjoint} to \( T: U \to V \) if for every \( x \in U \) and \( y \in V \) we have
  \begin{equation*}
    \inprod {Tx} y = \inprod x {T^* y}.
  \end{equation*}

  If an adjoint exists, it is unique, and in particular \( (T^*)^* = T \). But an adjoint may not exist.

  If \( U = V \) and \( T = T^* \), we say that the operator is \term{self-adjoint} or \term{symmetric}. In the case of a complex inner product space, we say that \( T \) is \enquote{\term{Hermitian}} instead of \enquote{symmetric}.

  Note that this definition holds for linear operators and the concept differs from the similar concepts for bilinear forms. The inner product itself is symmetric or Hermitian by definition.
\end{definition}
\begin{defproof}
  We will show uniqueness. Suppose that, for some linear maps \( R \) and \( S \), for any \( x \in U \) and \( y \in V \) we have
  \begin{equation*}
    \inprod {T x} y = \inprod x {R y} = \inprod x {S y}.
  \end{equation*}

  Then, for a fixed nonzero \( x_0 \) and for any \( y \), we have
  \begin{equation*}
    \inprod {x_0} {(R - S) y} = 0.
  \end{equation*}

  Since the inner product is nondegenerate, this is possible if \( R - S \) is the zero operator. Hence, \( R = S \).
\end{defproof}

\begin{proposition}\label{thm:image_of_adjoint}
  A linear operator \( T: U \to V \) and its \hyperref[def:adjoint_operator]{adjoint} \( T^*: V \to U \) have isomorphic images.

  In particular, the \hyperref[def:column_and_row_spaces]{column and row spaces} of a matrix are isomorphic.
\end{proposition}
\begin{proof}
  By \fullref{thm:quotient_structure_universal_property}, \( T \) \hyperref[def:factors_through]{factors through}
  \begin{equation*}
    U / \ker T \cong \img T.
  \end{equation*}

  More precisely, there exists an operator \( R: \img T \to V \) such that \( T = R \bincirc \pi \), where \( \pi \) is the projection into \( \img T \).

  We have \( T^* = \pi^* \bincirc R^* \). Hence, \( T^* \) also factors through \( \img T \), and thus
  \begin{equation*}
    \dim \img T^* \leq \dim \img T.
  \end{equation*}

  Conversely,
  \begin{equation*}
    \dim \img T = \dim \img (T^*)^* \leq \dim \img T^*.
  \end{equation*}

  Therefore, the dimensions of the images of \( T \) and \( T^* \) coincide, and hence the images are isomorphic.
\end{proof}

\begin{proposition}\label{thm:conjugate_transpose}
  The \hyperref[def:conjugate_transpose]{conjugate transpose} \( A^* \) of the matrix \( A \) corresponds to the \hyperref[def:adjoint_operator]{adjoint operator} of \( A \) (with respect to the \hyperref[def:inner_product_space]{dot product}).
\end{proposition}
\begin{proof}
  \begin{equation*}
    \inprod {Ax} {y}
    =
    (Ax)^* y
    =
    (x^* A^*) y
    =
    x^* (A^* y)
    =
    \inprod {x} {A^* y}.
  \end{equation*}
\end{proof}

\begin{definition}\label{def:invariant_subset}\mimprovised
  We say that the subset \( B \) of any \hyperref[def:set]{plain set} \( A \) is \term{invariant} under the endofunction \( f: A \to A \) if \( f[B] \subseteq B \).
\end{definition}
\begin{comments}
  \item Invariant subsets allow us to restrict \( f \) to an endofunction on \( B \).
  \item \enquote{Invariant} in this sense often refers to linear subspaces invariant under a linear map. Examples of such usage include \incite[218]{Knapp2016BasicAlgebra} and \incite[sec. 28.5]{Тыртышников2007}.
\end{comments}

\begin{proposition}\label{thm:invariant_subspace}
  Let \( T: V \to V \) be a linear operator. \hyperref[def:invariant_subset]{Invariant} under \( T \) subspaces of \( V \) have the following basic properties:
  \begin{thmenum}
    \thmitem{thm:invariant_subspace/kernel} The kernel of \( T \) is invariant.
    \thmitem{thm:invariant_subspace/image} The image of \( T \) is invariant.
    \thmitem{thm:invariant_subspace/complement} If \( T \) is \hyperref[def:adjoint_operator]{self-adjoint} and \( U \) is invariant under \( T \), so is its \hyperref[def:orthogonal_complement]{orthogonal complement} \( U^\perp \).
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:invariant_subspace/kernel} If \( x \in \ker T \), then \( Tx = 0 \in \ker T \).

  \SubProofOf{thm:invariant_subspace/image} If \( y \in \img T \), then \( Ty \in \img T \) because \( \img T \) is a subspace of \( V \).

  \SubProofOf{thm:invariant_subspace/complement} Let \( x \in U \) and \( y \in U^\perp \). We know that \( Tx \in U \) and thus \( \inprod {Tx} y = 0 \). If \( T \) is self-adjoint, then
  \begin{equation*}
    0 = \inprod {Tx} y = \inprod x {Ty},
  \end{equation*}
  and thus \( Ty \in U^\perp \).
\end{proof}

\begin{definition}\label{def:unitary_matrix}
  We say that a square real (resp. complex) matrix is \term{orthogonal} (resp. \term{unitary}) if any of the following conditions hold:
  \begin{thmenum}
    \thmitem{def:unitary_matrix/transpose} The \hyperref[def:transpose_matrix]{transpose matrix} (resp. \hyperref[def:conjugate_transpose]{conjugate transpose}) is also the \hyperref[def:inverse_matrix]{inverse matrix}.
    \thmitem{def:unitary_matrix/columns} The columns of the matrix are pairwise \hyperref[def:orthogonality]{orthonormal} with respect to the \hyperref[def:inner_product_space]{dot product}.
  \end{thmenum}
\end{definition}
\begin{defproof}
  \EquivalenceSubProof{def:unitary_matrix/transpose}{def:unitary_matrix/columns} Let
  \begin{equation*}
    A = \begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}.
  \end{equation*}

  Then
  \begin{equation*}
    A A^*
    =
    \begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}
    \begin{pmatrix} a_1^* & \cdots & a_n^* \end{pmatrix}
    =
    \begin{pmatrix}
      a_1 a_1^*  & \cdots & a_1 a_n^* \\
      \vdots     & \ddots & \vdots \\
      a_n a_1^*  & \cdots & a_n a_n^*
    \end{pmatrix}.
  \end{equation*}

  Hence, \( A A^* = I_n \) if and only if
  \begin{equation*}
    a_i a_j^* = \begin{cases}
      1, &i = j \\
      0, &i \neq j
    \end{cases}.
  \end{equation*}
\end{defproof}

\begin{definition}\label{def:unitary_groups}\mimprovised
  The \hyperref[def:unitary_matrix]{orthogonal} (resp. unitary) matrices form a subgroup of the \hyperref[def:linear_groups]{general linear group} \( \grp{GL}(n) \), which we denote by \( \grp{O}(n) \) (resp \( \op{U}(n) \)).

  The subgroup of orthogonal (resp. unitary) matrices with determinant \( 1 \) is called the \term{special orthogonal (resp. unitary) group} \( \grp{SO}(n) \) (resp. \( \grp{SU}(n) \)).
\end{definition}

\begin{definition}\label{def:unitary_operator}\mimprovised
  We say that a linear automorphism \( T: V \to V \) over the real (resp. complex) inner product space \( V \) is \term{orthogonal} (resp. \term{unitary}) if any of the following conditions hold:
  \begin{thmenum}
    \thmitem{def:unitary_operator/inverse} The \hyperref[def:adjoint_operator]{adjoint operator} \( T^*: V \to V \) exists and is the inverse of \( T \).
    \thmitem{def:unitary_operator/inner_product} We have
    \begin{equation*}
      \inprod {Tx} {Ty} = \inprod x y.
    \end{equation*}
  \end{thmenum}
\end{definition}
\begin{defproof}
  \ImplicationSubProof{def:unitary_operator/inverse}{def:unitary_operator/inner_product} If \( T^* = T^{-1} \), then
  \begin{equation*}
    \inprod {Tx} {Ty} = \inprod x {T^* T y} = \inprod x y.
  \end{equation*}

  \ImplicationSubProof{def:unitary_operator/inner_product}{def:unitary_operator/inverse} Conversely, suppose that
  \begin{equation*}
    \inprod {Tx} {Ty} = \inprod x y.
  \end{equation*}

  Then \( T^{-1} \) is the unique adjoint of \( T \) because
  \begin{equation*}
    \inprod x {T^{-1}y}
    =
    \inprod {Tx} {TT^{-1}y}
    =
    \inprod {Tx} y
    =
    \inprod x {T^* y}
    =
    \inprod x {T^* y}.
  \end{equation*}
\end{defproof}

\begin{theorem}[Finite-dimensional spectral theorem]\label{thm:finite_dimensional_spectral_theorem}
  Let \( V \) be a real (resp. complex) vector space of dimension \( n \).

  \begin{thmenum}
    \thmitem{thm:finite_dimensional_spectral_theorem/basis} Every \hyperref[def:transpose_matrix]{symmetric} (resp. \hyperref[def:conjugate_transpose]{Hermitian}) endomorphism on \( V \) induces an \hyperref[def:orthogonality]{orthonormal} basis of eigenvectors.

    \thmitem{thm:finite_dimensional_spectral_theorem/eigenvalues} Every symmetric (resp. Hermitian) endomorphism on \( V \) has \( n \) real eigenvalues (counting multiplicity).

    \thmitem{thm:finite_dimensional_spectral_theorem/matrix} Every symmetric (resp. Hermitian) \( n \times n \) matrix \( A \) is \hyperref[def:diagonalizable_matrix]{diagonalizable} via an \hyperref[def:unitary_matrix]{orthogonal} (resp. \hyperref[def:unitary_matrix]{unitary}) matrix. More precisely, we can decompose \( A \) as
    \begin{equation*}
      A = P^{-1} D P,
    \end{equation*}
    where \( D \) is a real diagonal matrix of eigenvalues and \( P \) is a unitary matrix whose columns are eigenvalues of \( A \).

    Furthermore, the \( k \)-th diagonal entry of \( D \) is an eigenvalue of \( A \) for which the \( k \)-th column of \( P \) is an eigenvector.
  \end{thmenum}
\end{theorem}
\begin{proof}
  We will only consider Hermitian matrices since the proof for symmetric matrices is identical.

  \SubProofOf{thm:finite_dimensional_spectral_theorem/basis} We will use induction on \( n \) to show that every Hermitian endomorphism on an \( n \)-dimensional space induces an orthonormal basis of eigenvectors. The case \( n = 1 \) is obvious.

  Suppose that the statement holds for \( n - 1 \), let \( V \) be an \( n \)-dimensional space and let \( T \) be an endomorphism on \( V \). By \fullref{thm:fundamental_theorem_of_algebra}, the characteristic polynomial has at least one eigenvalue. Let \( (\lambda, x) \) be an eigenpair, let \( e_1 \coloneqq x / \norm x \) and let
  \begin{equation*}
    U \coloneqq \linspan\set{ e_1 }.
  \end{equation*}

  The space \( U \) is invariant under \( T \) because
  \begin{equation*}
    T(t e_1) = t(T e_1) = (t\lambda) e_1.
  \end{equation*}

  By \fullref{thm:invariant_subspace/complement}, its \hyperref[def:orthogonality]{orthogonal complement} \( U^\perp \) is also invariant under \( T \). We can thus restrict \( T \) to \( U^\perp \). Furthermore, \( \dim U^\perp = n - 1 \) by \fullref{thm:direct_sum_with_orthogonal_complement} and \fullref{thm:rank_of_direct_sum}. Hence, by the inductive hypothesis, there exists an orthonormal basis \( e_2, \ldots, e_n \) of \( U^\perp \) of eigenvectors of \( T\restr_{U^\perp} \).

  Therefore, \( e_1, \ldots, e_n \) is an orthonormal basis of \( \BbbC^n = U \oplus U^\perp \) consisting of eigenvectors of \( T \).

  \SubProofOf{thm:finite_dimensional_spectral_theorem/eigenvalues}
  \SubProof*{Proof for complex numbers} Every characteristic polynomial of degree \( n \) over \( \BbbC \) has \( n \) roots, counting multiplicity. For every eigenpair \( (\lambda, x) \) of \( T: V \to V \) we have
  \begin{equation*}
    \lambda \inprod x x
    =
    \inprod {\lambda x} x
    =
    \inprod {T x} x
    =
    \inprod x {T x}
    =
    \inprod x {\lambda x}
    =
    \overline \lambda \inprod x x.
  \end{equation*}

  Since inner products are positive definite, \( \inprod x x \) is a positive real number and hence we can cancel it to obtain the equality \( \lambda = \overline \lambda \). Hence, if \( \lambda \) is an eigenvalue, it is a real number.

  \SubProof*{Proof for real numbers} If \( V \) is instead a real vector space, the characteristic polynomial \( p_T(\Lambda) \) of \( T \) belongs to \( \BbbR[\Lambda] \). It can be \hyperref[thm:polynomial_algebra_universal_property]{evaluated} over \( \BbbC \), which will give us \( n \) roots over \( \BbbC \). But these roots are real, hence \( p_T(\Lambda) \) has \( n \) roots over \( \BbbR \). These are the eigenvalues of \( T \).

  \SubProofOf{thm:finite_dimensional_spectral_theorem/matrix} Follows from \fullref{thm:finite_dimensional_spectral_theorem/basis}, \fullref{thm:finite_dimensional_spectral_theorem/eigenvalues} and \fullref{thm:operator_diagonalizability}.
\end{proof}

\begin{definition}\label{def:singular_value}
  We say that \( \sigma \) is a \term{singular value} of the operator \( T: U \to V \) if \( \sigma^2 \) is an eigenvalue of \( T^* \bincirc T \). \Fullref{thm:finite_dimensional_spectral_theorem/eigenvalues} implies that singular values are always real.

  Note that this definition implicitly assumes the existence of an adjoint operator.
\end{definition}

\begin{theorem}[Singular value decomposition]\label{thm:singular_value_decomposition}
  \hfill
  \begin{thmenum}
    \thmitem{thm:singular_value_decomposition/basis} For every linear operator \( T: U \to V \) between finite-dimensional real or complex vector spaces, there exist orthonormal bases of \( U \) and \( V \) such that the matrix of \( T \) with respect to them is diagonal with the \hyperref[def:singular_value]{singular values} of \( T \) on its diagonal.

    If \( m \leq n \), this matrix has the form
    \begin{equation}\label{eq:thm:singular_value_decomposition/matrix}
      \Sigma
      =
      \begin{pmatrix}
        \sigma_1 & 0        & \cdots & 0        & 0 & \cdots & 0 \\
        0        & \sigma_2 & \cdots & 0        & 0 & \cdots & 0 \\
        \vdots   & \vdots   & \ddots & 0        & 0 & \cdots & 0 \\
        0        & 0        & 0      & \sigma_m & 0 & \cdots & 0
      \end{pmatrix}.
    \end{equation}

    \thmitem{thm:singular_value_decomposition/matrix} For every \( m \times n \) real or complex matrix \( A \), there exists an \( m \times m \) \hyperref[def:unitary_matrix]{unitary matrix} \( U \) and an \( n \times n \) unitary matrix \( V \) such that
    \begin{equation*}
      A = U \Sigma V^*,
    \end{equation*}
    where \( \Sigma \) is a diagonal matrix with the singular values of \( A \) on its diagonal.
  \end{thmenum}
\end{theorem}
\begin{proof}
  \SubProofOf{thm:singular_value_decomposition/basis} \Fullref{thm:finite_dimensional_spectral_theorem} gives us a basis \( e_1, \ldots, e_n \) of \( U \) of eigenvectors of \( T^* \bincirc T \). Suppose that only \( e_1, \ldots, e_r \) have nonzero eigenvalues and let \( \sigma_1, \ldots, \sigma_r \) be the corresponding singular values, i.e.
  \begin{equation*}
    [T^* \bincirc T] e_k = \sigma_k^2 e_k.
  \end{equation*}

  We are interested in the kernel of \( T \). Suppose that \( Tx = 0_V \). Then
  \begin{equation*}
    0_U
    =
    [T^* \bincirc T] x
    =
    \sum_{k=1}^n \inprod { [T^* \bincirc T]x } { e_k }_U e_k
    =
    \sum_{k=1}^n \sigma_k^2 \inprod { x } { e_k }_U e_k
  \end{equation*}

  We have \( \sigma_k^2 \inprod { x } { e_k }_U = 0 \), which implies \( \inprod x { e_k }_U = 0 \) for \( k \leq r \). Thus, \( x \) is a linear combination of \( e_{r+1}, \ldots, e_n \), which implies that \( e_{k+1}, \ldots, e_n \) is a basis of \( \ker T \).

  We will now construct a basis. For \( k = 1, \ldots, r \) define
  \begin{equation*}
    f_k \coloneqq \frac 1 {\sigma_k} T e_k.
  \end{equation*}

  Since
  \begin{equation*}
    T x = \sum_{k=1}^n \inprod x { e_k } T e_k
  \end{equation*}
  and since \( T e_{r+1} = \cdots = T e_n = 0_V \), we conclude that the vectors \( f_1, \ldots, f_r \) span \( \img T \). They are also pairwise orthogonal:
  \begin{equation*}
    \inprod { f_i } { f_j }_V
    =
    \frac 1 {\sigma_j^2} \inprod { T e_i } { T e_j }_V
    =
    \frac 1 {\sigma_j^2} \inprod { e_i } { [T^* \bincirc T] e_j }_U
    =
    \frac 1 {\sigma_j^2} {\sigma_j^2} \inprod { e_i } { e_j }_U
    =
    \begin{cases}
      1, &i = j \\
      0, &\T{otherwise.}
    \end{cases}
  \end{equation*}

  Therefore, \( f_1, \ldots, f_r \) is a basis of \( \img T \). \Fullref{thm:image_of_adjoint} implies that \( \img T \cong \img T^* \) and \fullref{thm:rank_nullity_theorem} and \fullref{thm:basis_of_direct_sum} imply that it is sufficient to take an orthonormal basis \( f_{r+1}, \ldots, f_m \) of \( \ker T^* \) in order for \( f_1, \ldots, f_m \) to be an orthonormal basis of \( V \).

  The \( (i, j) \)-th entry of the matrix of \( T \) with respect to the bases \( e_1, \ldots, e_n \) and \( f_1, \ldots, f_m \) is \( \inprod { T e_i } { f_j }_V \).
  \begin{itemize}
    \item If \( i \leq r \), then
    \begin{equation*}
      \inprod { T e_i } { f_j }_V
      =
      \sigma_i \inprod { f_i } { f_j }_V
      =
      \begin{cases}
        \sigma_i, &i = j \\
        0,        &i \neq j \\
      \end{cases}
    \end{equation*}

    \item If \( i > r \), then \( e_i \) is in the kernel of \( T \) and hence
    \begin{equation*}
      \inprod { T e_i } { f_j }_V
      =
      \inprod { 0_V } { f_j }_V
      =
      0.
    \end{equation*}
  \end{itemize}

  Therefore, the desired matrix has the form \eqref{eq:thm:singular_value_decomposition/matrix}.

  \SubProofOf{thm:singular_value_decomposition/matrix} Follows from \fullref{thm:singular_value_decomposition/basis} by defining
  \begin{equation*}
    U \coloneqq \parens*
    {
      \begin{array}{c|c|c}
        f_1 & \cdots & f_m
      \end{array}
    }
  \end{equation*}
  and
  \begin{equation*}
    V \coloneqq \parens*
    {
      \begin{array}{c|c|c}
        e_1 & \cdots & e_n
      \end{array}
    }.
  \end{equation*}

  Denote by \( u_{i,j} \), \( v_{i,j} \) and \( s_{i,j} \) the \( (i, j) \)-th entries of \( U \), \( V \) and \( \Sigma \). Note that
  \begin{equation*}
    e_k = \begin{pmatrix} v_{k,1} \\ \cdots \\ v_{k,n} \end{pmatrix}
  \end{equation*}
  and
  \begin{equation*}
    f_k = \begin{pmatrix} u_{1,k} \\ \cdots \\ u_{m,k} \end{pmatrix}.
  \end{equation*}

  Since for \( k \leq r \) we have \( \sigma_k f_k = A e_k \), for the \( i \)-th entry of \( \sigma_k f_k \) we have
  \begin{equation}\label{eq:thm:singular_value_decomposition/matrix/scalars}
    \sigma_k u_{i,k} = \sum_{l=1}^n a_{i,l} v_{k,l}.
  \end{equation}

  We have
  \begin{equation*}
    s_{i,j} = \begin{cases}
      \sigma_i, &i = j \leq r \\
      0,        &\T{otherwise.}
    \end{cases}
  \end{equation*}

  Then \( U \Sigma \) is an \( m \times n \) matrix whose \( (i, j) \)-th entry is
  \begin{equation*}
    \sum_{k=1}^m u_{i,k} s_{k,j}
    =
    \begin{cases}
      u_{i,j} \sigma_j, &j \leq r \\
      0,                &\T{otherwise.}
    \end{cases}
  \end{equation*}

  It is only nonzero if \( j \leq r \). Then the \( (i, j) \)-th entry of \( U \Sigma V^* \) is
  \begin{equation*}
    \sum_{k=1}^r u_{i,k} \sigma_k v_{k,j}
    \reloset {\eqref{eq:thm:singular_value_decomposition/matrix/scalars}} =
    \sum_{k=1}^r \sum_{l=1}^n a_{i,l} v_{k,l} v_{k,l}
    =
    \sum_{j=1}^n a_{i,j} \underbrace{ \sum_{k=1}^r v_{k,i} v_{k,j} }_{ v_i^* v_j }
    =
    a_{i,j}.
  \end{equation*}

  Therefore, \( A = U \Sigma V^* \).
\end{proof}
