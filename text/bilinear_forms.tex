\subsection{Bilinear forms}\label{subsec:bilinear_forms}

In this subsection, we restrict ourselves to fields rather than arbitrary rings.

We define \hyperref[def:bilinear_form]{bilinear forms} over arbitrary fields, although they are almost exclusively used over the field of real numbers. In the latter case, they are a special case of \hyperref[def:sesquilinear_form]{sesquilinear forms} over complex numbers. This relationship is made precise via \hyperref[def:complexification]{complexification}. This is discussed further in \fullref{rem:real_field_extensions}.

\begin{definition}\label{def:bilinear_form}\mcite[249]{Knapp2016BasicAlgebra}
  A \term{bilinear form} over the vector space \( V \) over \( \BbbK \) is a bilinear form is a \hyperref[def:multilinear_function]{multilinear function} with signature \( L: V \times V \to \BbbK \).
\end{definition}

\begin{remark}\label{rem:matrices_as_bilinear_forms}
  Similarly to what we discussed in \fullref{rem:matrices_as_functions}, matrices correspond to linear functions and vice versa. Square matrices correspond to \hyperref[def:bilinear_form]{bilinear forms}.

  Let \( e_1, \ldots, e_n \) be the \hyperref[def:sequence_space]{standard basis} of \( \BbbK^n \). This basis allows us to identify vectors of \( \BbbK^n \) with column vectors. To every \( n \times n \) matrix \( A \), there corresponds a bilinear form
  \begin{equation*}
    L_A(x, y) \coloneqq x^T A y.
  \end{equation*}

  Similarly, given a bilinear form \( L \), we can build the following matrix:
  \begin{equation*}
    A_L \coloneqq
    \begin{pmatrix}
      L(e_1, e_1) & \cdots & L(e_1, e_n) \\
      \vdots      & \ddots & \vdots      \\
      L(e_n, e_1) & \cdots & L(e_n, e_n)
    \end{pmatrix}.
  \end{equation*}

  This matrix is called the generalized \term{Gram matrix} corresponding to \( L \).
\end{remark}

\begin{proposition}\label{thm:symmetric_bilinear_form_matrix}
  The \hyperref[rem:matrices_as_bilinear_forms]{Gram matrix} of a \hyperref[def:symmetric_function]{symmetric} \hyperref[def:bilinear_form]{bilinear form} is \hyperref[def:transpose_matrix]{symmetric}.
\end{proposition}
\begin{proof}
  Trivial.
\end{proof}

\begin{definition}\label{def:bilinear_form_radicals}\mcite[250]{Knapp2016BasicAlgebra}
  Let \( L: V \times V \to \BbbK \) be a bilinear form. We define its \term{left radical}
  \begin{equation*}
    \set{ x \in V \given \qforall {y \in V} L(x, y) = 0 }
  \end{equation*}
  and its \term{right radical}
  \begin{equation*}
    \set{ y \in V \given \qforall {x \in V} L(x, y) = 0 }
  \end{equation*}

  Note that if \( L \) is symmetric or skew-symmetric, the two radicals are identical, and we speak simply of the \term{radical} \( \sqrt L \).
\end{definition}

\begin{definition}\label{def:degenerate_bilinear_form}\mcite[249]{Knapp2016BasicAlgebra}
  We say that a \hyperref[def:bilinear_form]{bilinear form} is \term{degenerate} if either its left or right \hyperref[def:bilinear_form_radicals]{radical} is not trivial.
\end{definition}

\begin{example}\label{ex:def:bilinear_form}\hfill
  \begin{thmenum}
    \thmitem{ex:def:bilinear_form/asymmetric_degenerate} The matrix
    \begin{equation*}
      \begin{pmatrix}
        0 & 1 \\
        0 & 0
      \end{pmatrix}
    \end{equation*}
    corresponds to a \hyperref[def:degenerate_bilinear_form]{degenerate} \hyperref[def:bilinear_form]{bilinear form}. Its \hyperref[def:bilinear_form_radicals]{left radical} is
    \begin{equation*}
      \set*{ \begin{pmatrix} 0 \\ r \end{pmatrix} \given* r \in \BbbK }.
    \end{equation*}

    Its right radical is
    \begin{equation*}
      \set*{ \begin{pmatrix} r \\ 0 \end{pmatrix} \given* r \in \BbbK }.
    \end{equation*}

    \thmitem{ex:def:bilinear_form/symmetric_degenerate} The matrix
    \begin{equation*}
      \begin{pmatrix}
        1 & 0 \\
        0 & 0
      \end{pmatrix}
    \end{equation*}
    is also degenerate. It is symmetric, however, and its left and right radicals coincide with
    \begin{equation*}
      \set*{ \begin{pmatrix} 0 \\ r \end{pmatrix} \given* r \in \BbbK }.
    \end{equation*}

    \thmitem{ex:def:bilinear_form/euclidean} The identity matrix induces the nondegenerate bilinear form \( (x, y) \mapsto x^T y \). It is called the \term{Euclidean product}.
  \end{thmenum}
\end{example}

\begin{definition}\label{def:homogeneous_polynomial}\mimprovised
  We say that a \hyperref[def:polynomial_algebra]{polynomial} is \term{homogeneous} of degree \( n \) if all of its monomials have degree \( n \).
\end{definition}

\begin{definition}\label{def:homogenous_function}\mimprovised
  We say that the function \( f: V \to \BbbK \) is \term{homogeneous} of degree \( n \) if
  \begin{equation*}
    f(t x) = t^n f(x).
  \end{equation*}

  This is a generalization of \hyperref[eq:def:semimodule/homomorphism/homogeneity]{homogeneity} used in the definition of linear maps.
\end{definition}

\begin{proposition}\label{thm:homogeneous_polynomial_is_homogeneous_function}
  A \hyperref[def:homogeneous_polynomial]{homogeneous polynomial} is a \hyperref[def:homogenous_function]{homogeneous function} of the same degree.
\end{proposition}
\begin{proof}
  Trivial.
\end{proof}

\begin{proposition}\label{thm:polarization_identity}\mcite{nLab:polarization_identity}
  Let \( L: V \times V \to \BbbK \) be a \hyperref[def:symmetric_function]{symmetric} \hyperref[def:bilinear_form]{bilinear form} and define \( Q(x) \coloneqq L(x, x) \). Then the \term{polarization identity} holds:
  \begin{equation}\label{thm:polarization_identity/polarization_identity}
    Q(x + y) - Q(x - y) = 2 L(x, y) + 2 L(y, x)
  \end{equation}

  The similar looking, but slightly less useful parallelogram law also holds:
  \begin{equation}\label{thm:polarization_identity/parallelogram_law}
    Q(x + y) + Q(x - y) = 2 Q(x) + 2 Q(y)
  \end{equation}

  We can \enquote{recover} \( L \) from \( Q \):
  \begin{equation}\label{thm:polarization_identity/definition}
    L(x, y) \coloneqq \frac 1 2 \bracks{ Q(x + y) - Q(x) - Q(y) }.
  \end{equation}
\end{proposition}
\begin{proof}
  The identities all follow from the bilinearity of \( L \), that is,
  \begin{equation*}
    Q(x \pm y)
    =
    L(x, x) \pm L(x, y) \pm L(y, x) + L(y, y)
    =
    [Q(x) + Q(y)] \pm [L(x, y) + L(y, x)].
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:quadratic_forms}
  There is a bijective correspondence between \hyperref[def:symmetric_function]{symmetric} \hyperref[def:bilinear_form]{bilinear forms} and \hyperref[def:homogeneous_polynomial]{homogeneous} \hyperref[def:polynomial_degree]{quadratic polynomials}.

  A \term{quadratic form} \( Q: V \to \BbbK \) is defined as either \( Q(x) \coloneqq L(x, x) \) for a symmetric bilinear form \( L \), or as the \hyperref[thm:polynomial_algebra_universal_property]{polynomial function} of a homogeneous quadratic polynomial.

  \hi{Real} quadratic forms are simply quadratic forms over \( \BbbK = \BbbR \). \hi{Complex} quadratic forms, by contrast, differ from the general theory --- see \fullref{rem:complex_quadratic_form}.
\end{proposition}
\begin{proof}
  First, let \( L: V \times V \to \BbbK \) be a bilinear form. Let \( e_k, k \in \mscrK, \) be a \hyperref[def:hamel_basis]{basis} of \( V \). Define the polynomial
  \begin{equation*}
    p_L(X_k \given k \in \mscrK) \coloneqq \sum_{i \in \mscrK} \sum_{j \in \mscrK} L(e_i, e_j) X_i X_j.
  \end{equation*}

  Conversely, let \( p(X_k \given k \in \mscrK) \) be a homogeneous quadratic polynomial over the set of indeterminates \( \mscrX \). Via \eqref{thm:polarization_identity/definition}, we can define
  \begin{equation*}
    L_p(x, y) = \frac 1 2 \bracks{ p(x + y) - p(x) - p(y) },
  \end{equation*}
  where \( x \) and \( y \) are vectors from \( V \) (i.e. \( \mscrX \)-indexed tuples).

  Given a symmetric bilinear form \( L \), we have
  \begin{balign*}
    &\phantom{{}={}}
    L_{p_L}(x, y)
    = \\ &=
    \frac 1 2 \parens*{ \sum_{i \in \mscrK} \sum_{j \in \mscrK} L(e_i, e_j) (x_i + y_i) (x_j + y_j) - \sum_{i \in \mscrK} \sum_{j \in \mscrK} L(e_i, e_j) x_i x_j - \sum_{i \in \mscrK} \sum_{j \in \mscrK} L(e_i, e_j) y_i y_j }
    = \\ &=
    \frac 1 2 \sum_{i \in \mscrK} \sum_{j \in \mscrK} L(e_i, e_j) x_i y_j + \frac 1 2 \sum_{i \in \mscrK} \sum_{j \in \mscrK} L(e_i, e_j) y_i x_j
    = \\ &=
    \sum_{j \in \mscrK} L\parens*{ \sum_{i \in \mscrK} x_i e_i, \sum_{j \in \mscrK} y_j e_j }
    = \\ &=
    L(x, y).
  \end{balign*}

  Conversely, given a homogeneous quadratic polynomial \( p \), we have
  \begin{balign*}
    &\phantom{{}={}}
    p_{L_p}(X_k \given k \in \mscrK)
    = \\ &=
    \sum_{i \in \mscrK} \sum_{j \in \mscrK} L_p(e_i, e_j) X_i X_j
    = \\ &=
    \sum_{i \in \mscrK} \sum_{j \in \mscrK} L_p(e_i, e_j) X_i X_j
    = \\ &=
    \frac 1 2 \sum_{i \in \mscrK} \sum_{j \in \mscrK} \bracks{ p(e_i + e_j) - p(e_i) - p(e_j) } X_i X_j.
  \end{balign*}

  By definition, all \hyperref[def:basis_decomposition]{coordinate projections} of \( e_i \) are zero except for the \( i \)-th coordinate, which is one. Hence, the value \( p(e_i) \) is the coefficient before \( X_i^2 \) in \( p \), and similarly for \( p(e_j) \). The value \( p(e_i + e_j) \) is the sum of coefficients before \( X_i^2 \), \( X_i X_j \) and \( X_j^2 \). Therefore,
  \begin{equation*}
    p(e_i + e_j) - p(e_i) - p(e_j)
  \end{equation*}
  is the coefficient before \( X_i X_j \). Furthermore, \( X_i X_j \) is equal to \( X_j X_i \), and so are their coefficients, which allows us to cancel \( \ifrac 1 2 \) above. Thus,
  \begin{equation*}
    p_{L_p}(X_k \given k \in \mscrK) = p(X_k \given k \in \mscrK).
  \end{equation*}
\end{proof}

\begin{definition}\label{def:complexification}\mcite[def. 2.1]{Conrad2020Complexification}
  Let \( V \) be a real vector space. The \term{complexification} \( V^\BbbC \) of \( V \) is the \hyperref[def:semimodule_direct_product]{direct sum} \( V \oplus V \) equipped with the canonical inclusion
  \begin{equation*}
    \begin{aligned}
      &\iota_V: V \to V \oplus V, \\
      &\iota_V(x) \coloneqq (x, 0_V).
    \end{aligned}
  \end{equation*}

  We regard it as a complex vector space by defining scalar multiplication as
  \begin{equation*}
    (a + bi) \cdot (x, y) = (ax - by, bx + ay).
  \end{equation*}

  To avoid working with ordered pairs, we use that
  \begin{equation*}
    (x, y) = \iota(x) + i \cdot \iota(y).
  \end{equation*}

  Conversely, given a complex vector space \( W \), we define its \term{decomplexification} \( W^\BbbR \) as the same underlying Abelian group but with scalars restricted to real numbers.

  Therefore, \( \iota \) embeds \( V \) into \( (V^\BbbC)^\BbbR = V \oplus V \).
\end{definition}

\begin{proposition}\label{thm:basis_of_complexification}
  If \( B \) is a basis of the real vector space \( V \), then
  \begin{equation*}
    B^\BbbC \coloneqq \set{ \iota(b) \given b \in B }
  \end{equation*}
  is a basis of its \hyperref[def:complexification]{complexification} \( V^\BbbC \).
\end{proposition}
\begin{proof}
  It is obvious that the vectors in \( B^\BbbC \) are linearly independent. We have to show that they span \( V^\BbbC \).

  Let \( (x, y) \) be a vector of \( V^\BbbC = V \oplus V \). Then
  \begin{equation*}
    (x, y)
    =
    \iota(x) + i \cdot \iota(y)
    =
    \iota\parens*{ \sum_{b \in B} \pi_b(x) \cdot b } + i \cdot \iota\parens*{ \sum_{b \in B} \pi_b(y) \cdot b }
    =
    \sum_{b \in B} \parens[\Big]{ \pi_b(x) + i \cdot \pi_b(y) } \cdot \iota(b).
  \end{equation*}
\end{proof}

\begin{theorem}[Complexification universal property]\label{thm:complexification_universal_property}
  The \hyperref[def:complexification]{complexification} \( V^\BbbC \) of a real vector space \( V \) satisfies the following \hyperref[rem:universal_mapping_property]{universal mapping property}:
  \begin{displayquote}
    For every complex vector space \( W \) and every real linear map \( T: V \to W^\BbbR \), there exists a unique complex linear map \( T^\BbbC: V^\BbbC \to W \) such that the following diagram commutes:
    \begin{equation}\label{eq:thm:complexification_universal_property/diagram}
      \begin{aligned}
        \includegraphics[page=1]{output/thm__complexification_universal_property.pdf}
      \end{aligned}
    \end{equation}
  \end{displayquote}

  Note that \( W^\BbbR \) and \( W \) have the same underlying abelian group but the scalar multiplication in \( W^\BbbR \) is restricted to real numbers.

  Via \fullref{rem:universal_mapping_property}, \( (\anon*)^\BbbC \) becomes \hyperref[def:category_adjunction]{left adjoint} to the \hyperref[def:complexification]{decomplexification} \hyperref[def:concrete_category]{forgetful functor}
  \begin{equation*}
    (\anon*)^\BbbR: \cat{Vect}_\BbbC \to \cat{Vect}_\BbbR.
  \end{equation*}
\end{theorem}
\begin{proof}
  Given \( T: V \to W^\BbbR \) and \( x \in V \), the map \( T^\BbbC \) must satisfy
  \begin{equation*}
    T^\BbbC(x) = Tx.
  \end{equation*}

  This suggests the only possible definition
  \begin{equation*}
    \begin{aligned}
      &T^\BbbC: V^\BbbC \to W, \\
      &T^\BbbC(x + i \cdot y) \coloneqq Tx + i \cdot Ty.
    \end{aligned}
  \end{equation*}
\end{proof}

\begin{definition}\label{def:antilinear_function}\mimprovised
  We say that the function \( L: V \to W \) between complex vector spaces is \term{antilinear} if it satisfies the additivity condition \eqref{eq:def:semimodule/homomorphism/additive} and if
  \begin{equation}\label{eq:def:antilinear_function}
    L(tx) = \overline t L(x).
  \end{equation}

  That is, we enhance the homogeneity condition \eqref{eq:def:semimodule/homomorphism/homogeneity} from \fullref{def:semimodule/homomorphism} with \hyperref[def:complex_numbers]{complex conjugation}.
\end{definition}

\begin{definition}\label{def:sesquilinear_form}\mcite[258]{Knapp2016BasicAlgebra}
  A \term{sesquilinear\footnote{\enquote{sesqui} is a Latin prefix meaning \enquote{one and a half}} form} over the \hyperref[def:complex_numbers]{complex} vector space \( V \) is a function \( L: V \times V \to \BbbC \) that is \hyperref[def:semimodule/homomorphism]{linear} in the first argument and \hyperref[def:antilinear_function]{antilinear} in the second.

  Unlike bilinear forms, we have \( L(x, ty) = \overline t L(x, y) \) rather than \( L(x, ty) = t L(x, y) \). Sesquilinear forms coincide with bilinear forms when restricted to real numbers.
\end{definition}

\begin{definition}\label{def:hermitian_form}\mcite[258]{Knapp2016BasicAlgebra}
  A \hyperref[def:sesquilinear_form]{sesquilinear form} \( L: V \times V \to \BbbC \) is called \term{Hermitian} if
  \begin{equation*}
    L(x, y) = \overline{L(y, x)}.
  \end{equation*}

  Hermitian forms coincide with symmetric forms when restricted to real numbers.

  This should not be confused with \hyperref[def:adjoint_operator]{Hermitian linear operators}.
\end{definition}

\begin{proposition}\label{thm:complexification_of_symmetric_bilinear_form}
  If \( L: V \times V \to \BbbR \) is a real \hyperref[def:symmetric_function]{symmetric} \hyperref[def:bilinear_form]{bilinear form}, then its \hyperref[def:complexification]{complexification}
  \begin{equation*}
    \begin{aligned}
      &L^\BbbC: V^\BbbC \times V^\BbbC \to \BbbC, \\
      &L^\BbbC\parens[\Big]{ \iota(x) + i \cdot \iota(y), \iota(u) + i \cdot \iota(v) } \coloneqq L(x, u) + L(y, v) - i \cdot L(x, v) + i \cdot L(u, y).
    \end{aligned}
  \end{equation*}
  is a \hyperref[def:hermitian_form]{Hermitian form}.

  Furthermore,
  \begin{equation*}
    L^\BbbC(\iota(x), \iota(y)) = L(x, y).
  \end{equation*}
\end{proposition}
\begin{proof}
  The additivity of \( L^\BbbC = L \oplus L \) follows from the additivity of \( L \).

  Note that
  \begin{equation*}
    (a + bi) (\iota(x) + i \cdot \iota(y)) = \iota(ax - by) + i \cdot \iota(bx + ay).
  \end{equation*}

  By the homogeneity and symmetry of \( L \), we have linearity in the first argument:
  \begin{balign*}
    &\phantom{{}={}}
    L^\BbbC \parens[\Big]{ (a + bi) \parens[\Big]{ \iota(x) + i \cdot \iota(y) }, \iota(u) + i \cdot \iota(v) }
    = \\ &=
    L(ax - by, u) + L(bx + ay, v) - i \cdot L(ax - by, v) + i \cdot L(u, bx + ay)
    = \\ &=
    a L(x, u) - b L(y, u) + b L(x, v) + a L(y, v) - ai L(x, v) + bi L(y, v) + bi L(u, x) + ai L(u, y)
    = \\ &=
    (a + bi) L(x, u) + i(a + bi) L(u, y) - i(a + bi) L(x, v) + (a + bi) L(y, v)
    = \\ &=
    (a + bi) \cdot L^\BbbC \parens[\Big]{ \iota(x) + i \cdot \iota(y), \iota(u) + i \cdot \iota(v) }
  \end{balign*}
  and antilinearity in the second:
  \begin{balign*}
    &\phantom{{}={}}
    L^\BbbC \parens[\Big]{ \iota(x) + i \cdot \iota(y), (a + bi) \parens[\Big]{ \iota(u) + i \cdot \iota(v) } }
    = \\ &=
    L(x, au - bv) + L(y, bu + av) - i \cdot L(x, bu + av) + i \cdot L(au - bv, y)
    = \\ &=
    a L(x, u) - b L(x, v) + b L(y, u) + a L(y, v) - bi L(x, u) - ai L(x, v) + ai L(u, y) - bi L(v, y)
    = \\ &=
    (a - bi) L(x, u) - i (a - bi) L(x, v) + i (a - bi) L(u, y) + (a - bi) L(y, v)
    = \\ &=
    (a - bi) \cdot L^\BbbC \parens[\Big]{ \iota(x) + i \cdot \iota(y), \iota(u) + i \cdot \iota(v) }.
  \end{balign*}

  The Hermitian property follows easily.
\end{proof}

\begin{definition}\label{def:conjugate_transpose}
  The \term{conjugate transpose} \( A^* \) of the matrix \( A \) over the complex numbers is the \hyperref[def:transpose_matrix]{transpose matrix} in which we take the complex conjugate of every entry.

  If \( A = A^* \), we say that the matrix is \term{Hermitian}.
\end{definition}

\begin{remark}\label{rem:complex_quadratic_form}
  \hyperref[thm:quadratic_forms]{Quadratic forms} can be defined for complex numbers in multiple ways. The obvious definition is to take the base field to be \( \BbbC \). The more popular definition is presented below.

  Let \( L: V \times V \to \BbbC \) be a \hyperref[def:hermitian_form]{Hermitian} \hyperref[def:sesquilinear_form]{sesquilinear form}. Exchanging its parameters yields
  \begin{equation*}
    L(x, x) = \overline {L(x, x)},
  \end{equation*}
  which ensures that \( Q(x) \coloneqq L(x, x) \) is always a real number. Thus, we regard \( Q \) as a function from \( V \) to \( \BbbR \). This is not a quadratic form in the sense of \fullref{thm:quadratic_forms}, but we will call it a \term{complex quadratic form}.

  The definition via homogeneous quadratic polynomials is no longer compatible.
\end{remark}

\begin{definition}\label{thm:quadratic_forms_definiteness}\mimprovised
  We say that the real or \hyperref[rem:complex_quadratic_form]{complex quadratic form} \( Q: V \to \BbbR \) is
  \begin{thmenum}
    \thmitem{thm:quadratic_forms_definiteness/positive_semidefinite} \term{positive semidefinite} if \( Q(x) \geq 0 \) for all \( x \in V \).
    \thmitem{thm:quadratic_forms_definiteness/negative_semidefinite} \term{negative semidefinite} if \( Q(x) \leq 0 \) for all \( x \in V \).
    \thmitem{thm:quadratic_forms_definiteness/positive_definite} \term{positive definite} if \( Q(x) > 0 \) for all \( x \neq 0_V \).
    \thmitem{thm:quadratic_forms_definiteness/negative_definite} \term{negative definite} if \( Q(x) < 0 \) for all \( x \neq 0_V \).
    \thmitem{thm:quadratic_forms_definiteness/indefinite} \term{indefinite} otherwise.
  \end{thmenum}

  The above terminology also applies to symmetric bilinear or Hermitian sesquilinear forms, since they can be used to obtain a quadratic form.
\end{definition}

\begin{proposition}\label{thm:quadratic_forms_are_nondegenerate}
  \hyperref[thm:quadratic_forms_definiteness]{Definite} \hyperref[thm:quadratic_forms]{quadratic forms} are \hyperref[def:degenerate_bilinear_form]{nondegenerate}.
\end{proposition}
\begin{proof}
  In the real case, the quadratic form is induced by some symmetric bilinear form
  \begin{equation*}
    L: V \times V \to \BbbR.
  \end{equation*}

  Suppose also that \( L \) is positive definite. Due to positive (resp. negative) definiteness, for every \( x \neq 0 \), \( L(x, y) \) is positive (resp. negative) when \( y = x \). Hence, \( L(x, y) = 0 \) for every \( y \) if and only if \( x = 0 \). Hence, \( L \) is nondegenerate.

  In the complex case, the quadratic form is induced by some Hermitian sesquilinear form
  \begin{equation*}
    L: V \times V \to \BbbC.
  \end{equation*}

  Again, \( L(x, y) \) is zero for all \( y \) when \( x = 0 \), hence \( L \) is nondegenerate.
\end{proof}

\begin{definition}\label{def:inner_product_space}\mimprovised
  A \term{real inner product space} is a vector space \( V \) over \( \BbbR \) equipped with a \hyperref[thm:quadratic_forms_definiteness/positive_definite]{positive definite} \hyperref[def:symmetric_function]{symmetric} \hyperref[def:bilinear_form]{bilinear form}
  \begin{equation*}
    \inprod \anon \anon: V \times V \to \BbbR.
  \end{equation*}

  A \term{complex inner product space} is a vector space \( V \) over \( \BbbC \) equipped with a \hyperref[thm:quadratic_forms_definiteness/positive_definite]{positive definite} \hyperref[def:hermitian_form]{Hermitian} \hyperref[def:sesquilinear_form]{sesquilinear form}
  \begin{equation*}
    \inprod \anon \anon: V \times V \to \BbbC.
  \end{equation*}

  This notation is generalized for application of linear functionals --- see \fullref{rem:dual_space_bilinear_form}.

  By default, for \( \BbbK^n \) we assume that the inner product is the \term{dot product} \( \inprod x y \coloneqq x^T y \) or, when working over complex inner product spaces, \( \inprod x y \coloneqq y^* x \).
\end{definition}

\begin{remark}\label{rem:structure_hierarchy}
  The hierarchy in \cref{fig:rem:structure_hierarchy} describes how, given any of the structures there, we automatically also have available all the ones to the bottom of it. For example, every real or complex inner product space is also a normed space, metric space and so forth. We will use this implicitly.

  \begin{figure}[!ht]
    \caption{Hierarchy of important mathematical structures}\label{fig:rem:structure_hierarchy}
    \smallskip
    \hfill
    \begin{forest}
      [
        {\hyperref[def:inner_product_space]{inner product}}, name=product
        [
          {\hyperref[def:norm]{norm}}, name=norm, edge label={node[midway,left]{\hyperref[def:bilinear_form_induced_norm]{induced norm}}}
          [
            {\hyperref[def:metric_space]{metric}}, name=metric, edge label={node[midway,left]{\hyperref[def:norm_induced_metric]{induced metric}}}
            [
              {\hyperref[def:uniform_space]{uniformity}}, name=uniformity, edge label={node[midway,left]{\hyperref[def:metric_uniformity]{metric uniformity}}}
              [
                {\hyperref[def:topological_space]{topology}}, name=topology, edge label={node[midway,left]{\hyperref[def:uniform_topology]{uniform topology}}}
              ]
            ]
          ]
        ]
      ]
    \end{forest}
    \hfill\hfill
  \end{figure}
\end{remark}

\begin{definition}\label{def:orthogonality}\mimprovised
  We say that the vectors \( x \) and \( y \) are \term{orthogonal} and write \( x \perp y \) if \( \inprod x y = 0 \). If, in addition, both vectors have unit norm, we say that they are \term{orthonormal}.
\end{definition}

\begin{remark}\label{rem:inner_product_basis_decomposition}
  Let \( V \) be an \hyperref[def:inner_product_space]{inner product space} over \( \BbbC \) and let \( e_1, \ldots, e_n \) be an orthonormal basis of \( V \). By \fullref{thm:basis_projection_orthonormal},
  \begin{equation*}
    \pi_{e_i}(e_j) = \begin{cases}
      1, &i = j \\
      0, &i \neq j
    \end{cases}.
  \end{equation*}

  By \fullref{thm:free_semimodule_universal_property}, there exist unique linear maps that satisfy the above conditions. Therefore,
  \begin{equation*}
    \pi_{e_i}(e_j) = \inprod {e_i} {e_j}
  \end{equation*}
  and hence the \hyperref[def:basis_decomposition]{basis decomposition} of \( x \) becomes
  \begin{equation}\label{eq:rem:inner_product_basis_decomposition/decomposition}
    x = \sum_{k=1}^n \inprod {x} {e_k} e_k.
  \end{equation}

  If \( V \) has no inner product defined and \( e_1, \ldots, e_n \) is an arbitrary basis, we can define the inner product
  \begin{equation}\label{eq:rem:inner_product_basis_decomposition/product}
    \inprod x y = \sum_{k=1}^n \pi_{e_k}(x) \cdot \overline{\pi_{e_k}(y)}.
  \end{equation}

  It is obviously bilinear, Hermitian and positive definite. Furthermore, \( e_1, \ldots, e_n \) is an \hyperref[def:orthogonality]{orthonormal} basis with respect to this inner product.

  This reduces in an obvious way to real vector spaces.
\end{remark}

\begin{theorem}[Pythagoras' theorem]\label{thm:pythagoras_theorem}
  In an \hyperref[def:inner_product_space]{inner product space}, if \( x \) and \( y \) are \hyperref[def:orthogonality]{orthogonal}, then
  \begin{equation*}
    \norm{x + y}^2 = \norm{x}^2 + \norm{y}^2.
  \end{equation*}
\end{theorem}
\begin{proof}
  Since \( \inprod x y = \inprod y x = 0 \), then
  \begin{equation*}
    \norm{x + y}^2
    =
    \inprod {x + y} {x + y}
    =
    \inprod x x + \inprod x y + \inprod y x + \inprod y y
    =
    \norm{x}^2 + \norm{y}^2.
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:orthogonal_implies_linearly_independent}
  Two nonzero orthogonal vectors are necessarily \hyperref[def:linear_dependence]{linearly independent}.
\end{proposition}
\begin{proof}
  We will prove the converse. Suppose that \( x = ty \) and that both vectors are nonzero. In particular, \( t \neq 0 \). Then
  \begin{equation*}
    \inprod x y = t \inprod y y.
  \end{equation*}

  Since inner products are positive definite, \( \inprod y y > 0 \) and hence \( \inprod x y > 0 \). Therefore, \( x \) and \( y \) are not orthogonal.
\end{proof}

\begin{definition}\label{def:orthogonal_complement}\mimprovised
  The \term{orthogonal complement} of the subspace \( U \) of \( V \) is the subspace
  \begin{equation*}
    U^\perp \coloneqq \set{ y \in U \given \qforall {x \in U} \inprod x y = 0 }
  \end{equation*}
  of \hyperref[def:orthogonality]{orthogonal} to all members of \( U \) subspaces.

  The (bi)linearity of the inner product ensures that \( U^\perp \) is indeed a subspace.

  This concept generalizes to \hyperref[def:orthogonal_complement]{annihilators}.
\end{definition}

\begin{theorem}[Gramâ€“Schmidt orthogonalization]\label{thm:gramm_schmidt_orthogonalization}
  Let \( e_1, \ldots, e_n \) be a \hyperref[def:orthogonality]{basis} for the vector space \( V \). For \( i = 1, \ldots, n \) define
  \begin{equation*}
    f_k \coloneqq e_k - \sum_{i=1}^{k-1} \frac { \inprod {e_k} {f_i} } { \inprod {f_i} {f_i} } f_i.
  \end{equation*}

  Then \( f_1, \ldots, f_n \) is an \hyperref[def:orthogonality]{orthogonal} basis. We can then additionally rescale these vectors to obtain an \hyperref[def:orthogonality]{orthonormal} basis.
\end{theorem}
\begin{proof}
  Since for \( k \leq n \), the vector \( f_k \) is a linear combination of \( e_1, \ldots, e_k \), we have
  \begin{equation*}
    \linspan\set{ f_1, \ldots, f_n } = \linspan\set{ e_1, \ldots, e_n }.
  \end{equation*}

  We will use induction on \( n \) to show that \( f_1, \ldots, f_{n-1} \) are orthogonal (and, in particular, linearly independent). Since \( f_1 = e_1 \), the case \( n = 1 \) is trivial. Suppose that the first \( n - 1 \) vectors are orthogonal. Then, for \( k < n \),
  \small
  \begin{equation*}
    \inprod{f_n} {f_k}
    =
    \inprod*{ e_n - \sum_{i=1}^{n-1} \frac { \inprod {e_n} {f_i} } { \inprod {f_i} {f_i} } f_i} {f_k}
    =
    \inprod {e_n} {f_k} - \sum_{i=1}^{n-1} \frac { \inprod {e_n} {f_i} } { \inprod {f_i} {f_i} } \underbrace{ \inprod{f_i } {f_k} }_{ 0 \T*{if} i \neq j }
    =
    \inprod {e_n} {f_k} - \frac { \inprod {e_n} {f_k} } { \inprod {f_k} {f_k} } \inprod {f_k} {f_k}
    =
    0.
  \end{equation*}
  \normalsize

  Therefore, \( f_1, \ldots, f_n \) are pairwise orthogonal.
\end{proof}

\begin{corollary}\label{thm:finite_dimensional_orthonormal_basis_existence}
  Any finite-dimensional vector space has an orthonormal basis.
\end{corollary}
\begin{proof}
  We can simply apply \fullref{thm:gramm_schmidt_orthogonalization} to any basis.
\end{proof}

\begin{proposition}\label{thm:direct_sum_with_orthogonal_complement}
  Let \( U \) be a subspace of the finite-dimensional inner product space \( V \). Then
  \begin{equation*}
    V \cong U \oplus U^\perp,
  \end{equation*}
  where \( U^\perp \) is the \hyperref[def:orthogonal_complement]{orthogonal complement} of \( U \).
\end{proposition}
\begin{proof}
  \Fullref{thm:finite_dimensional_orthonormal_basis_existence} ensures that an orthonormal basis \( e_1, \ldots, e_n \) of \( U \) exists. \Fullref{thm:def:vector_space/expansion} allows us to expand this to a basis of \( V \) and \fullref{thm:gramm_schmidt_orthogonalization} allows us to orthogonalize the entire basis of \( V \). Let \( e_{n+1}, \ldots, e_m \) be the remaining vectors of the orthogonalized basis. Then
  \begin{equation*}
    U = \linspan{ e_1, \ldots, e_n }.
  \end{equation*}

  Furthermore, \( e_k \) is orthogonal to each of \( e_1, \ldots, e_n \) for \( k > n \), hence also to their linear combinations. Therefore,
  \begin{equation*}
    \linspan{ e_{n+1}, \ldots, e_m } \subsetneq U^\perp.
  \end{equation*}

  If \( e_{m+1} \) is an additional basis vector of \( U^\perp \), then it is a basis vector of \( V \). But that would mean that both \( e_1, \ldots, e_m \) and \( e_1, \ldots, e_{m+1} \) are bases of \( V \), which contradicts \fullref{thm:vector_space_dimension}.

  Hence,
  \begin{equation*}
    U^\perp = \linspan{ e_{n+1}, \ldots, e_m }.
  \end{equation*}

  Therefore,
  \begin{equation*}
    V \cong U \oplus U^\perp.
  \end{equation*}
\end{proof}

\begin{theorem}[Cauchy-Bunyakovsky-Schwarz inequality]\label{thm:cauchy_bunyakovsky_schwarz_inequality}
  In a real or complex \hyperref[def:inner_product_space]{inner product space}, the following inequality holds:
  \begin{equation}\label{thm:cauchy_bunyakovsky_schwarz_inequality/inequality}
    \abs{ \inprod x y} \leq \norm x \cdot \norm y.
  \end{equation}

  Furthermore, equality holds if and only if \( x \) and \( y \) are linearly dependent.
\end{theorem}
\begin{proof}
  \SubProof{Proof of inequality} Fix \( x, y \in V \) and \( t \in \BbbC \). If either \( x \) or \( y \) is the zero vector, the statement is trivially true. Suppose that both are nonzero.

  We have
  \begin{balign*}
    Q(x + y)
     & =
    \inprod {x + y} {x + y}
    =    \\ &=
    Q(x) + \overline t \inprod x y +  \inprod y x + \abs{t}^2 Q(y)
    =    \\ &=
    Q(x) + 2\real \parens*{ t \overline{ \inprod x y} } + \abs{t}^2 Q(y).
  \end{balign*}

  Take \( t \coloneqq - \ifrac { \inprod x y} {Q(y)} \), so that
  \begin{equation*}
    Q(x + y)
    =
    Q(x) - 2 \frac {\abs{ \inprod x y}^2} {Q(y)} + \frac {\abs{ \inprod x y}^2} {Q(y)}
    =
    Q(x) - \frac {\abs{ \inprod x y}^2} {Q(y)}.
  \end{equation*}

  Since \( Q(x + y) \geq 0 \), it follows that
  \begin{balign*}
    Q(x) - \frac {\abs{ \inprod x y}^2} {Q(y)} &\geq 0                  \\
    Q(x) Q(y)                                 &\geq \abs{ \inprod x y}^2.
  \end{balign*}

  \SubProof{Proof of equality} If \( x \) and \( y \) are linearly dependent, equality obviously holds. Conversely, suppose that equality holds. This implies that
  \begin{equation*}
    Q(x + y) = 0,
  \end{equation*}
  which by the positive definiteness of \( Q \) means that \( x = -ty \). Thus, \( x \) and \( y \) are linearly dependent.
\end{proof}
