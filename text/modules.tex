\subsection{Modules}\label{subsec:modules}

\begin{definition}\label{def:module}
  A \term{module} is a \hyperref[def:semimodule]{semimodule} over a \hyperref[def:ring]{ring} rather than a \hyperref[def:semiring]{semiring}.

  This makes the identity law \eqref{eq:def:semimodule/operation/scalar_multiplication_action/identity} redundant.

  Modules have the following metamathematical properties:
  \begin{thmenum}
    \thmitem{def:module/theory} The first-order theory is identical to the \hyperref[def:semimodule/theory]{theory of semimodules}.

    \thmitem{def:module/homomorphism} A \hyperref[def:first_order_homomorphism]{first-order homomorphism} between two \( R \)-modules \( M \) and \( N \) is simply a \hyperref[def:semimodule/homomorphism]{linear map}.

    \thmitem{def:module/submodel} The set \( A \subseteq M \) is a \hyperref[def:first_order_submodel]{submodel} of \( M \) if it is a sub-semimodule of \( M \), i.e. a subgroup of \( M \) that is closed under scalar multiplication. We say that \( A \) is a \term{submodule} of \( M \).

    As a consequence of \fullref{thm:positive_formulas_preserved_under_homomorphism}, the image of a module homomorphism is a submodule of its codomain.

    \thmitem{def:module/trivial} The \hyperref[rem:trivial_structure]{trivial} \( R \)-module is the \hyperref[def:pointed_set/trivial]{trivial pointed set} \( \set{ 0 } \).

    We sometimes denote the zero of the module via \( \vect 0 \).

    \thmitem{def:module/bottom} The \hyperref[thm:substructures_form_complete_lattice/bottom]{bottom substructure} of any \( R \)-module is isomorphic to the trivial \( R \)-module \( \set{ \vect 0 } \).

    \thmitem{def:module/bimodule} A \term{bimodule} is simply a \hyperref[def:semimodule/bisemimodule]{bisemimodule} over a ring.

    \thmitem{def:module/category} For a fixed ring \( R \), we denote the \hyperref[def:category_of_small_first_order_models]{category of \( \mscrU \)-small models} by \( \ucat{Mod}_R \).

    It is a very well-behaved category, even more than the category \hyperref[def:group/category]{\( \ucat{Grp} \)} of \( \mscrU \)-small groups.
    \begin{itemize}
      \item The trivial module \( \set{ 0 } \) is a zero object. Therefore, we can define kernels and cokernels, and cokernels for modules are particularly simple.

      \item The \hyperref[def:free_semimodule]{free semimodules} over a ring are modules, and \fullref{thm:free_semimodule_universal_property} ensures that this is left adjoint to the forgetful functor \( U: \ucat{Mod}_R \to \ucat{Set} \). Therefore, by \fullref{thm:first_order_categorical_invertibility}, the monomorphisms are exactly the injective homomorphisms, and that the \hyperref[def:subobject_and_quotient]{categorical subobjects} correspond to submodules.

      \item Every epimorphism in \( \ucat{Mod}_R \) is surjective. This will be proved in \fullref{thm:module_epimorphisms_are_surjective}. Along with \fullref{thm:group_epimorphisms_are_normal}, this shows that the \hyperref[def:subobject_and_quotient]{categorical quotient objects} correspond to \hyperref[def:module/quotient]{quotient modules}, which we will define shortly.
    \end{itemize}

    \thmitem{def:module/kernel} The \hyperref[def:zero_morphisms/kernel]{kernel} of an \( R \)-module homomorphism \( \varphi: M \to N \) is its \hyperref[def:zero_locus]{zero locus} \( \varphi^{-1}(0_N) \). This is a submodule of \( M \). It is precisely the kernel of the underlying group in the sense of \fullref{def:group/kernel}, and the \hyperref[def:zero_morphisms/cokernel]{categorical kernel} in the category of modules.

    \thmitem{def:module/quotient} The \hyperref[def:zero_morphisms/cokernel]{categorical cokernel} of an \( R \)-homomorphism \( \varphi: M \to N \) in the category \( \cat{Mod}_R \) is simply the additive \hyperref[def:group/quotient]{quotient group} \( N / \img \varphi \). The quotient group is again a module over \( R \) because \( N \) is closed under scalar multiplication and, for every coset \( x + N \),
    \begin{equation*}
      r(x + N) = rx + rN = rx + N
    \end{equation*}
    is again a coset in \( N / \img \varphi \).

    In particular, given a submodule \( N \) of \( M \), we can form the \term{quotient module} \( M / N \). Quotients are conveniently characterized by \fullref{thm:quotient_module_universal_property}.

    \thmitem{def:module/simple} Analogously to \hyperref[def:group/simple]{simple groups}, if the only proper \hyperref[def:module/submodel]{submodule} of \( R \) is the \hyperref[def:module/trivial]{trivial module} \( \set{ 0_M } \), we say that \( M \) is a \term{simple module}.

    The trivial module itself is not simple, because it has no proper ideals.
  \end{thmenum}
\end{definition}

\begin{proposition}\label{thm:abelian_group_is_module}
  We have an \hyperref[rem:category_similarity/isomorphism]{isomorphism of categories} \( \hyperref[def:abelian_group]{\cat{Ab}} \cong \hyperref[def:module]{\cat{Mod}_\BbbZ} \).

  More concretely, every abelian group \( G \) is a left module over \( \BbbZ \) with scalar multiplication given by \hyperref[rem:additive_semigroup/multiplication]{recursively defined multiplication}
  \begin{equation}\label{eq:thm:abelian_group_is_module/operation}
    \begin{aligned}
      &\cdot: \BbbZ \times G \to G \\
      &n \cdot x \coloneqq \begin{cases}
        0_G,           &n = 0, \\
        n \cdot x + x, &n > 1, \\
        -(n \cdot x),  &n < 1.
      \end{cases}
    \end{aligned}
  \end{equation}

  Conversely, in every module over \( \BbbZ \), scalar multiplication matches the recursively defined multiplication.

  Compare this result to \fullref{thm:commutative_monoid_is_semimodule}.
\end{proposition}
\begin{proof}
  Simple refinement of \fullref{thm:commutative_monoid_is_semimodule}.
\end{proof}

\begin{theorem}[Quotient module universal property]\label{thm:quotient_module_universal_property}
  For every \( R \)-\hyperref[def:module]{module} \( M \) and every submodule \( N \) of \( M \), the \hyperref[def:module/quotient]{quotient module} \( R / I \) has the following \hyperref[rem:universal_mapping_property]{universal mapping property}:
  \begin{displayquote}
    Every \( R \)-module homomorphism \( \varphi: M \to K \) satisfying \( N \subseteq \ker \varphi \) \hyperref[def:factors_through]{uniquely factors through} \( M / N \). That is, there exists a unique \( R \)-module homomorphism \( \widetilde{\varphi}: M / N \to K \), such that the following diagram commutes:
    \begin{equation}\label{eq:thm:quotient_module_universal_property/diagram}
      \begin{aligned}
        \includegraphics[page=1]{output/thm__quotient_module_universal_property}
      \end{aligned}
    \end{equation}

    In the case where \( N = \ker \varphi \), \( \widetilde{\varphi} \) is an \hyperref[def:first_order_embedding]{embedding}.
  \end{displayquote}

  Compare this result to \fullref{thm:quotient_group_universal_property} and \fullref{thm:quotient_algebra_universal_property}.
\end{theorem}
\begin{proof}
  Simple refinement of \fullref{thm:quotient_group_universal_property}.
\end{proof}

\begin{theorem}[Quotient submodule lattice theorem]\label{thm:lattice_theorem_for_submodules}
  Given a \hyperref[def:module/submodel]{submodule} \( N \) of \( M \), the function \( K \mapsto K / N \) is a \hyperref[def:semilattice/homomorphism]{lattice isomorphism} between the \hyperref[thm:substructures_form_complete_lattice]{lattice of submodules} of \( M \) containing \( N \) and the lattice of submodules of the \hyperref[def:module/quotient]{quotient} \( M / N \).

  Compare this result to \fullref{thm:quotient_structure_lattice_theorem} and \fullref{thm:lattice_theorem_for_rings}.
\end{theorem}
\begin{proof}
  Simple refinement of \fullref{thm:quotient_structure_lattice_theorem}.
\end{proof}

\begin{proposition}\label{thm:module_epimorphisms_are_surjective}
  Every \hyperref[def:morphism_invertibility/right_cancellative]{epimorphism} in \hyperref[def:group/category]{\( \cat{Mod}_R \)} is \hyperref[def:function_invertibility/surjective]{surjective}.
\end{proposition}
\begin{proof}
  Let \( \varphi: M \to N \) be an \( R \)-module epimorphism. Consider the canonical projection \( \pi: N \to N / \img \varphi \) and the zero morphism \( z: N \to N / \img \varphi \). Clearly
  \begin{equation*}
    \pi \bincirc \varphi = z \bincirc \varphi,
  \end{equation*}
  and thus \( \pi = z \) is the zero morphism.

  By, \fullref{thm:def:group/kernel_cokernel_compatibility}, \( \ker \pi = \img \varphi \), and since \( \ker \pi = N \), it follows that \( \varphi \) is a surjective function.
\end{proof}

\begin{definition}\label{def:module_presentation}
  A \term{presentation} of the \( R \)-\hyperref[def:module]{module} \( M \) is an \hyperref[def:module/homomorphism]{epimorphism} \( \varphi: R^{\oplus A} \to M \), where \( R^{\oplus A} \) is a \hyperref[def:free_semimodule]{free semimodule}.

  By \fullref{thm:quotient_module_universal_property},
  \begin{equation*}
    M = \img \varphi \cong R^{\oplus A} / \ker \varphi.
  \end{equation*}

  Analogously to \hyperref[def:group_presentation]{group presentations}, we say that \( M \) is finitely generated/related/presented if there exists an appropriate presentation.
\end{definition}

\begin{proposition}\label{thm:module_presentation_existence}
  Every module has at least one \hyperref[def:module_presentation]{presentation}.

  Compare this to \fullref{thm:group_presentation_existence} and \fullref{thm:algebra_presentation_existence}.
\end{proposition}
\begin{proof}
  This can be proven analogously to \fullref{thm:group_presentation_existence}.
\end{proof}

\begin{definition}\label{def:linear_dependence}\mimprovised
  Let \( M \) be an \( R \)-\hyperref[def:module]{module} and fix a subset \( E \subseteq M \). We say that the elements of \( E \) are \term{linearly independent} if any of the following conditions hold:

  \begin{thmenum}
    \thmitem{def:linear_dependence/direct} A \hyperref[rem:linear_combinations]{linear combination} in \( E \) sums to zero if and only if it is \hyperref[def:free_semimodule]{trivial}.

    \thmitem{def:linear_dependence/evaluation} The \hyperref[thm:free_semimodule_universal_property]{linear combination evaluation map}
    \begin{equation*}
      \begin{aligned}
        &\Phi_E: R^{\oplus E} \to M \\
        &\Phi_E( \seq{ t_e }_{e \in E} ) \coloneqq \sum_{e \in E} t_e e
      \end{aligned}
    \end{equation*}
    is \hyperref[def:function_invertibility/injective]{injective}.
  \end{thmenum}

  Unsurprisingly, if the elements of \( E \) are not \term{linearly independent}, we say that they are \term{linearly dependent}.

  Compare this concept to \hyperref[def:algebraic_dependence]{algebraic dependence}.
\end{definition}
\begin{defproof}
  \ImplicationSubProof{def:linear_dependence/direct}{def:linear_dependence/evaluation} Suppose that only the trivial linear combination of \( E \) sums to zero. If \( \sum_{e \in E} t_e e = \sum_{e \in E} r_e e \), then
  \begin{equation*}
    \sum_{e \in E} t_e e - \sum_{e \in E} r_e e
    \reloset {\eqref{eq:def:semiring/left_distributivity}} =
    \sum_{e \in E} (t_e - r_e) e
    =
    0,
  \end{equation*}
  implying that \( t_e = r_e \) for every \( e \in E \).

  Hence, \( \Phi_E \) is injective.

  \ImplicationSubProof{def:linear_dependence/evaluation}{def:linear_dependence/direct} Trivial.
\end{defproof}

\begin{remark}\label{rem:linear_dependence_ease_ring}
  Like all concepts related to \hyperref[rem:linear_combinations]{linear combinations}, linear dependence may behave differently depending on the underlying ring. For example, every irrational number is a linear combination of itself, but it is not a linear combination of rational numbers.

  For simplicity, we will not specify the ring explicitly unless this may cause confusion.
\end{remark}

\begin{proposition}\label{thm:def:linear_dependence}
  \hyperref[def:linear_dependence]{Linear (in)dependence} in the \( R \)-module \( M \) has the following basic properties:
  \begin{thmenum}
    \thmitem{thm:def:linear_dependence/zero} The zero vector \( 0_M \) is by itself linearly dependent.

    \thmitem{thm:def:linear_dependence/monotonicity} If \( E \) is a linearly \hi{dependent} set and \( E \subseteq F \), then \( F \) is also a linearly dependent set.

    \thmitem{thm:def:linear_dependence/antimonotonicity} If \( F \) is a linearly \hi{independent} set and \( E \subseteq F \), then \( E \) is also a linearly independent set.

    \thmitem{thm:def:linear_dependence/dependent_combination} The set \( E \cup \set{ x } \) is linearly dependent if and only if \( x \in \linspan E \).

    This may not hold for more general modules.

    \thmitem{thm:def:linear_dependence/span_dependent} For any set of vectors \( E \), if \( x \in \linspan E \setminus E \), then \( E \cup \set{ x } \) is a linearly dependent set.

    A partial converse is stated in \fullref{thm:def:vector_space/span_independent}.
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:def:linear_dependence/zero} Trivial.

  \SubProofOf{thm:def:linear_dependence/monotonicity} Trivial.

  \SubProofOf{thm:def:linear_dependence/antimonotonicity} Trivial.

  \SubProofOf{thm:def:linear_dependence/span_dependent} By \fullref{thm:span_via_linear_combinations}, there exists a linear combination of members of \( E \) such that
  \begin{equation*}
    x = \sum_{k=1}^n t_k x_k.
  \end{equation*}

  If \( x = 0_M \), then \( E \cup \set{ x } \) is linearly dependent by \fullref{thm:def:linear_dependence/zero} and \fullref{thm:def:linear_dependence/monotonicity}.

  If \( x \neq 0_M \), then \( E \cup \set{ x } \) is linearly dependent because \( x \) is a nontrivial linear combination of other vectors of \( E \).
\end{proof}

\begin{example}\label{ex:def:linear_dependence}
  We list several (counter)examples for \hyperref[def:linear_dependence]{linear dependence}:
  \begin{thmenum}
    \thmitem{ex:def:linear_dependence/not_in_span} The columns
    \begin{equation*}
      \begin{pmatrix}
        0 \\ 1 \\ 0
      \end{pmatrix}
      \begin{pmatrix}
        1 \\ 1 \\ 0
      \end{pmatrix}
      \begin{pmatrix}
        0 \\ 1 \\ 1
      \end{pmatrix}
    \end{equation*}
  \end{thmenum}
\end{example}

\begin{definition}\label{def:hamel_basis}\mimprovised
  Let \( M \) be a left \( R \)-\hyperref[def:module]{module} and fix a subset \( E \subseteq M \). We say that \( E \) is a \term{Hamel basis} or simply \term{basis} of \( M \) if any of the following equivalent conditions hold:

  \begin{thmenum}
    \thmitem{def:hamel_basis/independent} It is a \hyperref[thm:span_via_linear_combinations]{spanning set} of \hyperref[def:linear_dependence]{linearly independent} elements.

    \thmitem{def:hamel_basis/evaluation} The \hyperref[thm:free_semimodule_universal_property]{linear combination evaluation map}
    \begin{equation*}
      \begin{aligned}
        &\Phi_E: R^{\oplus E} \to M \\
        &\Phi_E( \seq{ t_e }_{e \in E} ) \coloneqq \sum_{e \in E} t_e e
      \end{aligned}
    \end{equation*}
    is \hyperref[def:function_invertibility/bijective]{bijective}.

    \thmitem{def:hamel_basis/free} \( M \) is \hyperref[def:semimodule/homomorphism]{linearly isomorphic} to the \hyperref[def:free_semimodule]{free module} \( R^{\oplus E} \).

    It is established terminology to say that \( M \) is a free module.
  \end{thmenum}
\end{definition}
\begin{defproof}
  \ImplicationSubProof{def:hamel_basis/independent}{def:hamel_basis/evaluation} Suppose that \( E \) is a spanning set of linearly independent elements.

  \Fullref{def:linear_dependence/evaluation} is satisfied, hence \( \Phi_E \) is injective.

  Furthermore, \( E \) is spanning, meaning that \( \linspan E = M \). By \fullref{def:linear_dependence/evaluation}, it is surjective.

  \ImplicationSubProof{def:hamel_basis/evaluation}{def:hamel_basis/free} If \( \Phi_E \) is bijective, then it is a linear isomorphism.

  \ImplicationSubProof{def:hamel_basis/free}{def:hamel_basis/independent} Suppose that \( \Psi: R^{\oplus E} \to M \) is a linear isomorphism. By \fullref{def:linear_dependence/evaluation}, \( \Psi \) is surjective, and hence a spanning set of \( M \). Furthermore, it satisfies \fullref{def:linear_dependence/evaluation}, meaning that \( E \) is a linearly independent set.
\end{defproof}

\begin{example}\label{ex:module_without_basis}
  As a \hyperref[thm:abelian_group_is_module]{\( \BbbZ \)-module}, the additive group of \( \BbbQ \) has no basis.

  Indeed, given any two rational numbers \( \ifrac a b \) and \( \ifrac c d \), we have
  \begin{equation*}
    cb \cdot \frac a b + da \cdot \frac c d = 0.
  \end{equation*}

  Therefore, only singleton sets of rational numbers are linearly independent with respect to \( \BbbZ \). But no single integer generates \( \BbbQ \).
\end{example}

\begin{proposition}\label{thm:def:hamel_basis}
  \hyperref[def:hamel_basis]{Bases} of the \( R \)-module \( M \) have the following basic properties:
  \begin{thmenum}
    \thmitem{thm:def:hamel_basis/span} Every \hyperref[def:linear_dependence]{linearly independent} subset of \( M \) is a basis for its \hyperref[def:semimodule/submodel]{linear span}.

    \thmitem{thm:def:hamel_basis/maximal_independent} Every basis of \( M \) is maximal among \hyperref[thm:def:linear_dependence]{linearly independent} sets.

    The converse holds for vector spaces --- see \fullref{thm:def:vector_space/minimal_spanning}.

    \thmitem{thm:def:hamel_basis/minimal_spanning} Every basis of \( M \) is minimal among spanning sets.

    The converse holds for vector spaces --- see \fullref{thm:def:vector_space/minimal_spanning}.
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:def:hamel_basis/span} Trivial.

  \SubProofOf{thm:def:hamel_basis/maximal_independent} Follows from \fullref{thm:def:linear_dependence/span_dependent}.

  \SubProofOf{thm:def:hamel_basis/minimal_spanning} Let \( E \) be a basis of \( M \) and suppose that \( F \subsetneq E \) is also a spanning set of \( M \).

  Then there exists some vector \( x \in E \setminus F \). Since both sets are spanning, \( \linspan F = \linspan E \). By \fullref{thm:def:linear_dependence/span_dependent}, \( F \cup \set{ x } \) is linearly dependent, and by \fullref{thm:def:linear_dependence/antimonotonicity}, \( E \) is linearly dependent. This contradicts the assumption that \( E \) has a basis.

  Therefore, no proper subset of \( E \) is spanning for \( M \).
\end{proof}

\begin{proposition}\label{thm:basis_of_direct_sum}
  Consider the \( R \)-modules \( \seq{ M_k }_{k \in \mscrK} \). Suppose that \( \seq{ E_k }_{k \in \mscrK} \) are bases of the corresponding modules. Then the set
  \begin{equation*}
    E \coloneqq \bigcup_{k \in \mscrK} E_k
  \end{equation*}
  is a basis for the \hyperref[def:semimodule_direct_product]{direct sum}
  \begin{equation*}
    M \coloneqq \bigoplus_{k \in \mscrK} M_k.
  \end{equation*}
\end{proposition}
\begin{proof}
  Regard \( M_k \) as a subspace of the direct sum \( M \). The subspaces \( M_k \) and \( M_n \) are disjoint for \( k \neq n \). Thus, every vector from \( M \) is a unique sum of vectors from the subspaces.

  Since, for every \( k \in \mscrK \), every vector from \( M_k \) can further be uniquely represented as a linear combination of vectors from \( E_k \), we conclude that every vector from \( M \) is a linear combination of vectors from \( E \).
\end{proof}

\begin{definition}\label{def:basis_decomposition}\mimprovised
  If \( E \) is a basis of the \( R \)-\hyperref[def:module]{module} \( M \), the inverse of the linear isomorphism from \fullref{def:hamel_basis/evaluation} is
  \begin{equation*}
    \begin{aligned}
      &\pi_E: M \to R^{\oplus E} \\
      &\pi_E\parens[\Big]{ \sum_{e \in E} t_e \cdot e } \coloneqq \seq{ t_e }_{e \in E}.
    \end{aligned}
  \end{equation*}

  We denote the \( e \)-th component of this function by \( \pi_e: M \to R \). This is also a linear map, which we call the \term{coordinate projection} for \( e \).

  For every vector \( x \) in \( M \), we thus have
  \begin{equation*}
    x = \sum_{e \in E} \pi_e(x) \cdot e.
  \end{equation*}

  This linear combination is unique, and we call it the \term{decomposition} of \( x \) along \( E \).

  As in \fullref{rem:linear_combinations}, we sometimes take only the basis vectors with nonzero coefficients as \enquote{the} decomposition.
\end{definition}
\begin{defproof}
  The decomposition is indeed unique --- if
  \begin{equation*}
    x = \sum_{e \in E} t_e e = \sum_{e \in E} r_e e,
  \end{equation*}
  then
  \begin{equation*}
    \sum_{e \in E} (t_e - r_e) e = 0_M.
  \end{equation*}

  Since \( E \) is a basis, it is linearly independent and hence \( t_e = r_e \) for all \( e \) in \( E \).
\end{defproof}

\begin{definition}\label{def:semimodule_torsion}\mimprovised
  We say that the semimodule element \( x \) is a \term{torsion element} if there exists some nonzero scalar \( t \) such that \( tx \) is the zero vector. A semimodule without torsion elements is called \term{torsion-free}.
\end{definition}

\begin{example}\label{ex:def:semimodule_torsion}
  We list examples of \hyperref[def:semimodule_torsion]{torsion elements}:
  \begin{thmenum}
    \thmitem{ex:def:semimodule_torsion/scalars} When regarded as a module over itself, every zero divisor of a ring is a torsion element. For example, \( 2 \) and \( 3 \) in \( \BbbZ_6 \).

    \thmitem{ex:def:semimodule_torsion/matrices} The matrix
    \begin{equation*}
      \begin{pmatrix}
        2 & 0 \\
        0 & 3
      \end{pmatrix}
    \end{equation*}
    is a torsion element of the \hyperref[thm:matrix_algebra]{matrix algebra} \( \BbbZ_6^{2 \times 2} \) because \( 2 \cdot 3 = 3 \cdot 4 = 0 \).
  \end{thmenum}
\end{example}

\begin{proposition}\label{thm:basis_implies_torsion_free}
  If the \( R \)-module \( M \) has a \hyperref[def:hamel_basis]{basis}, it is \hyperref[def:semimodule_torsion]{torsion-free}.
\end{proposition}
\begin{proof}
  Suppose that \( tx \) for some nonzero scalar \( t \) and nonzero vector \( x \). Let \( E \) be a basis of \( M \) and let \( x = \sum_{e \in E} \pi_e(x) \cdot e \) be the basis decomposition of \( x \). Then this is a nontrivial linear combination that sums to zero, which contradicts the assumption that \( E \) is a basis.
\end{proof}

\begin{proposition}\label{thm:basis_projection_orthonormal}
  If the \( R \)-module \( M \) has a \hyperref[def:hamel_basis]{basis} \( E \), then the \hyperref[def:basis_decomposition]{projection functionals} satisfy
  \begin{equation}\label{eq:thm:basis_projection_orthonormal}
    \pi_e(f) = \begin{cases}
      1, &e = f, \\
      0, &f \in E \setminus \set{ e }.
    \end{cases}
  \end{equation}
\end{proposition}
\begin{proof}
  We have
  \begin{equation*}
    f = \sum_{e \in B} \pi_e(f) \cdot e.
  \end{equation*}

  Thus,
  \begin{equation*}
    0 = f - \sum_{e \in B} \pi_e(f) \cdot e = (1 - \pi_f(f)) \cdot f - \sum_{e \in E \setminus \set{ f }} \pi_e(f).
  \end{equation*}

  Since the vectors in \( E \) are linearly independent, \eqref{eq:thm:basis_projection_orthonormal} follows.
\end{proof}

\begin{definition}\label{def:sequence_space}
  Let \( R \) be a \hyperref[def:ring/commutative]{commutative ring} and let \( \alpha \) be either a finite \hyperref[def:ordinal]{ordinal} or the \hyperref[thm:omega_is_an_ordinal]{smallest infinite ordinal} \( \omega \). We call the \hyperref[def:free_semimodule]{free module} \( R^\alpha \) a \term{sequence space} or, in case \( \alpha \) is finite, also a \term{coordinate space}.

  The \term{standard basis} of the \( R^\alpha \) is the \hyperref[def:transfinite_sequence]{transfinite sequence} \( \seq{ e_i }_{j < \alpha} \) whose \hyperref[def:basis_decomposition]{projection maps} satisfy
  \begin{equation*}
    \pi_i(e_j) = \begin{cases}
      0, &i = j, \\
      1, &i \neq j.
    \end{cases}
  \end{equation*}

  When \( \alpha \) is finite, the vectors in \( R^\alpha \) are \hyperref[def:sequence]{finite sequences}, and we often conflate them with \hyperref[def:array/column_vector]{column vectors} or, less often, \hyperref[def:array/row_vector]{row vectors}. See \fullref{rem:vector_etymology} for a more detailed discussion of terminology. Similarly, when \( \alpha = \omega \), we conflate the vectors with (infinite) \hyperref[def:sequence]{sequences}.
\end{definition}

\begin{proposition}\label{thm:basis_of_polynomial_ring}
  For a nontrivial commutative ring \( R \), the set
  \begin{equation*}
    \set*{ \prod_{X \in \mscrX} X^{\gamma_X} \given \gamma \T{is a \hyperref[def:multi_index]{multi-index}} }
  \end{equation*}
  of all monomials is a \hyperref[def:hamel_basis]{basis} for \hyperref[def:polynomial_algebra]{polynomial ring} \( R[\mscrX] \).
\end{proposition}
\begin{proof}
  In \fullref{def:polynomial_algebra}, we have defined a polynomial as a \hyperref[def:free_semimodule]{free \( R \)-module} over the set of all monomials.
\end{proof}

\begin{definition}\label{def:vector_space}
  A \term{vector space} is a \hyperref[def:module]{left module} over a \hyperref[def:field]{field}.

  We denote the category of vector spaces over \( \BbbK \) by \( \cat{Vect}_{\BbbK} \).
\end{definition}

\begin{proposition}\label{thm:def:vector_space}
  The \hyperref[def:vector_space]{vector space} \( V \) over \( \BbbK \) has the following basic properties:
  \begin{thmenum}
    \thmitem{thm:def:vector_space/span_independent} For a linearly independent set \( A \), if \( x \in V \setminus \linspan A \), then \( A \cup \set{ x } \) is also a linearly independent set.

    A more general converse holds in \fullref{thm:def:linear_dependence/span_dependent}.

    \thmitem{thm:def:vector_space/maximal_independent} Every maximally \hyperref[thm:def:linear_dependence]{linearly independent} subset of \( V \) is a basis.

    The converse holds more generally --- see \fullref{thm:def:hamel_basis/maximal_independent}.

    \thmitem{thm:def:vector_space/minimal_spanning} Every minimal spanning subset of \( V \) is a basis.

    The converse holds more generally --- see \fullref{thm:def:hamel_basis/minimal_spanning}.

    \thmitem{thm:def:vector_space/expansion} Given a subspace \( U \) of \( V \), if \( A \) is a finite basis of \( U \) and if \( V \) has a finite basis, then we can extend \( A \) to a finite basis of \( V \). That is, there exists a finite basis \( E \) of \( V \) such that \( A \subseteq E \).

    \thmitem{thm:def:vector_space/dimension_lemma} If \( \linspan\set{ a_1, \ldots, a_n } \subseteq \linspan\set{ b_1, \ldots, b_m } \), then \( n \leq m \).
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:def:vector_space/span_independent} Suppose that \( x \in V \setminus \linspan A \) and that \( A \cup \set{ x } \) is linearly dependent, Then there exist coefficients \( t_0, t_1, \ldots, t_n \) such that
  \begin{equation*}
    t_0 x + \sum_{k=1}^n t_k a_k = 0_M.
  \end{equation*}

  If \( t_0 = 0_\BbbK \), then \( \sum_{k=1}^n t_k a_k \in \linspan A \) and thus \( t_1 = \cdots = t_n = 0_\BbbK \).

  Otherwise, we can divide by \( t_0 \) to obtain
  \begin{equation*}
    x = -\sum_{k=1}^n \frac {t_k} {t_0} a_k,
  \end{equation*}
  which implies that \( x \in \linspan A \), contradicting our choice of \( x \).

  Therefore, \( A \cup \set{ x } \) satisfies \fullref{def:linear_dependence}.

  \SubProofOf{thm:def:vector_space/maximal_independent} Let \( A \) be a maximally linearly independent set.

  Suppose that it is not a spanning set and let \( x \in V \setminus \linspan A \). By \fullref{thm:def:vector_space/span_independent}, the set \( A \cup \set{ x } \) is linearly independent, contradicting the maximality of \( A \).

  Therefore, \( A \) is a spanning set for \( V \).

  \SubProofOf{thm:def:vector_space/minimal_spanning} Let \( A \) be a minimal spanning set. Suppose that it is linearly dependent. Then there exist distinct vectors \( x_1, \ldots, x_n \) in \( A \) and scalars \( t_1, \ldots, t_n \), at least one of which is nonzero, such that
  \begin{equation*}
    \sum_{k=1}^n t_k x_k = 0_M.
  \end{equation*}

  Let \( k_0 \) be the smallest index such that \( t_{k_0} \neq 0_R \). Then
  \begin{equation*}
    x_{k_0} = -\sum_{k \neq k_0} \frac {t_k} {t_{k_0}} a_k.
  \end{equation*}

  Then \( \linspan A = \linspan A \setminus \set{ x_{k_0} } \), contradicting the minimality of \( A \).

  \SubProofOf{thm:def:vector_space/expansion} Let \( a_1, \ldots, a_n \) be a basis of \( U \) and let \( b_1, \ldots, b_m \) be a basis of \( V \).

  We use recursion on \( k \leq n \) to build linearly independent sets of the form
  \begin{equation*}
    \set{ a_1, \ldots, a_n, b_{i_1}, \ldots, b_{i_k} },
  \end{equation*}
  at least one of which will be a basis.

  The base case \( k = 0 \) is vacuous. Now suppose that the vectors \( a_1, \ldots, a_n, b_{i_1}, \ldots, b_{i_k} \) are linearly independent. If there exists an index \( i_{k+1} \) distinct from \( i_1, \ldots, i_k \) such that \( b_{i_{k+1}} \) is not in
  \begin{equation*}
    L_k \coloneqq \linspan\set{ a_1, \ldots, a_n, b_{i_1}, \ldots, b_{i_k} },
  \end{equation*}
  then, by \fullref{thm:def:vector_space/span_independent}, the vectors \( a_1, \ldots, a_n, b_{i_1}, \ldots, b_{i_k} \) are linearly independent. Otherwise, every basis vector \( b_1, \ldots, b_n \) belongs to \( L_k \), hence it is a spanning set of \( V \) of linearly independent vectors.

  Since the basis \( b_1, \ldots, b_n \) of \( V \) necessarily belongs to \( L_n \), it follows that \( L_k \) will be a basis of \( V \) for some \( k \leq n \).

  \SubProofOf{thm:def:vector_space/dimension_lemma} We will use induction on \( n \). The base case \( n = 0 \) holds because \( \set{ 0_V } \) is the only submodule of the linear span of zero vectors.

  Suppose that the statement holds for \( n - 1 \) and note that \( b_m \) can be \hyperref[def:basis_decomposition]{decomposed} as
  \begin{equation*}
    b_m = \sum_{k=1}^n t_k a_k.
  \end{equation*}

  Then
  \begin{equation*}
    a_k = \frac 1 {t_{k_0}} b_m - \sum_{k \neq k_0} \frac {t_k} {t_{k_0}} a_{k_0}.
  \end{equation*}

  Thus,
  \begin{equation*}
    \linspan\set{ a_1, \ldots, a_n } = \linspan\set{ a_1, \ldots, a_{k_0 - 1}, a_{k_0 + 1}, \ldots, a_n, b_m }.
  \end{equation*}

  We can thus remove \( b_m \) to obtain the inclusion
  \begin{equation*}
    \linspan\set{ b_1, \ldots, b_{m-1} } \subseteq \linspan\set{ a_1, \ldots, a_{k_0 - 1}, a_{k_0 + 1}, a_n }.
  \end{equation*}

  From the inductive hypothesis, we conclude that \( m - 1 \leq n - 1 \), and hence \( m \leq n \).
\end{proof}

\begin{theorem}[Vector space basis existence]\label{thm:vector_space_basis_existence}
  Every \hyperref[def:vector_space]{vector space} has a \hyperref[def:hamel_basis]{basis}.
\end{theorem}
\begin{comments}
  \item Within \hyperref[def:zfc]{\logic{ZF}}, this theorem is equivalent to the \hyperref[def:zfc/choice]{axiom of choice} --- see \fullref{thm:axiom_of_choice_equivalences/vector_space_bases}.
\end{comments}
\begin{proof}
  \ImplicationSubProof[thm:zorns_lemma]{Zorn's lemma}[thm:vector_space_basis_existence]{vector space existence} Let \( V \) be a vector space over \( \BbbK \). Let \( \mathcal{B} \) be the family of all linearly independent \hyperref[rem:linear_combinations]{subsets} of \( V \).

  The family \( \mathcal{B} \) is nonempty since any \hyperref[rem:singleton_sets]{singleton} from \( V \) belongs to \( \mathcal{B} \). The union of any chain \( \mathcal{B}' \subseteq \mathcal{B} \) can then contain only linearly independent elements since otherwise we would have that some set in \( \mathcal{B}' \) is not linearly independent. Thus, \fullref{thm:zorns_lemma} shows the existence of a maximal linearly independent set \( E \). By \fullref{thm:def:vector_space/maximal_independent}, \( E \) is a basis.

  \ImplicationSubProof[thm:vector_space_basis_existence]{vector space existence}[def:zfc/choice]{the axiom of choice} Shown in \cite{Blass1984}.
\end{proof}

\begin{proposition}\label{thm:vector_space_dimension}
  All \hyperref[def:hamel_basis]{bases} of a \hyperref[def:vector_space]{vector space} are \hyperref[def:equinumerosity]{equinumerous}.

  We define the \term{dimension} \( \dim V \) of the vector space \( V \) as the \hyperref[thm:cardinality_existence]{cardinality} of any of its bases. By \fullref{thm:vector_space_basis_existence}, every vector space has a dimension.
\end{proposition}
\begin{proof}
  Let \( A \) and \( E \) be bases of \( V \).

  \SubProof{Proof for finite bases} Suppose that \( a_1, \ldots, a_n \) are the vectors of \( A \) and, aiming at a contradiction, suppose that \( E \) is infinite.

  Every vector \( a_i \) of \( A \) can be \hyperref[def:basis_decomposition]{decomposed} along \( E \) as
  \begin{equation*}
    a_i = \sum_{j=1}^{m_i} t^{(i)}_j e^{(i)}_j.
  \end{equation*}
  for appropriate scalars from \( \BbbK \) and vectors from \( E \). Since this can be done for every \( a_i \) in \( A \), we conclude that
  \begin{equation*}
    V = \linspan\set{ a_1, \ldots, a_n } = \linspan\set{ e^{(1)}_1, \ldots, e^{(1)}_{m_1}, \ldots, e^{(n)}_1, \ldots, e^{(n)}_{m_n} }.
  \end{equation*}

  Hence, a finite subset of \( E \) spans \( V \), contradicting \fullref{thm:def:hamel_basis/minimal_spanning}. Therefore, \( E \) must be a finite set.

  Let \( b_1, \ldots, b_m \) be the vectors of \( E \). By applying \fullref{thm:def:vector_space/dimension_lemma} twice, we conclude that \( n \leq m \) and \( m \leq n \), hence \( n = m \).

  \SubProof{Proof for infinite bases} Suppose that both \( A \) and \( E \) are infinite. Let \( S_x \) be the set of vectors in \( E \) with nonzero coefficients in the decomposition of \( x \in V \).
  \begin{itemize}
    \item \( S_x \) is necessarily a finite set.
    \item Every vector \( x \) in \( V \) belongs to \( S_a \) for some \( a \in A \).

    Indeed, \( x \) can be decomposed along \( A \) as
    \begin{equation*}
      x = \sum_{a \in A} \pi_a(x) \cdot a,
    \end{equation*}
    and \( x \in S_{a_k} \) whenever \( \pi(a) \neq 0_\BbbK \).

    \item For every basis vector \( e \) in \( E \), \( S_e = \set{ e } \).

    \item We have
    \begin{equation*}
      E \subseteq \bigcup_{a \in A} S_a
    \end{equation*}
  \end{itemize}

  Therefore,
  \begin{equation*}
    \card(B)
    \leq
    \card\parens[\Bigg]{ \bigcup_{a \in A} S_a }
    \leq
    \card\parens[\Bigg]{ \coprod_{a \in A} S_a }
    \leq
    \card(A \times \omega)
    \leq
    \card(A) \cdot \aleph_0
    \reloset {\ref{thm:simplified_cardinal_arithmetic/infinite}} \leq
    \card(A).
  \end{equation*}

  Since \( A \) and \( E \) were arbitrary bases, we can exchange them to obtain the converse inequality, and thus \( \card(A) = \card(B) \).
\end{proof}

\begin{proposition}\label{thm:commutative_module_rank}
  All \hyperref[def:hamel_basis]{bases} of a \hyperref[def:module]{module} over a nontrivial \hyperref[def:ring/commutative]{commutative ring} are \hyperref[def:equinumerosity]{equinumerous}.

  We define the \term{rank} of a module as the \hyperref[thm:cardinality_existence]{cardinality} of any of its bases. This is a generalization of \hyperref[thm:vector_space_dimension]{vector space dimensions}. Unlike vector space dimensions, however, module ranks may not exist --- see \fullref{ex:module_without_basis}. Modules with at least one basis are often called \term{free}, however we will prefer to use the term for modules with a concrete bases, as in \fullref{def:free_semimodule}.
\end{proposition}
\begin{proof}
  Suppose that \( A \) and \( E \) are bases of the \( R \)-module \( M \). Let \( I \) be a \hyperref[def:semiring_ideal/maximal]{maximal ideal} of \( R \).

  By \fullref{thm:quotient_by_maximal_ideal}, \( \BbbK = R / I \) is a field (this is a forward reference to \fullref{thm:quotient_by_maximal_ideal}). Given an isomorphism \( \Phi: R^{\oplus A} \to R^{\oplus E} \) with components \( \Phi_e: R^{\oplus A} \to R \), define
  \begin{equation*}
    \begin{aligned}
      &\Psi: \BbbK^{\oplus A} \to \BbbK^{\oplus E} \\
      &\Psi\parens[\Big]{ \seq{ t_a + I }_{a \in A} } \coloneqq \seq[\Big]{ \Phi_e(\seq{ t_a }_{a \in A}) + I }_{e \in E}
    \end{aligned}
  \end{equation*}

  This is clearly an isomorphism if it is well-defined. Indeed, suppose we are given \( t_a + I = t_a' + I \) for every \( a \in A \), i.e. \( t_a - t_a' \in I \). Then
  \small
  \begin{equation*}
    \Phi_e(\seq{ t_a }_{a \in A}) - \Phi_e(\seq{ t_a' }_{a \in A})
    =
    \Phi_e(\seq{ t_a }_{a \in A} - \seq{ t_a' }_{a \in A})
    =
    \Phi_e(\seq{ t_a - t_a' }_{a \in A})
    =
    \sum_{a \in A} (\underbrace{ t_a - t_a' }_I) \Phi_e(a)
    \in
    I.
  \end{equation*}
  \normalsize

  Thus,
  \begin{equation*}
    \Phi_e(\seq{ t_a }_{a \in A}) + I = \Phi_e(\seq{ t_a' }_{a \in A}) + I.
  \end{equation*}

  Therefore, \( \Psi \) is a well-defined linear isomorphism. Applying \fullref{thm:vector_space_dimension}, we conclude that \( A \) and \( E \) are equinumerous.
\end{proof}

\begin{corollary}\label{thm:finitely_generated_module_basis}
  For a nontrivial commutative ring \( R \), if a \hyperref[def:module_presentation]{finitely-generated} \( R \)-module has a \hyperref[def:hamel_basis]{basis}, the basis is finite.
\end{corollary}
\begin{proof}
  Follows from \fullref{thm:commutative_module_rank}.
\end{proof}

\begin{example}\label{ex:field_submodules}
  For a nontrivial \hyperref[def:ring/commutative]{commutative ring} \( R \), every ideal of \( R \) is a submodule of the \hyperref[thm:commutative_module_rank]{rank-one} \hyperref[def:module]{module} \( R \). For example, both \( \BbbZ \) and \( 2\BbbZ \) are \( \BbbZ \)-modules of rank one.

  For a \hyperref[def:field]{field} \( \BbbK \), every ideal of \( \BbbK \) is a submodule of the \hyperref[thm:vector_space_dimension]{unidimensional} \hyperref[def:vector_space]{vector space} \( \BbbK \). The only ideals of \( \BbbK \) are the zero ideal, whose dimension is zero, and the field itself, whose dimension is one.
\end{example}

\begin{proposition}\label{thm:modules_with_same_rank_are_isomorphic}
  Two \( R \)-modules having the same \hyperref[thm:commutative_module_rank]{rank} are \hyperref[def:semimodule/homomorphism]{linearly isomorphic}.
\end{proposition}
\begin{proof}
  Suppose that \( A \) is a \hyperref[def:hamel_basis]{basis} of \( M \) and \( E \) is a basis of \( N \). Also suppose that both \( M \) and \( N \) have the same rank.

  Then \( A \) is equinumerous with \( E \) and there exists a bijective function \( f: A \to E \). We can define the map \( \varphi: A \to R^{\oplus E} \) by sending \( a \) to the embedding of \( f(a) \) in \( R^{\oplus E} \). We can then \hyperref[thm:free_semimodule_universal_property]{extend} \( \varphi \) to a linear isomorphism \( \Phi: R^{\oplus A} \to R^{\oplus E} \).

  Therefore, there exists an isomorphism between \( M \) and \( R^{\oplus A} \), between \( R^{\oplus A} \) and \( R^{\oplus E} \) and between \( R^{\oplus E} \) and \( N \). Hence, \( M \) and \( N \) are isomorphic.
\end{proof}

\begin{lemma}\label{thm:vector_space_dimension_monotonicity}
  For vector spaces \( U \subseteq V \) we have \( \rank U \leq \rank V \).
\end{lemma}
\begin{proof}
  Let \( A \) be a basis of \( U \). By \fullref{thm:def:vector_space/expansion}, there exists a basis \( E \) of \( V \) such that \( A \subseteq E \). Then clearly \( \rank U \leq \rank V \).
\end{proof}

\begin{proposition}\label{thm:rank_nullity_via_subspaces}
  Let \( U \) be a subspace of the \hyperref[thm:vector_space_dimension]{finite-dimensional} \hyperref[def:vector_space]{vector space} \( V \). Then
  \begin{equation*}
    V \cong U \oplus (V / U).
  \end{equation*}
\end{proposition}
\begin{proof}
  By \fullref{thm:vector_space_dimension_monotonicity}, \( \dim U \leq \dim V \). Let \( a_1, \ldots, a_m \) be an ordered basis for \( U \). By \fullref{thm:def:vector_space/expansion}, there exist vectors \( a_{m+1}, \ldots, a_n \) such that \( a_1, \ldots, a_n \) is a basis of \( V \). We will show that \( a_{m+1} + U, \ldots, a_n + U \) is a basis of \( V / U \), thus demonstrating the isomorphism.

  First, we must show that it is a spanning set. Fix a vector \( x + U \) from \( V / U \). We know that \( x \) can be decomposed as
  \begin{equation*}
    x = \sum_{k=1}^n \pi_k(x) \cdot a_k.
  \end{equation*}

  Since \( a_1, \ldots, a_m \) belong to \( U \),
  \begin{equation*}
    x + U = \sum_{k={m+1}}^n \pi_k(x) \cdot a_k + U.
  \end{equation*}

  Hence, \( x + U \) can be represented as a linear combination of the vectors \( a_{m+1} + U, \ldots, a_n + U \).

  To show uniqueness, suppose that
  \begin{equation*}
    x + U = \sum_{k={m+1}}^n t_k (a_k + U) = \sum_{k={m+1}}^n r_k (a_k + U).
  \end{equation*}

  Then
  \begin{equation*}
    U = \sum_{k={m+1}}^n (t_k - r_k) (a_k + U).
  \end{equation*}

  Since neither of \( a_{m+1}, \ldots, a_n \) are in \( U \), it follows that \( t_k = r_k \) for \( k = m + 1, \ldots, n \).

  Therefore, \( a_{m+1} + U, \ldots, a_n + U \) is a basis of \( V / U \).
\end{proof}

\begin{proposition}\label{thm:rank_of_direct_sum}
  Assuming that the \( R \)-modules \( M_1, \ldots, M_n \) have bases, the \hyperref[thm:commutative_module_rank]{rank} of their direct sum
  \begin{equation*}
    M_1 \oplus \cdots \oplus M_n
  \end{equation*}
  is the \hyperref[def:cardinal_arithmetic/addition]{sum of cardinals}
  \begin{equation*}
    \rank M_1 + \cdots + \rank M_n.
  \end{equation*}
\end{proposition}
\begin{proof}
  Follows from \fullref{thm:basis_of_direct_sum}.
\end{proof}

\begin{theorem}[Rank-nullity theorem]\label{thm:rank_nullity_theorem}
  For every \hyperref[def:semimodule/homomorphism]{linear map} \( \varphi: U \to V \) between \hyperref[thm:vector_space_dimension]{finite-dimensional} \hyperref[def:vector_space]{vector spaces}, we have
  \begin{equation}\label{eq:thm:rank_nullity_theorem}
    U \cong \ker \varphi \oplus \img \varphi.
  \end{equation}

  In particular,
  \begin{equation}\label{eq:thm:rank_nullity_theorem/dim}
    \dim U = \dim \ker \varphi + \dim \img \varphi.
  \end{equation}

  The dimension of the kernel of \( \varphi \) is often called the \term{nullity} of \( \varphi \) and the dimension of the image - the \term{rank} of \( \varphi \)
\end{theorem}
\begin{proof}
  By \fullref{thm:quotient_module_universal_property},
  \begin{equation*}
    \img \varphi \cong U / \ker \varphi.
  \end{equation*}

  By \fullref{thm:rank_nullity_via_subspaces},
  \begin{equation*}
    U \cong \ker \varphi \oplus (U / \ker \varphi) \cong \ker \varphi \oplus \img \varphi.
  \end{equation*}

  The equality \eqref{thm:rank_nullity_theorem} then follows from \fullref{thm:rank_of_direct_sum}.
\end{proof}
