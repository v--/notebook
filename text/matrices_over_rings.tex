\section{Matrices over rings}\label{sec:matrices_over_rings}

We will define and prove some fundamental notions about matrices. We will start with matrices over plain sets and end up with matrices over nontrivial noetherian commutative rings. This is about as general as we want to go without the underlying ring being a field. The main benefit is being able to work with the \hyperref[def:integers]{ring of integers} or more general semirings, like the \hyperref[def:tropical_semiring]{tropical semirings}.

\begin{definition}\label{def:array}\mimprovised
  Let \( S \) be any nonempty \hyperref[def:set]{set} and \( n_1, \ldots, n_k \) be \hyperref[def:integer_signum]{positive integers}. An \term[ru=массив (\cite[sec. 40.1]{Тыртышников2007ЛинейнаяАлгебра})]{array} of shape \( n_1 \times \cdots \times n_k \) is a \hyperref[def:function]{function} with signature
  \begin{equation*}
    A: \set{ 1, 2, \ldots, n_1 } \times \ldots \times \set{ 1, 2, \ldots, n_k } \to S.
  \end{equation*}

  \enquote{Multi-dimensional array} is also used as a term, but we will avoid it because the terminology conflicts with \hyperref[thm:vector_space_dimension]{vector space dimensions}.

  We can regard \enquote{\( n_1 \times \cdots \times n_k \)} simultaneously as a convenient notation and as a \hyperref[def:cartesian_product]{Cartesian product} of finite \hyperref[def:ordinal]{ordinals} (modulo the fact that finite ordinals are zero-based).

  In particular:
  \begin{thmenum}
    \thmitem{def:array/matrix} A two-dimensional array of shape \( m \times n \) is usually called a \term[ru=матрица (\cite[18]{Обрешков1962ВисшаАлгебра}), ru=матрица (\cite[\S 1.1]{Тыртышников2007ЛинейнаяАлгебра})]{matrix}. Let \( A \) be an \( m \times n \)-matrix. We will denote \( A \) as
    \begin{equation*}
      A = \seq{ a_{i,j} }_{i,j=1}^{m,n}
    \end{equation*}
    or graphically as the table
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{m,1} & a_{m,2} & \cdots & a_{m,n}
      \end{pmatrix}.
    \end{equation*}

    \thmitem{def:array/square_matrix} A \term[bg=квадратна матрица (\cite[18]{Обрешков1962ВисшаАлгебра}), ru=квадратна матрица (\cite[\S 1.1]{Тыртышников2007ЛинейнаяАлгебра})]{square matrix} of order \( n \) is simply an \( n \times n \) matrix.

    \thmitem{def:array/column_vector} A \term{column vector} of dimension \( m \) is simply a \( m \times 1 \) matrix
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} \\
        \vdots  \\
        a_{m,1}
      \end{pmatrix}.
    \end{equation*}

    When \( S \) is a \hyperref[def:semiring]{semiring} \( R \), we often identify the set of all \( m \)-dimensional column vectors with the \hyperref[def:sequence_space]{coordinate space} \( R^m \).

    \thmitem{def:array/row_vector} A \term{row vector} of dimension \( n \) is simply an \( 1 \times n \) matrix
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,n}
      \end{pmatrix}.
    \end{equation*}
  \end{thmenum}
\end{definition}

\begin{example}\label{ex:def:array}
  We give some examples of \hyperref[def:array]{arrays}:
  \begin{thmenum}
    \thmitem{ex:def:array/zero_dimensional} The unique array with signature \( (0, \ldots, 0) \) may seem pathological, however it behaves well and will serve as an important example.

    We will call the unique \( 0 \times 0 \) matrix the \term[en=zero-dimensional matrix (\cite[example 1.5.19]{Perrone2024StartingCategoryTheory})]{zero-dimensional matrix}.
  \end{thmenum}
\end{example}

\begin{remark}\label{rem:vector_etymology}
  In practice, the terms \enquote{vector}, \enquote{tuple} and \enquote{finite sequence} are used interchangeably. Formally, the concepts differ slightly:

  \begin{itemize}
    \item \enquote{Vector} refers to an element of a \hyperref[def:vector_space]{vector space} or, more generally, a \hyperref[def:semimodule]{semimodule}. \hyperref[def:array/column_vector]{Column vectors} and \hyperref[def:array/row_vector]{row vectors} are important special cases.

    \item Ordered tuples are defined and discussed in \fullref{def:ordered_tuple}. Tuples are technically \hyperref[def:indexed_family]{indexed families} and the latter are defined without reference to functions, which we use to define both arrays and sequences.
  \end{itemize}
\end{remark}

\begin{definition}\label{def:block_matrix}
  A \term{block matrix} is a \enquote{matrix of matrices}. That is, a matrix of the form
  \begin{equation*}
    \begin{pmatrix}
      A_{1,1} & A_{1,2} & \cdots & A_{1,n} \\
      A_{2,1} & A_{2,2} & \cdots & A_{2,n} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      A_{m,1} & A_{m,2} & \cdots & A_{m,n}
    \end{pmatrix},
  \end{equation*}
  where all \( A_{i,j} \) are matrices of compatible dimensions.

  We can write the block matrix
  \begin{equation*}
    \begin{pmatrix}
      A      & \cdots & B      \\
      \vdots & \ddots & \vdots \\
      C      & \cdots & D
    \end{pmatrix}
  \end{equation*}
  as
  \begin{equation*}
    \parens*
      {
        \begin{array}{ccc|c|ccc}
          a_{1,1}   & \cdots & a_{1,n_A}   & \cdots & b_{1,1}   & \cdots & b_{1,n_B} \\
          \vdots    & \ddots & \vdots      & \cdots & \vdots    & \ddots & \vdots \\
          a_{m_A,1} & \cdots & a_{m_A,n_A} & \cdots & b_{m_B,1} & \cdots & b_{m_B,n_B} \\
          \hline
          \vdots    & \vdots & \vdots      & \ddots & \vdots    & \vdots & \vdots \\
          \hline
          c_{1,1}   & \cdots & c_{1,n_C}   & \cdots & d_{1,1}   & \cdots & d_{1,n_D} \\
          \vdots    & \ddots & \vdots      & \cdots & \vdots    & \ddots & \vdots \\
          c_{m_C,1} & \cdots & c_{m_C,n_C} & \cdots & d_{m_D,1} & \cdots & d_{m_D,n_D} \\
        \end{array}
      }.
  \end{equation*}

  Given any matrix \( A = \seq{ a_{i,j} }_{i,j=1}^{n,m} \), we can represent it via its block matrix of rows
  \begin{equation*}
    \parens*
      {
        \begin{array}{c}
          a_{1,\anon*} \\
          \hline
          a_{2,\anon*} \\
          \hline
          \vdots \\
          \hline
          a_{n,\anon*}
        \end{array}
      },
  \end{equation*}
  and its block matrix of columns
  \begin{equation*}
    \parens*
      {
        \begin{array}{c|c|c|c}
          a_{\anon*,1} & a_{\anon*,2} & \cdots & a_{\anon*,m}
        \end{array}
      }
  \end{equation*}
\end{definition}

\begin{definition}\label{def:matrix_diagonal}
  The \term{main diagonal} of the matrix \( A = \seq{ a_{i,j} }_{i,j=1}^{m,n} \) is the sequence \( a_{1,1}, \ldots, a_{i,i}, \ldots, a_{k,k} \), where \( k \coloneqq \min\set{ m, n } \). The \term{antidiagonal} is instead \( a_{1,k}, \ldots, a_{i,k-i}, \ldots, a_{k,n-k} \). These can be visualized as follows:
  \begin{equation*}
    \begin{pmatrix}
      \fbox{\( a_{1,1} \)} & a_{1,2}                & \cdots & a_{1,k-1}                & \fbox{\( a_{k,k} \)} & \cdots \\
      a_{2,1}              & \fbox{\( a_{2,2} \)}   &        & \fbox{\( a_{2,k-1} \)}   & a_{2,k}              &        \\
      \vdots               &                        & \ddots &                          & \vdots               &        \\
      a_{k-1,1}            & \fbox{\( a_{k-1,2} \)} &        & \fbox{\( a_{k-1,k-1} \)} & a_{k-1,k}            & \cdots \\
      \fbox{\( a_{k,1} \)} & a_{k,2}                & \cdots & a_{k,k-1}                & \fbox{\( a_{k,k} \)} &        \\
      \vdots               &                        &        & \vdots                   &                      & \ddots
    \end{pmatrix}
  \end{equation*}

  Over a \hyperref[def:semiring]{semiring}, we say that a square matrix \term{diagonal} if all entries outside the main diagonal are zero. For brevity, we write
  \begin{equation*}
    \op{diag}(a_1, \ldots, a_n)
    \coloneqq
    \begin{pmatrix}
      a_1    & 0      & \cdots & 0      \\
      0      & a_2    & \cdots & 0      \\
      \vdots & \vdots & \ddots & \vdots \\
      0      & 0      & \cdots & a_n
    \end{pmatrix}
  \end{equation*}

  The notation \( \op{diag}(A) \) is also used to denote the sequence of diagonal entries of \( A \).
\end{definition}

\begin{proposition}\label{thm:matrix_algebra}
  Denote by \( R^{m \times n} \) the set of \( m \times n \) \hyperref[def:array/matrix]{matrices} over the \hyperref[def:semiring]{semiring} \( R \). We define three operations on matrices:
  \begin{thmenum}
    \thmitem{thm:matrix_algebra/addition} We define \term{matrix addition} \( +: R^{m \times n} \times R^{m \times n} \to R^{m \times n} \) componentwise as
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,n} \\
        \vdots  & \ddots & \vdots  \\
        a_{m,1} & \cdots & a_{m,n}
      \end{pmatrix}
      +
      \begin{pmatrix}
        b_{1,1} & \cdots & b_{1,n} \\
        \vdots  & \ddots & \vdots  \\
        b_{m,1} & \cdots & b_{m,n}
      \end{pmatrix}
      \coloneqq
      \begin{pmatrix}
        a_{1,1} + b_{1,1} & \cdots & a_{1,n} + b_{1,n} \\
        \vdots            & \ddots & \vdots            \\
        a_{m,1} + b_{m,1} & \cdots & a_{m,n} + b_{m,n}
      \end{pmatrix}.
    \end{equation*}

    With addition, \( R^{m \times n} \) becomes an \hyperref[def:binary_operation/commutative]{commutative} \hyperref[def:monoid]{monoid} with neutral element the \term{zero matrix}
    \begin{equation}\label{eq:thm:matrix_algebra/matrix_multiplication/zero}
      \begin{pmatrix}
        0       & 0      & \cdots & 0      \\
        0       & 0      & \cdots & 0      \\
        \vdots  & \cdots & \ddots & \vdots \\
        0       &        & \cdots & 0
      \end{pmatrix}.
    \end{equation}

    \thmitem{thm:matrix_algebra/scalar_multiplication} We define \term{scalar multiplication} \( \cdot: R \times R^{m \times n} \to R^{m \times n} \) as
    \begin{equation*}
       \lambda \cdot \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,n} \\
        \vdots  & \ddots & \vdots  \\
        a_{m,1} & \cdots & a_{m,n}
      \end{pmatrix}
      \coloneqq
      \begin{pmatrix}
        \lambda a_{1,1} & \cdots & \lambda a_{1,n} \\
        \vdots          & \ddots & \vdots          \\
        \lambda a_{m,1} & \cdots & \lambda a_{m,n}
      \end{pmatrix}.
    \end{equation*}

    Under \hyperref[thm:matrix_algebra/addition]{addition} and \hyperref[thm:matrix_algebra/scalar_multiplication]{scalar multiplication}, \( R^{m \times n} \) becomes an \( R \)-\hyperref[def:semimodule]{semimodule}.

    \thmitem{thm:matrix_algebra/matrix_multiplication} We define \term{matrix multiplication} in two steps. The definition is justified by \fullref{thm:matrix_and_linear_function_algebras}. First, if \( \seq{ a_j }_{j=1}^n \) is a \hyperref[def:array/row_vector]{row vector} and \( \seq{ b_i }_{j=1}^n \) is a \hyperref[def:array/column_vector]{column vector}, we define their \term{inner product} as
    \begin{equation*}
      a \cdot b \coloneqq \sum_{i=1}^n a_i b_i.
    \end{equation*}

    We can now define matrix multiplication \( \cdot: R^{m \times n} \times R^{n \times k} \to R^{m \times k} \) as
    \begin{equation*}
     \parens*
       {
         \begin{array}{c}
            a_{1,-} \\
            a_{2,-} \\
            \vdots \\
            a_{m,-}
          \end{array}
        }
      \cdot
      \parens*
        {
          \begin{array}{c|c|c|c}
            b_{-,1} & b_{-,2} & \cdots & b_{-,k}
          \end{array}
        }
      \coloneqq
      \begin{pmatrix}
        a_{1,-} \cdot b_{-,1} & a_{1,-} \cdot b_{-,2} & \vdots & a_{1,-} \cdot b_{-,k} \\
        a_{2,-} \cdot b_{-,1} & a_{2,-} \cdot b_{-,2} & \vdots & a_{2,-} \cdot b_{-,k} \\
        \vdots                & \vdots                & \ddots & \vdots                \\
        a_{m,-} \cdot b_{-,1} & a_{m,-} \cdot b_{-,2} & \cdots & a_{m,-} \cdot b_{-,k}
      \end{pmatrix}.
    \end{equation*}

    If \( n \), \( m \) and \( k \) are equal, \( R^{n \times n} \) becomes an \( R \)-\hyperref[def:algebra_over_semiring]{algebra} under \hyperref[thm:matrix_algebra/matrix_multiplication]{matrix multiplication} with multiplicative identity the \term{identity matrix} of order \( n \)
    \begin{equation}\label{eq:thm:matrix_algebra/matrix_multiplication/identity}
      \op{diag}(\underbrace{ 1, \cdots, 1 }_{n \T*{ones}})
      =
      \begin{pmatrix}
        1       & 0      & \cdots & 0      \\
        0       & 1      & \cdots & 0      \\
        \vdots  & \ddots & \ddots & \vdots \\
        0       &        & \cdots & 1
      \end{pmatrix}.
    \end{equation}
  \end{thmenum}
\end{proposition}
\begin{proof}
  \todo{Revise proof}

  The semimodule structure is inherited by the \hyperref[thm:commutative_monoid_is_semimodule]{semimodule structure} on \( R \). We will show that, if \( n = n \), matrix multiplication is associative and bilinear. Fix matrices
  \begin{equation*}
    \begin{aligned}
      A = \seq{ a_{i,j} }_{i,j=1}^{m,k} && B = \seq{ b_{i,j} }_{i,j=1}^{k,l} && C = \seq{ c_{i,j} }_{i,j=1}^{l,n}.
    \end{aligned}
  \end{equation*}

  \SubProofOf[def:binary_operation/associative]{associativity} The \( (i, j) \)-th entry in \( D \coloneqq (AB)C \) is
  \begin{equation*}
    d_{i,j} = \sum_{s=1}^n \parens*{ \sum_{r=1}^n a_{i,r} \cdot b_{r,s} } \cdot c_{s,j}.
  \end{equation*}

  Due to distributivity,
  \begin{equation*}
    d_{i,j}
    =
    \sum_{s=1}^n \sum_{r=1}^n a_{i,r} \cdot b_{r,s} \cdot c_{s,j}
    =
    \sum_{r=1}^n a_{i,r} \cdot \parens*{ \sum_{s=1}^n b_{r,s} \cdot c_{s,j} },
  \end{equation*}
  which is the \( (i, j) \)-th entry in \( A(BC) \).

  Therefore, \( (AB)C = A(BC) \).

  \SubProofOf[eq:def:additive_function]{additivity} Again due to distributivity,
  \begin{equation*}
    \sum_{r=1}^n \parens*{ a_{i,r} + b_{i,r} } \cdot c_{r,j}
    =
    \sum_{r=1}^n a_{i,r} \cdot c_{r,j} + \sum_{r=1}^n b_{i,r} \cdot c_{r,j}.
  \end{equation*}

  Therefore, \( (A + B)C = AC + BC \). The proof that \( A(B + C) = AB + AC \) is analogous.

  \SubProofOf[def:homogeneous_function]{homogeneity} Again due to distributivity,
  \begin{equation*}
    t \cdot \sum_{r=1}^n a_{i,r} \cdot b_{r,j}
    =
    \sum_{r=1}^n (t \cdot a_{i,r}) \cdot b_{r,j}
    =
    \sum_{r=1}^n a_{i,r} \cdot (t \cdot b_{r,j}).
  \end{equation*}

  Therefore, \( t(AB) = (tA)B = A(tB) \).
\end{proof}

\begin{proposition}\label{thm:columns_and_rows_of_matrix_product}
  The \( i \)-th row of \( AB \) is the product of \( A \) by the \( i \)-th row of \( A \) by \( B \).

  Similarly, the \( j \)-th column of \( AB \) is the product of \( A \) by the \( j \)-th column of \( B \).
\end{proposition}
\begin{proof}
  Straightforward.
\end{proof}

\begin{example}\label{ex:matrix_multiplication_is_noncommutative}
  For \( n > 1 \), the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) is a noncommutative ring. Consider the following example:
  \begin{align*}
    \begin{pmatrix}
      0 & 0 \\
      0 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix}
    &=
    \begin{pmatrix}
      0 & 0 \\
      1 & 0
    \end{pmatrix},
    \\
    \begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix}
    \begin{pmatrix}
      0 & 0 \\
      0 & 1
    \end{pmatrix}
    &=
    \begin{pmatrix}
      0 & 0 \\
      0 & 0
    \end{pmatrix}.
  \end{align*}
\end{example}

\begin{remark}\label{rem:matrices_as_functions}
  Let \( R \) be a \hyperref[def:ring/commutative]{commutative ring} and let \( e_1, \ldots, e_n \) be the \hyperref[def:sequence_space]{standard basis} of \( R^n \). The \hyperref[def:basis_decomposition]{coordinate projections} \( \pi_{e_1}, \ldots, \pi_{e_n} \) allow us to identify \( R^n \) with the module \( R^{n \times 1} \) of \hyperref[def:array/column_vector]{column vectors} by regarding the vector \( x \) from \( R^n \) as the column vector
  \begin{equation*}
    \begin{pmatrix}
      \pi_{e_1}(x) \\
      \vdots \\
      \pi_{e_n}(x)
    \end{pmatrix}.
  \end{equation*}

  Under this identification, the columns on the identity matrix \eqref{eq:thm:matrix_algebra/matrix_multiplication/identity} are precisely the column vectors of the standard basis.

  Let \( A \) be an \( m \times n \) matrix over \( R \). If we regard \( R^n \) as a set of column vectors, then \hyperref[thm:matrix_algebra/matrix_multiplication]{matrix multiplication} allows us to regard \( A \) as the function \( x \mapsto Ax \), which maps column vectors from \( R^n \) to column vectors in \( R^m \).

  This justifies using juxtaposition for application of linear maps, e.g. \( Lx \) rather than \( L(x) \).

  Conversely, let \( e_1, \ldots, e_n \) be the standard basis of \( R^n \) and \( f_1, \ldots, f_m \) --- of \( R^m \). The linear map \( L: R^n \to R^m \) corresponds to the following matrix:
  \begin{equation*}
    \begin{pmatrix}
      \pi_{e_1}(L f_1) & \cdots & \pi_{e_1}(L f_1) \\
      \vdots           & \ddots & \vdots       \\
      \pi_{e_n}(L f_m) & \cdots & \pi_{e_n}(L f_m)
    \end{pmatrix}.
  \end{equation*}
\end{remark}

\begin{proposition}\label{thm:matrix_and_linear_function_algebras}
  For a \hyperref[def:ring/commutative]{commutative ring} \( R \), the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{m \times n} \) is \hyperref[def:algebra_over_semiring/homomorphism]{isomorphic} to the \hyperref[thm:functions_over_algebra]{linear function algebra} \( \hom(R^n, R^m) \)\fnote{Note that the maps are from \( R^n \) to \( R^m \) and not vice versa}.
\end{proposition}
\begin{proof}
  Follows from our discussion in \fullref{rem:matrices_as_functions} due to linearity.
\end{proof}

\begin{remark}\label{rem:double_index_maps}
  We want to be able to map single indices to double indices and vice versa, for example for the purpose of \fullref{thm:matrix_spaces_are_free_modules}. As an example, we want to be able to \enquote{linearize} an \( m \times n \) matrix such as the \( 2 \times 3 \) matrix
  \begin{equation}\label{eq:rem:double_index_maps/example/matrix}
    \begin{pmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6
    \end{pmatrix}
  \end{equation}
  into the tuple
  \begin{equation}\label{eq:rem:double_index_maps/example/row_major}
    (1, 2, 3, 4, 5, 6)
  \end{equation}
  and vice versa. This is called \term{row-major order} of the elements of a matrix. The \term{column-major order} would instead be
  \begin{equation}\label{eq:rem:double_index_maps/example/column_major}
    (1, 4, 2, 5, 3, 6).
  \end{equation}

  Let \( m \) and \( n \) be \hyperref[def:integer_signum]{positive integers}. We will explicitly define functions for linearizing a matrix like \eqref{eq:rem:double_index_maps/example/matrix} into its row-major order \eqref{eq:rem:double_index_maps/example/row_major}. Consider the sets
  \begin{align*}
    S &\coloneqq \overbrace{ \set{ 1, \ldots, mn - 1, mn } }^{\T{single indices}}
    \\
    D &\coloneqq \underbrace{ \set{ 1, \ldots, m } \times \set{ 1, \ldots, n } }_{\T{double indices}}
  \end{align*}
  and the mutually inverse operations
  \begin{align}
    &\begin{aligned}\label{eq:rem:double_index_maps/sharp}
      &\sharp: S \to D \\
      &\sharp(k) \coloneqq \parens[\Big]{ \quot(k - 1, m) + 1, \rem(k - 1, m) + 1 } \\
    \end{aligned}
    \\[0.5\baselineskip]
    &\begin{aligned}\label{eq:rem:double_index_maps/flat}
      &\flat: D \to S \\
      &\flat(i, j) \coloneqq (i - 1) \cdot m + (j - 1) + 1.
    \end{aligned}
  \end{align}

  The operation \( \sharp \) encodes the matrix \eqref{eq:rem:double_index_maps/example/matrix} into its row-major order \eqref{eq:rem:double_index_maps/example/row_major} and \( \flat \) does the opposite. Both operations are trivial except for the shifting needed in to allow us to use \hyperref[def:euclidean_domain]{remainders and quotients}.

  We can easily verify that \( \sharp \) is a \hyperref[def:morphism_invertibility/left_invertible]{left inverse} of \( \flat \) (note that \( j < m \)):
  \begin{align*}
    \sharp(\flat(i, j))
    &=
    \sharp\parens[\Big]{ (i - 1) \cdot m + (j - 1) + 1 }
    = \\ &=
    \parens[\Big]{ \quot(\cdots, m) + 1, \rem(\cdots, m) + 1 }
    = \\ &=
    \parens[\Big]{ (i - 1) + 1, (j - 1) + 1 }
    = \\ &=
    (i, j).
  \end{align*}

  We can just as easily verify that \( \flat \) is a \hyperref[def:morphism_invertibility/right_invertible]{right inverse} of \( \sharp \):
  \begin{align*}
    \flat(\sharp(k))
    &=
    \flat\parens[\Big]{ \quot(k, m) + 1, \rem(k, m) + 1 }
    = \\ &=
    \quot(k, m) \cdot m + \rem(k, m)
    = \\ &=
    k.
  \end{align*}

  Hence, \( \sharp \) is fully invertible with inverse \( \flat \). By \fullref{thm:function_invertibility_categorical/fully_invertible}, it is bijective.
\end{remark}

\begin{proposition}\label{thm:matrix_spaces_are_free_modules}
  The \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{m \times n} \) is isomorphic as a \hyperref[def:semimodule]{semimodule} to \( R^{mn} \).
\end{proposition}
\begin{proof}
  \Fullref{rem:double_index_maps} gives us a semimodule isomorphism between \( m \times n \) matrices and \( mn \)-dimensional column vectors when extended to linear maps via \fullref{thm:free_semimodule_universal_property}.
\end{proof}

\begin{definition}\label{def:matrix_determinant}\mcite[215]{Knapp2016BasicAlgebra}
  The \term{determinant} for the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) over the \hyperref[def:semiring/commutative]{commutative semiring} \( R \) is the function
  \begin{equation}\label{eq:def:matrix_determinant}
    \begin{aligned}
      &\det: R^{n \times n} \to R \\
      &\det(\seq{ a_{i,j} }_{i,j=1}^n) \coloneqq \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n a_{i,\sigma(i)},
    \end{aligned}
  \end{equation}
  where \( S_n \) is the \hyperref[def:symmetric_group]{symmetric group} and \( \sgn \) is the \hyperref[def:permutation_parity]{sign} of the permutation \( \sigma \).

  Following \incite[493]{Eisenbud1995CommutativeAlgebra}, we will define the determinant for the \hyperref[ex:def:array/zero_dimensional]{zero-dimensional matrix} as \( 1 \).

  \Fullref{thm:similar_matrices_and_determinants} allows us to define determinants for linear endomorphisms rather than square matrices.

  See our proof of \fullref{thm:determinant_on_columns} for a justification of the definition.
\end{definition}

\begin{definition}\label{def:symmetric_function}\mcite[def. 2.11.1]{Savage2008ModelsOfComputation}
  Given a function \( f: X^n \to Y \), where \( X \) and \( Y \) are \hyperref[def:set]{plain sets}, we say that \( f \) is \term{symmetric} if, for any \hyperref[def:symmetric_group]{permutation} \( \sigma \in S_n \), we have
  \begin{equation*}
    f(x_1, \ldots, x_n) = f(x_{\sigma(1)}, \ldots, x_{\sigma(n)}).
  \end{equation*}

  A permutation can be decomposed into \hyperref[def:cyclic_permutation]{transpositions} due to \fullref{thm:permutation_decomposition_existence}. Hence, the above condition reduces to the simpler condition of \( f \) being invariant with respect to swapping any two arguments. That is,
  \begin{equation*}
    f(\ldots, x_{i-1}, \fbox{\( x_i \)}, x_{i+1}, \cdots, x_{j-1}, \fbox{\( x_j \)}, x_{j+1}, \ldots)
    =
    f(\ldots, x_{i-1}, \fbox{\( x_j \)}, x_{i+1}, \cdots, x_{j-1}, \fbox{\( x_i \)}, x_{j+1}, \ldots).
  \end{equation*}

  In the case where \( n = 2 \), this reduces to the simple condition
  \begin{equation*}
    f(x, y) = f(y, x).
  \end{equation*}

  Symmetric functions should not be confused with symmetric binary relations defined in \fullref{def:binary_relation/symmetric}.
\end{definition}

\begin{definition}\label{def:antisymmetric_function}\mimprovised
  Given a function \( f: X^n \to Y \), where \( X \) is a \hyperref[def:set]{plain set} and \( Y \) is an \hyperref[con:additive_semigroup]{additive group}, we say that \( f \) is \term{antisymmetric} if, for any \hyperref[def:symmetric_group]{permutation} \( \sigma \in S_n \), we have
  \begin{equation*}
    f(x_1, \ldots, x_n) = \sgn(\sigma) \cdot f(x_{\sigma(1)}, \ldots, x_{\sigma(n)}).
  \end{equation*}

  A permutation can be decomposed into \hyperref[def:cyclic_permutation]{transpositions} due to \fullref{thm:permutation_decomposition_existence}. Hence, the above condition reduces to the simpler condition of \( f \) changing sign when swapping any two arguments. That is,
  \begin{equation*}
    f(\ldots, x_{i-1}, \fbox{\( x_i \)}, x_{i+1}, \cdots, x_{j-1}, \fbox{\( x_j \)}, x_{j+1}, \ldots)
    =
    -f(\ldots, x_{i-1}, \fbox{\( x_j \)}, x_{i+1}, \cdots, x_{j-1}, \fbox{\( x_i \)}, x_{j+1}, \ldots).
  \end{equation*}

  In the case where \( n = 2 \), this reduces to the simple condition
  \begin{equation*}
    f(x, y) = -f(y, x).
  \end{equation*}

  Antisymmetric functions should not be confused with antisymmetric binary relations defined in \fullref{def:binary_relation/antisymmetric}.
\end{definition}

\begin{definition}\label{def:alternating_function}\mimprovised
  Given a commutative ring \( R \), and \( R \)-module \( M \) and a \hyperref[def:multilinear_function]{multilinear function} \( f: M \to R \), we say that \( f \) is \term{alternating} if, \( x_i = x_j \) implies that
  \begin{equation*}
    f(x_1, \ldots, x_i, \ldots, x_j, \ldots x_n) = 0.
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:alternating_multilinear_is_antisymmetric}
  If a \hyperref[def:multilinear_function]{multilinear map} is \hyperref[def:alternating_function]{alternating}, it is \hyperref[def:antisymmetric_function]{antisymmetric}. The converse holds if \( 2 \) is invertible.
\end{proposition}
\begin{proof}
  \SufficiencySubProof If \( f \) is an alternating multilinear map, then
  \begin{equation*}
    0
    =
    f(\cdots, x_i + x_j, \cdots, x_i + x_j, \cdots)
    =
    f(\cdots, x_i, \cdots, x_j, \cdots)
    +
    f(\cdots, x_j, \cdots, x_i, \cdots).
  \end{equation*}

  Therefore,
  \begin{equation*}
    f(\cdots, x_i, \cdots, x_j, \cdots)
    =
    -f(\cdots, x_j, \cdots, x_i, \cdots).
  \end{equation*}

  \NecessitySubProof If \( f \) is an antisymmetric multilinear map, then
  \begin{equation*}
    0
    =
    f(\cdots, x_i + x_i, x_i, \cdots)
    =
    2 f(\cdots, x_i, x_i, \cdots).
  \end{equation*}

  If \( 2 \) is invertible, this implies
  \begin{equation*}
    f(\cdots, x_i, x_i, \cdots) = 0.
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:determinant_on_columns}
  In the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) over the commutative ring \( R \), the determinant function \( \det: R^{n \times n} \to R \) can be regarded as a function that maps \( n \) column vectors from \( R^n \) to \( R \). That is,
  \begin{equation}\label{eq:thm:determinant_on_columns}
    \det(v_1, \cdots, v_n) = \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n \pi_{\sigma(i)} (v_i).
  \end{equation}

  The determinant is an \hyperref[def:alternating_function]{alternating} \hyperref[def:multilinear_function]{multilinear function} on columns. Furthermore, it is the unique alternating multilinear function \( f(v_1, \ldots, v_n) \) such that \( f(e_1, \ldots, e_n) = 1 \), where \( e_1, \ldots, e_n \) are vectors of the \hyperref[def:sequence_space]{standard basis} in \( R^n \).
\end{proposition}
\begin{proof}
  Let \( \pi_1, \ldots, \pi_n \) be the \hyperref[def:basis_decomposition]{projection functionals} corresponding to the \hyperref[def:sequence_space]{standard basis} \( e_1, \ldots, e_n \).

  \SubProof{Proof of multilinearity} Due to linearity of the coordinate projection functionals \( \pi_i \) and due to distributivity in \( R \), for every \( j \) we have
  \begin{align*}
    &\phantom{{}={}}
    \det(\cdots, v_{j-1}, ty + rz, v_{j+1}, \cdots)
    = \\ &=
    \sum_{\sigma \in S_n} \sgn(\sigma) \cdot \pi_{\sigma(j)} (ty + rz) \prod_{i \neq j} \pi_{\sigma(i)} (v_i)
    = \\ &=
    t \sum_{\sigma \in S_n} \sgn(\sigma) \cdot \pi_{\sigma(j)} (y) \prod_{i \neq j} \pi_{\sigma(i)} (v_i) + r \sum_{\sigma \in S_n} \sgn(\sigma) \cdot \pi_{\sigma(j)} (z) \prod_{i \neq j} \pi_{\sigma(i)} (v_i)
    = \\ &=
    t \cdot \det(\cdots, y, \cdots) + r \cdot \det(\cdots, z, \cdots).
  \end{align*}

  \SubProof{Proof of alternation} If \( v_i = v_j \), then for every even (resp. odd) permutation \( \sigma \), the permutation \( \cycle{i, j} \bincirc \sigma \) is odd (resp. even), and hence they cancel out in the sum \eqref{eq:thm:determinant_on_columns}. This holds for every permutation, hence it remains for the determinant to be zero.

  \SubProof{Proof of \( \det(I_n) = 1 \)} Note that
  \begin{equation*}
    \prod_{i=1}^n \pi_i (e_{\sigma(i)}) \neq 0
  \end{equation*}
  if and only if \( i = \sigma(i) \) for every \( i = 1, \ldots, n \). This only holds for the identity permutation, hence
  \begin{equation*}
    \det(e_1, \ldots, e_n) = \prod_{i=1}^n \pi_i(e_i) = \prod_{i=1}^n 1 = 1.
  \end{equation*}

  \UniquenessSubProof Suppose that \( f(v_1, \ldots, v_n) \) is an alternating multilinear function such that \( f(e_1, \ldots, e_n) = 1 \).

  For an arbitrary column vector \( v_j \) in \( R^n \), we have
  \begin{equation*}
    v_j = \sum_{i=1}^n \pi_i(v_j) \cdot e_i.
  \end{equation*}

  Then
  \begin{align*}
    f(v_1, \ldots, v_n)
    &=
    f\parens*{ \sum_{i_1=1}^n \pi_{i_1}(v_1) \cdot e_{i_1}, \ldots, \sum_{i_n=1}^n \pi_{i_n}(v_n) \cdot e_{i_n} }
    = \\ &=
    \sum_{i_1=1}^n \pi_{i_1}(v_1) \cdots \sum_{i_n=1}^n \pi_{i_n}(v_n) f(e_{i_1}, \ldots, e_{i_n})
    = \\ &=
    \sum_{\sigma \in S_n} \pi_{\sigma(i)}(v_i) f(e_{\sigma(1)}, \ldots, e_{\sigma(n)}).
  \end{align*}

  The last step is valid because \( f \) is \hyperref[def:alternating_function]{alternating} and thus \( f(e_{i_1}, \cdots, e_{i_n}) \) is zero when not all of \( i_1, \ldots, i_n \) are distinct, and they are necessarily distinct if the indices are given by a permutation from \( S_n \).

  Finally, since \( f \) is \hyperref[def:antisymmetric_function]{antisymmetric} due to \fullref{thm:alternating_multilinear_is_antisymmetric},
  \begin{equation*}
    f(e_{\sigma(1)}, \ldots, e_{\sigma(n)}) = \sgn(\sigma) \underbrace{f(e_1, \ldots, e_n)}_{1 \T*{by assumption}} = \sgn(\sigma).
  \end{equation*}

  Therefore,
  \begin{equation*}
    f(v_1, \ldots, v_n) = \det(v_1, \ldots, v_n).
  \end{equation*}
\end{proof}

\begin{definition}\label{def:transpose_matrix}\mimprovised
  The \term{transpose matrix} of
  \begin{equation*}
    A = \begin{pmatrix}
      a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
      a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      a_{m,1} & a_{m,2} & \cdots & a_{m,n}
    \end{pmatrix}
  \end{equation*}
  is defined as
  \begin{equation*}
    A^T = \begin{pmatrix}
      a_{1,1} & a_{1,2} & \cdots & a_{n,1} \\
      a_{2,1} & a_{2,2} & \cdots & a_{n,2} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      a_{1,m} & a_{2,m} & \cdots & a_{n,m}
    \end{pmatrix}.
  \end{equation*}

  A matrix that is equal to its transpose is called \term{symmetric}.
\end{definition}

\begin{proposition}\label{thm:transpose_matrix_contravariant}
  \hyperref[def:transpose_matrix]{Matrix transposition} satisfies
  \begin{equation}\label{eq:thm:transpose_matrix_contravariant}
    (AB)^T = B^T \cdot A^T.
  \end{equation}
\end{proposition}
\begin{proof}
  Trivial.
\end{proof}

\begin{proposition}\label{thm:def:matrix_determinant}
  In the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) over the commutative ring \( R \), the \hyperref[def:matrix_determinant]{determinant} as function on matrices has the following basic properties:
  \begin{thmenum}
    \thmitem{thm:def:matrix_determinant/transpose} \( \det(A^T) = \det(A) \).
    \thmitem{thm:def:matrix_determinant/homogeneous} \( \det(tA) = t^n \cdot \det(A) \).
    \thmitem{thm:def:matrix_determinant/homomorphism}\mcite[sec. 6.7]{Тыртышников2007ЛинейнаяАлгебра} \( \det(AB) = \det(A) \cdot \det(B) \).

    That is, \( \det: R^{n \times n} \to R \) is a \hyperref[def:monoid/homomorphism]{monoid homomorphism} from the \hyperref[def:semiring]{multiplicative monoid} of the ring \( R^{n \times n} \) to the multiplicative monoid of \( R \).
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:def:matrix_determinant/transpose} The inverse of any permutation in \( S_n \) is also a permutation in \( S_n \), hence
  \begin{equation*}
    \det(A^T)
    =
    \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n a_{\sigma(i),i}
    =
    \sum_{\sigma \in S_n} \sgn(\sigma^{-1}) \prod_{i=1}^n a_{i,\sigma^{-1}(i)}
    =
    \det(A).
  \end{equation*}

  \SubProofOf{thm:def:matrix_determinant/homogeneous} Follows from \fullref{thm:determinant_on_columns}.

  \SubProofOf{thm:def:matrix_determinant/homomorphism} The \( j \)-th column of the product \( C = AB \) is
  \begin{equation*}
    c_{\anon*,j}
    =
    \sum_{i=1}^n b_{i,j} a_{\anon*,i}
    =
    \begin{pmatrix}
      \sum_{i=1}^n a_{1,i} b_{i,j} \\
      \vdots \\
      \sum_{i=1}^n a_{n,i} b_{i,j} \\
    \end{pmatrix}.
  \end{equation*}

  Since the determinant is a multilinear function on columns,
  \begin{balign*}
    \det(c_{\anon*,1}, \cdots, c_{\anon*,n})
    &=
    \det\parens*{ \sum_{i_1=1}^n b_{i_1,1} a_{\anon*,i_1}, \cdots, \sum_{i_n=1}^n b_{i_n,n} a_{\anon*,i_n} }
    = \\ &=
    \sum_{i_1=1}^n b_{i_1,1} \det\parens*{ a_{\anon*,i_1}, \cdots, \sum_{i_n=1}^n b_{i_n,n} a_{\anon*,i_n} }
    = \\ &=
    \sum_{i_1=1}^n b_{i_1,1} \cdots \sum_{i_n=1}^n b_{i_n,n} \det(a_{\anon*,i_1}, \cdots, a_{\anon*,i_n})
    = \\ &=
    \sum_{i_1=1}^n \cdots \sum_{i_n=1}^n b_{i_1,1} \cdots b_{i_n,n} \det(a_{\anon*,i_1}, \cdots, a_{\anon*,i_n}).
  \end{balign*}

  Since the determinant is \hyperref[def:alternating_function]{alternating} on columns, \( \det(a_{\anon*,i_1}, \cdots, a_{\anon*,i_n}) \) is zero when not all of \( i_1, \ldots, i_n \) are distinct. They are necessarily distinct if the indices are given by a permutation from \( S_n \). Therefore,
  \begin{balign*}
    \det(AB)
    &=
    \sum_{\sigma \in S_n} \prod_{i=1}^n a_{i,\sigma(i)} \cdot \sigma(a_{\anon*, \sigma(1)}, \ldots, a_{\anon*, \sigma(n)})
    = \\ &=
    \sum_{\sigma \in S_n} \prod_{i=1}^n a_{i,\sigma(i)} \cdot \sgn(\sigma) \cdot \sigma(a_{\anon*, 1}, \ldots, a_{\anon*, n})
    = \\ &=
    \det(B) \det(A).
  \end{balign*}
\end{proof}

\begin{definition}\label{def:submatrix}\mimprovised
  If for the matrices \( A = \seq{ a_{i,j} }_{i,j=1}^{m,n} \) and \( B = \seq{ b_{i,j} }_{i,j=1}^{k,k} \) over a commutative ring there exist \hyperref[def:order_function]{monotone functions}
  \begin{align*}
    &h: \set{ 1, \ldots, k } \to \set{ 1, \ldots, m }, \\
    &w: \set{ 1, \ldots, l } \to \set{ 1, \ldots, n },
  \end{align*}
  such that, for every \( i = 1, \ldots, k \) and \( j = 1, \ldots, l \) we have
  \begin{equation*}
    b_{i,j} = a_{h(i),w(j)}.
  \end{equation*}
\end{definition}

\begin{definition}\label{def:matrix_minor}\mimprovised
  A \term{minor} of a matrix is a \hyperref[def:matrix_determinant]{determinant} of a square \hyperref[def:submatrix]{submatrix}.
\end{definition}

\medskip

\begin{theorem}[Laplace expansion]\label{thm:laplace_expansion}\mcite[prop. 2.36]{Knapp2016BasicAlgebra}
  For a square matrix \( A = \seq{ a_{i,j} }_{i,j=1}^{n,n} \) over a commutative ring and a row index \( i \), we have
  \begin{equation*}
    \det A = \sum_{j=1}^n (-1)^{i + j} a_{i,j} \det A_{i,j},
  \end{equation*}
  where \( A_{i,j} \) is the \hyperref[def:submatrix]{submatrix} of \( A \) obtained by removing the \( i \)-th row and the \( j \)-th column.

  By \fullref{thm:def:matrix_determinant/transpose}, we can also expand along a column rather than a row.
\end{theorem}
\begin{proof}
  Denote the ring by \( R \). We will show that, for the \( i \)-th row,
  \begin{equation*}
    \begin{aligned}
      &\Phi: R^{n \times n} \to R, \\
      &\Phi(A) \coloneqq \sum_{j=1}^n (-1)^{i + j} a_{i,j} \det A_{i,j}.
    \end{aligned}
  \end{equation*}
  is an \hyperref[def:alternating_function]{alternating} \hyperref[def:multilinear_function]{multilinear function} on columns.

  Multilinearity follows from the multilinearity of determinants. For proving alternation, suppose that the \( k \)-th and \( l \)-th columns are equal. Then
  \begin{equation*}
    \Phi(A) = (-1)^{i + k} a_{i,k} \det A_{i,k} + (-1)^{i + l} a_{i,l} \det A_{i,l}.
  \end{equation*}

  The matrix \( A_{i,l} \) can be obtained from \( A_{i,k} \) by swapping \( \abs{k - l} \) columns. Since determinants are antisymmetric, it follows that
  \begin{equation*}
    \det A_{i,l} = (-1)^{k - l} \det A_{i,k}.
  \end{equation*}

  Furthermore, \( a_{i,k} = a_{i,l} \). Therefore,
  \begin{equation*}
    \Phi(A) = (-1)^{i + k} a_{i,k} \det A_{i,k} + (-1)^{(i + l) + (k - l)} a_{i,k} \det A_{i,k} = 0.
  \end{equation*}
\end{proof}

\begin{corollary}\label{thm:2x2_determinant}
  The \hyperref[def:matrix_determinant]{determinant} of
  \begin{equation*}
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
  \end{equation*}
  is \( ad - bc \).
\end{corollary}
\begin{proof}
  \Fullref{thm:laplace_expansion} allows us to expand by the first row:
  \begin{equation*}
    \det \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
    =
    a \cdot \det \begin{pmatrix} d \end{pmatrix} - b \cdot \det \begin{pmatrix} c \end{pmatrix}.
  \end{equation*}
\end{proof}

\begin{corollary}\label{thm:3x3_determinant}
  The \hyperref[def:matrix_determinant]{determinant} of
  \begin{equation*}
    \begin{pmatrix}
      a & b & c \\
      d & e & f \\
      g & h & i
    \end{pmatrix}
  \end{equation*}
  is
  \begin{equation}\label{eq:thm:3x3_determinant}
    (aei + bfg + cdh) - (ceg + fha + ibd).
  \end{equation}
\end{corollary}
\begin{proof}
  \Fullref{thm:laplace_expansion} allows us to expand by the first row:
  \begin{equation*}
    \det \begin{pmatrix}
      a & b & c \\
      d & e & f \\
      g & h & i
    \end{pmatrix}
    =
    a \cdot \det \begin{pmatrix}
      e & f \\
      h & i
    \end{pmatrix}
    -
    b \cdot \det \begin{pmatrix}
      d & f \\
      g & i
    \end{pmatrix}
    +
    c \cdot \det \begin{pmatrix}
      d & e \\
      g & h
    \end{pmatrix}
  \end{equation*}

  Then \eqref{eq:thm:3x3_determinant} follows via \fullref{thm:2x2_determinant}.
\end{proof}

\begin{remark}\label{rem:3x3_determinant}
  The expression \eqref{eq:thm:3x3_determinant} for calculating the \( 3 \times 3 \) determinant in \fullref{thm:3x3_determinant} can easily be remembered via a mnemonic.

  The products along the following diagonals are positive (the diagonals \enquote{wrap around}):
  \begin{equation*}
    \begin{pmatrix}
      a                   & \color{gray} b      & \color{lightgray} c \\
      \color{lightgray} d & e                   & \color{gray} f      \\
      \color{gray} g      & \color{lightgray} h & i
    \end{pmatrix}.
  \end{equation*}

  The products along the following diagonals are negative:
  \begin{equation*}
    \begin{pmatrix}
      \color{gray} a      & \color{lightgray} b & c                   \\
      \color{lightgray} d & e                   & \color{gray} f      \\
      g                   & \color{gray} h      & \color{lightgray} i
    \end{pmatrix}.
  \end{equation*}
\end{remark}

\begin{definition}\label{def:adjugate_matrix}\mimprovised
  The \term{cofactor matrix} of the \( m \times n \) matrix \( A \) is
  \begin{equation*}
    \seq{ (-1)^{i + j} \det A_{i,j} }_{i,j=1}^{m,n},
  \end{equation*}
  where \( A_{i,j} \) is the \hyperref[def:submatrix]{submatrix} of \( A \) obtained by removing the \( i \)-th row and the \( j \)-th column.

  The \term{adjugate matrix} \( A^{\op{adj}} \), also called the \term{classical adjoint matrix}, is the transpose of the cofactor matrix.
\end{definition}

\begin{proposition}\label{thm:inverse_via_adjunction}
  The \hyperref[def:adjugate_matrix]{adjugate matrix} of the square \( n \times n \) matrix \( A \) satisfies
  \begin{equation*}
    A \cdot A^{\op{adj}} = \det A \cdot I_n.
  \end{equation*}
\end{proposition}
\begin{proof}
  From \fullref{thm:laplace_expansion} it follows that the \( (i, i) \)-th entry of the matrix \( A \cdot A^{\op{adj}} \) is
  \begin{equation*}
    \sum_{k=1}^n (-1)^{i + k} a_{i,k} \det A_{i,k}
    =
    \det A.
  \end{equation*}

  For \( i \neq j \), the \( (i, j) \)-th entry is
  \begin{equation*}
    \sum_{k=1}^n (-1)^{i + j} a_{i,k} \det A_{j,k}
    =
    \det \widehat A_{j \mapsto i},
  \end{equation*}
  where \( \widehat A_{j \mapsto i} \) is the matrix obtained by replacing the \( j \)-th column in \( A \) with the \( i \)-th. The determinant is then zero because it is an alternating function on the columns.

  Thus, the proposition follows.
\end{proof}

\begin{definition}\label{def:inverse_matrix}
  We say that \( B \in R^{n \times m} \) is a \term{left inverse matrix} of \( A \in R^{m \times n} \) if \( BA \) is the identity matrix \( I_n \) and a \term{right inverse matrix} if \( AB \) is \( I_m \). These are precisely the left and right inverse linear maps in the correspondence described in \fullref{thm:matrix_and_linear_function_algebras}.

  Due to \fullref{thm:square_matrix_left_invertible_iff_right_invertible}, for square matrices, the two notions coincide, and we say that \( B \) is simply an \term{inverse} of \( A \). An inverse matrix, if it exists, is unique. We denote this inverse of \( A \) by \( A^{-1} \).

  We say that \( A \) is \term{invertible} if an inverse exists, and \term{singular} otherwise.
\end{definition}
\begin{defproof}
  The inverse is unique by \fullref{thm:monoid_inverse_unique}.
\end{defproof}

\begin{proposition}\label{thm:matrix_invertibility_via_kernel}
  Over a \hyperref[def:ring/commutative]{commutative ring} \( R \), the \( n \times n \) square matrix \( A \) is \hyperref[def:inverse_matrix]{invertible} if and only if, for any nonzero vector \( x \in R^n \), \( Ax \) is not the zero vector.
\end{proposition}
\begin{proof}
  \Fullref{thm:group_homomorphism_trivial_kernel} implies that \( A \) is invertible if and only if its kernel is trivial.
\end{proof}

\begin{proposition}\label{thm:square_matrix_left_invertible_iff_right_invertible}
  Over a nontrivial \hyperref[def:noetherian_semiring]{noetherian} commutative ring \( R \), a square matrix is \hyperref[def:inverse_matrix]{left invertible} if and only if it is \hyperref[def:inverse_matrix]{right invertible}.
\end{proposition}
\begin{proof}
  \NecessitySubProof Suppose that \( A \) is a right invertible matrix. When regarding \( A \) as a linear map via the identification from \fullref{rem:matrices_as_functions}, this implies that \( A \), as a linear map from \( R^n \) to \( R^n \), is right invertible. Then it is surjective and, by \fullref{thm:surjective_endomorphism_in_free_module}, an isomorphism. Therefore, \( A \) is a fully invertible matrix.

  \SufficiencySubProof Now suppose that \( A \) is a left inverse of \( B \). Then \( B \) is a right inverse of \( A \), and, by the other direction of the proposition, a two-sided inverse of \( A \).
\end{proof}

\begin{proposition}\label{thm:matrix_invertibility_via_determinants}\mcite[corr. 5.5]{Knapp2016BasicAlgebra}
  Over a \hyperref[def:ring/commutative]{commutative ring} \( R \), the matrix \( A \) is \hyperref[def:inverse_matrix]{invertible} if and only if its \hyperref[def:matrix_determinant]{determinant} is \hyperref[def:divisibility/invertible]{invertible}.
\end{proposition}
\begin{proof}
  \SufficiencySubProof Suppose that \( A \) is invertible.

  By \fullref{thm:def:matrix_determinant/homomorphism},
  \begin{equation*}
    \det(A^{-1}) \det(A) = \det(A^{-1} A) = \det(I_n) = 1,
  \end{equation*}
  hence \( \det(A) \) has a multiplicative inverse.

  \NecessitySubProof \todo{Prove}
\end{proof}

\begin{corollary}\label{thm:matrix_invertible_iff_transpose_invertible}
  Over a nontrivial commutative ring, a matrix is invertible if and only if its transpose is invertible.
\end{corollary}
\begin{proof}
  Suppose that \( A \) is an invertible matrix in \( R^{n \times n} \). \Fullref{thm:matrix_invertibility_via_determinants} implies that \( \det A \) is invertible in \( R \) and \fullref{thm:def:matrix_determinant/transpose} implies that \( \det A^T \) is invertible as well. Then \fullref{thm:matrix_invertibility_via_determinants} implies that \( A^T \) is also invertible in \( R^{n \times n} \).
\end{proof}

\begin{proposition}\label{matrix_invertibility_via_columns}
  In a nontrivial \hyperref[def:noetherian_semiring]{noetherian} commutative ring, the matrix \( A \) is \hyperref[def:inverse_matrix]{invertible} if and only if the \hyperref[def:block_matrix]{columns} of \( A \) are \hyperref[def:linear_dependence]{linearly independent}.
\end{proposition}
\begin{proof}
  \SufficiencySubProof As in \fullref{thm:determinant_on_columns}, regard \( \det(v_1, \ldots, v_n) \) are an alternating multilinear function on the columns of a matrix.

  Suppose that \( A \) is invertible. Then \fullref{thm:matrix_invertibility_via_determinants} implies that \( \det(v_1, \ldots, v_n) \) is invertible.

  Aiming at a contradiction, suppose that the column vectors \( v_1, \ldots, v_n \) are linearly dependent. Then there exists a nontrivial linear combination that sums to zero:
  \begin{equation*}
    \sum_{i=1}^n t_i v_i = 0.
  \end{equation*}

  Suppose that \( t_k \) is nonzero. Then
  \begin{align*}
    0
    &=
    \det\parens*{ v_1, \ldots, v_{k-1}, 0, v_{k+1}, \ldots, v_n }
    = \\ &=
    \det\parens*{ v_1, \ldots, v_{k-1}, \sum_{i=1}^n t_i v_i, v_{k+1}, \ldots, v_n }
    = \\ &=
    \sum_{i=1}^n t_i \det( v_1, \ldots, v_{k-1}, v_i, v_{k+1}, \ldots, v_n )
    = \\ &=
    t_k \det( v_1, \ldots, v_{k-1}, v_k, v_{k+1}, \ldots, v_n ).
  \end{align*}

  But we have assumed that \( \det( v_1, \ldots, v_{k-1}, v_k, v_{k+1}, \ldots, v_n ) \) is invertible and that \( t_k \) is nonzero. Hence, the determinant can only be a zero divisor if the ring is trivial, which we have assumed it is not. The obtained contradiction shows that \( v_1, \ldots, v_n \) are linearly independent.

  \NecessitySubProof Suppose that the columns of \( A \) are linearly independent. Consider the matrix equation
  \begin{equation*}
    Ax
    =
    \parens*
    {
      \begin{array}{c|c|c}
        a_{\anon*,1} & \cdots & a_{\anon*,n}
      \end{array}
    }
    \begin{pmatrix}
      x_1 \\ \vdots \\ x_n
    \end{pmatrix}
    =
    \sum_{k=1}^n x_k a_{\anon*,k}
    =
    \vect 0.
  \end{equation*}

  Since the columns are linearly independent, only \( x_1 = \cdots = x_n \) is a solution to this equation. Thus, when regarding \( A \) as the linear map \( x \mapsto Ax \), the \hyperref[def:module/kernel]{kernel} of \( A \) becomes trivial. By \fullref{thm:group_homomorphism_trivial_kernel}, this map is injective. As discussed in \fullref{def:module/category}, the injective linear maps are exactly the left invertible linear maps. Hence, there exists a left inverse of \( A \). Since \( A \) is a square matrix, by \fullref{thm:square_matrix_left_invertible_iff_right_invertible}, this implies that \( A \) is invertible.
\end{proof}

\begin{proposition}\label{thm:inverse_of_2x2_matrix}
  Assuming that the matrix is invertible,
  \begin{equation*}
    \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}^{-1}
    =
    \frac 1 {ad - bc}
    \begin{pmatrix}
      d  & -b \\
      -c & a
    \end{pmatrix}
  \end{equation*}
\end{proposition}
\begin{proof}
  Follows from \fullref{thm:inverse_via_adjunction}.
\end{proof}

\begin{proposition}\label{thm:determinant_of_inverse}
  The determinant of \( A^{-1} \) is \( \det(A)^{-1} \).
\end{proposition}
\begin{proof}
  Follows from \fullref{thm:inverse_via_adjunction}.
\end{proof}

\begin{definition}\label{def:linear_groups}\mimprovised
  The \hyperref[def:semiring]{multiplicative group} of the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) is called the \term{general linear group} \( \grp{GL}_R(n) \). These are the \hyperref[def:inverse_matrix]{invertible} \( n \times n \) matrices over \( R \).

  The subgroup of matrices with determinant \( 1 \) is called the \term{special linear group} \( \grp{SL}_R(n) \).
\end{definition}

\begin{concept}\label{con:change_of_basis}
  Let \( R \) be a \hyperref[def:ring/commutative]{commutative ring} and \( V \) be an \( R \)-module of finite \hyperref[def:module_rank]{rank}. Suppose that \( e_1, \ldots, e_n \) and \( f_1, \ldots, f_n \) are both bases of \( V \). Any vector \( v \) in \( V \) can be decomposed along \( e_1, \ldots, e_n \) to form the tuple \( x_1, \ldots, x_n \) and along \( f_1, \ldots, f_n \) to form \( y_1, \ldots, y_n \).

  \begin{equation*}
    f_j = \sum_{k=1}^n \pi_{e_k}(f_j) \cdot e_k.
  \end{equation*}

  Thus, for \( j = 1, \ldots, n \),
  \begin{equation*}
    y_j
    =
    \pi_{f_j}(y)
    =
    \pi_{f_j}\parens*{ \sum_{k=1}^n \pi_{e_k}(y) \cdot e_k }
    =
    \sum_{k=1}^n \pi_{e_k}(y) \cdot \pi_{f_j}(e_k)
    =
    \sum_{k=1}^n x_k \cdot \pi_{f_j}(e_k).
  \end{equation*}

  This can be alternatively written as
  \begin{equation*}
    \begin{pmatrix}
      y_1 \\ \vdots \\ y_n
    \end{pmatrix}
    =
    \begin{pmatrix}
      \pi_{f_1}(e_1) & \cdots & \pi_{f_1}(e_n) \\
      \vdots         & \ddots & \vdots         \\
      \pi_{f_n}(e_1) & \cdots & \pi_{f_n}(e_n)
    \end{pmatrix}
    \begin{pmatrix}
      x_1 \\ \vdots \\ x_n
    \end{pmatrix}.
  \end{equation*}

  The above matrix allows us to transform coordinates with respect to one basis to coordinates with respect to another. For this reason, we call it the \term{change of basis} matrix from \( e_1, \ldots, e_n \) to \( f_1, \ldots, f_n \).
\end{concept}

\begin{definition}\label{def:similar_matrices}\mcite[48]{Knapp2016BasicAlgebra}
  We say that the \( n \times n \) matrices \( A \) and \( B \) over the commutative ring \( R \) are \term{similar} if they are \hyperref[thm:group_conjugation_action]{conjugates} in the \hyperref[def:linear_groups]{general linear group} \( \grp{GL}_R(n) \). That is, if there exists an \hyperref[def:inverse_matrix]{invertible matrix} \( P \) such that \( A = P^{-1} B P \).
\end{definition}

\begin{proposition}\label{thm:similar_matrices_and_determinants}
  \hyperref[def:matrix_determinant]{Determinants} are invariant under \hyperref[def:similar_matrices]{matrix similarity}.

  This allows us to consider determinants of operators rather than matrices.
\end{proposition}
\begin{proof}
  By \fullref{thm:def:matrix_determinant/homomorphism} and \fullref{thm:determinant_of_inverse},
  \begin{equation*}
    \det(A) = \det(P^{-1} B P) = \det(P)^{-1} \det(B) \det(P) = \det(B).
  \end{equation*}
\end{proof}

\begin{remark}\label{rem:linear_operators_and_matrices}
  Let \( R \) be a \hyperref[def:ring/commutative]{commutative ring}. Unlike in the \( R \)-module \( R^n \) of tuples, in a general \( R \)-module of finite \hyperref[def:module_rank]{rank}, we have no concept of a \hyperref[def:sequence_space]{standard basis}. Instead, a linear operator \( T: U \to V \) corresponds to multiple matrix.

  If \( U \) has rank \( n \), a choice of basis for \( U \) is simply a choice of isomorphism with \( R^n \). Hence, given isomorphisms \( \varphi: U \to R^n \) and via \( \psi: V \to R^m \), the operator \( \psi^{-1} \bincirc T \bincirc \varphi \) becomes a function from \( R^n \) to \( R^m \). We can now use the identification with matrices discussed in \fullref{rem:matrices_as_functions}.

  This requires a choice of bases for \( U \) and for \( V \). If we have a matrix representing an operator and if we wish to use different bases, we must multiply it with the corresponding change of basis matrices discussed in \fullref{rem:matrices_as_functions}.
\end{remark}

\begin{proposition}\label{thm:matrices_of_operator_are_similar}
  Let \( V \) be a module of finite rank over a commutative ring and let \( T: V \to V \) be a linear endomorphism. Then two matrices \( A \) and \( B \) represent \( T \) if and only if they are \hyperref[def:similar_matrices]{similar}.
\end{proposition}
\begin{proof}
  \SufficiencySubProof Suppose that \( A \) represents \( T \) with respect to \( e_1, \ldots, e_n \) and \( B \) represents \( T \) with respect to \( f_1, \ldots, f_n \).

  Define \( P \) to be the \hyperref[con:change_of_basis]{change of basis} matrix from \( e_1, \ldots, e_n \) to \( f_1, \ldots, f_n \). Then
  \begin{equation*}
    A = P^{-1} B P.
  \end{equation*}

  \NecessitySubProof Suppose that \( A = P^{-1} B P \) and that \( B \) represents \( T \) with respect to \( e_1, \ldots, e_n \). Then \( A \) represents \( T \) with respect to the basis \( P e_1, \ldots, P e_n \).
\end{proof}
