\subsection{Convex functions}\label{subsec:convex_functions}

Let \( X \) be a Hausdorff \hyperref[def:topological_vector_space]{topological vector space} and \( D \) be a \hyperref[def:convex_set]{convex} subset of \( X \).

\begin{definition}\label{def:convex_functions}
  A function \( f: D \to \BbbR \) is called \term{convex} if any of the following equivalent conditions hold:

  \begin{defenum}
    \ilabel{def:convex_functions/ineq} For any two points \( x, y \in D \) and any \( t \in [0, 1] \) we have
    \begin{equation*}
      f(tx + (1-t)y) \leq tf(x) + (1-t)f(y).
    \end{equation*}

    \ilabel{def:convex_functions/epi} The \hyperref[def:epigraph]{epigraph}
    \begin{equation*}
      \epi f \coloneqq \{ (x, a) \in X \times \BbbR \colon f(x) \leq a \}
    \end{equation*}
    is convex.
  \end{defenum}

  If \( -g \) is convex for some function \( g: D \to \BbbR \), we call \( g \) \term{concave}.

  Note that definitions do not require any topological structure on \( X \). Most of their properties, however, require a topology.
\end{definition}
\begin{proof}
  Let \( x, y \in D \) and let \( t \in [0, 1] \).

  \SubProofImplication{def:convex_functions/ineq}{def:convex_functions/epi} Let \( \epi f \) be a convex set. Obviously \( (x, f(x)) \in D \) and \( (y, f(y)) \in D \). By the convexity of \( \epi f \), we have
  \begin{equation*}
    f(tx + (1-t)y) \leq tf(x) + (1-t)f(y).
  \end{equation*}

  Thus \( f \) is a convex function.

  \SubProofImplication{def:convex_functions/epi}{def:convex_functions/ineq} Let \( f \) be convex. Let \( a \geq f(x) \) and \( b \geq f(y) \) so that \( (x, a) \in \epi f \) and \( (y, b) \in \epi f \). Hence
  \begin{equation*}
    f(tx + (1-t)y) \leq tf(x) + (1-t)f(y) \leq ta + (1-t)b,
  \end{equation*}
  which implies that
  \begin{equation*}
    (tx + (1-t)y, ta + (1-t)b) \in \epi f.
  \end{equation*}

  Thus \( \epi f \) is a convex set.
\end{proof}

\begin{definition}\label{def:affine_functions_concave_and_convex}
  \hyperref[def:affine_operator]{Affine functions} \( f: X \to \BbbR \) are simultaneously convex and concave.
\end{definition}

\begin{proposition}\label{thm:convex_subdifferential_is_convex_and_weak*_closed}\mcite[exer. 1.10]{Phelps1993}
  For any convex function \( f \) and any \( x \in D \), the set \( \partial f(x) \) is convex and weak* closed.
\end{proposition}
\begin{proof}
  Fix \( x \in D \). If \( \partial f(x) \) is empty, then the theorem is trivially true.

  Suppose it is nonempty and \( y^*, z^* \in \partial f(x) \). For any \( x \in D \) we then have
  \begin{balign*}
     & \inner{y^*} {x - x} \leq f(x) - f(x), \\
     & \inner{z^*} {x - x} \leq f(x) - f(x).
  \end{balign*}

  Fix \( t \in [0, 1] \) and \( x \in D \). It follows that
  \begin{balign*}
    \inner{t y^* + (1-t) z^*} {x - x}
     & =
    t \inner{y^*} {x - x} + (1-t) \inner{z^*} {x - x}
    \leq \\ &\leq
    t [f(x) - f(x)] + (1-t) [f(x) - f(x)]
    =    \\ &=
    f(x) - f(x),
  \end{balign*}
  thus \( t y^* + (1-t)z^* \in \partial f(x) \) and hence \( \partial f(x) \) is convex.

  To prove weak*-closedness, we consider the decomposition
  \begin{balign*}
    \partial f(x)
     & =
    \{ x^* \in E^* \colon \forall x \in D, \inner {x^*} {x - x} \leq f(x) - f(x) \}
    =    \\ &=
    \bigcap_{x \in D} \{ x^* \in E^* \colon \inner {x^*} {x - x} \leq f(x) - f(x) \}
    =    \\ &=
    \bigcap_{x \in D} L(x)^{-1} (-\infty, f(x) - f(x)],
  \end{balign*}
  where
  \begin{balign*}
     & L: E \to E^{**}                  \\
     & L(x)(x^*) = \inner {x^*} {x - x}.
  \end{balign*}

  For each \( x \in E \), the functionals \( L(x) \) are weak*-to-weak continuous because the image \( L(E) \subseteq E^{**} \) is isometrically isomorphic to a translation of \( E \). Hence the preimage \( L(x)^{-1} (-\infty, f(x) - f(x)] \) is closed and \( \partial f(x) \) is weak*-closed as the intersection of weak*-closed sets.
\end{proof}

\begin{lemma}\label{thm:convex_difference_quotient_grows}
  For every point \( x \in X \) and every direction \( h \in S_X \) the difference quotient is a monotone function of \( t > 0 \), i.e. for \( 0 < s < t \)
  \begin{balign*}
    \frac {f(x + sh) - f(x)} s
    \leq
    \frac {f(x + th) - f(x)} t
  \end{balign*}
\end{lemma}
\begin{proof}
  \begin{balign*}
    \frac {f(x + sh) - f(x)} s
    =
    \frac t s \frac {f(x + \frac s t t h) - f(x)} t
    =
    \frac t s \frac {f\left(\frac s t (x + th) + (1 - \frac s t) x \right) - f(x)} t
    \leq \\ \leq
    \frac t s \frac {\frac s t f(x + t h) + (1 - \frac s t) f(x) - f(x)} t
    =
    \frac t s \frac s t \frac {f(x + th) - f(x)} t
    =
    \frac {f(x + th) - f(x)} t
  \end{balign*}
\end{proof}

\begin{proposition}\label{thm:convex_one_sided_derivatives_exist}
  For every point \( x \in X \) and every direction \( h \in S_X \) the one-sided derivative \( f_+'(x)(h) \) exists.
\end{proposition}
\begin{proof}
  We use the convexity of \( f \) to obtain
  \begin{balign*}
    f(x) = f \left(x + \frac {th} 2 - \frac {th} 2 \right) \leq \frac {f(x + th) + f(x - th)} 2,
    \\
    0 \leq [f(x - th) - f(x)] + [f(x + th) - f(x)],
    \\
    -[f(x - th) - f(x)] \leq [f(x + th) - f(x)],
    \\
    -\frac {f(x + t(-h)) - f(x)} t \leq \frac {f(x + th) - f(x)} t,
  \end{balign*}
  thus the difference quotient in \( f_+'(x)(h) \) is bounded below by the difference quotient for \( -f_+'(x)(-h) \).

  \Fullref{thm:convex_difference_quotient_grows} implies that the right difference quotient is non-increasing, thus both limits exist and
  \begin{equation*}
    -f_+'(x)(-h) \leq f_+'(x)(h).
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:convex_one_sided_derivatives_sublinear}
  For every point \( x \in X \) and every direction \( h \in S_X \) the one-sided derivative \( f_+'(x)(h) \) is a \hyperref[def:sublinear_functional]{sublinear functional}.
\end{proposition}
\begin{proof}
  \SubProofOf{def:sublinear_functional/subadditivity} It follows directly from
  \begin{balign*}
    \frac {f(x + t(a + b)) - f(x)} t
     & =
    \frac {f(\tfrac 1 2 (x + 2ta) + \tfrac 1 2 (x + 2tb)) - f(x)} t
    \leq \\ &\leq
    \frac {\tfrac 1 2 f(x + 2ta) + \tfrac 1 2 f(x + 2tb) - f(x)} t
    =    \\ &=
    \frac {f(x + 2ta) - f(x)} {2t} + \frac {f(x + 2tb) - f(x)} {2t}.
  \end{balign*}

  \SubProofOf{def:sublinear_functional/positive_homogeneity} For \( \lambda > 0 \) the equality \( f_+'(x)(\lambda h) = \lambda f_+'(x)(h) \) follows from
  \begin{balign*}
    \frac {f(x + t \lambda h) - f(x)} t
    =
    \lambda \frac {f(x + t \lambda h) - f(x)} {t \lambda}
  \end{balign*}
\end{proof}

\begin{corollary}\label{thm:convex_one_sided_derivative_negative_inequality}
  \begin{equation*}
    -f_+'(x)(-h) \leq f_+'(x)(h)
  \end{equation*}
\end{corollary}
\begin{proof}
  \begin{equation*}
    0 = f_+'(x)(h + (-h)) \leq f_+'(x)(h) + f_+'(x)(-h)
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:convex_iff_subdifferential_nonempty}
  The continuous function \( f: D \to X \) is convex if and only if its subdifferential \( \partial f(x) \) (see \fullref{def:subdifferentials/convex}) is nonempty for every \( x \) in \( D \).
\end{proposition}
% TODO: prove

\begin{proposition}
  \label{thm:convex_one_sided_derivative_is_max}
  For every direction \( h \in S_X \), we have that
  \begin{equation*}
    f_+'(x)(h) = \max\{ \inner {x^*} h \colon x^* \in \partial f(x) \}.
  \end{equation*}
\end{proposition}

\begin{theorem}\label{thm:singleton_subdifferential_implies_gateaux}
  If \( f \) is continuous and if the subdifferential \( \partial f(x) \) at \( x \in X \) is a singleton with element \( x^* \), then \( f \) is Gateaux differentiable at \( x \) and \( f_G'(x) = x^* \).
\end{theorem}
\begin{proof}
  Let \( h \in S_X \) be arbitrary. \Fullref{thm:convex_one_sided_derivatives_exist} implies that the one-sided derivatives \( f_+'(x)(-h) \) and \( f_+'(x)(h) \) exist and
  \begin{equation*}
    -f_+'(x)(-h) \leq f_+'(x)(h).
  \end{equation*}

  Assume\LEM that \( f \) is not Gateaux differentiable at \( x \), i.e. for some \( h_0 \in X \), we have a strict inequality. Then by \fullref{thm:convex_one_sided_derivative_is_max}
  \begin{balign*}
    \min\{ \inner {x^*} {h_0} \colon x^* \in \partial f(x) \}
    =
    -\max\{ \inner {x^*} {-h_0} \colon x^* \in \partial f(x) \}
    =
    -f_+'(x)(-h_0)
    < \\ <
    f_+'(x)(h_0)
    =
    \max\{ \inner {x^*} {h_0} \colon x^* \in \partial f(x) \},
  \end{balign*}
  which implies that there is more that one functional \( x^* \in \partial_C f(x) \). This contradicts the assumption of the theorem.

  Thus \( f \) is Gateaux differentiable at \( x \).
\end{proof}

\begin{theorem}\label{thm:rn_continuous_convex_partial_derivatives_imply_gateaux}\mcite[exer. 1.15(b]{Phelps1993})
  In \( \BbbR^n \), the existence of the partial derivatives at \( x \) for a continuous convex function \( f: D \to \BbbR \) at a point \( x \in D \) implies Gateaux differentiability.
\end{theorem}
\begin{proof}
  Let \( D \subseteq \BbbR^n \) be an open and convex set and let \( f: D \to \BbbR \) be continuous and convex. Then \( f_+'(x) \) exists everywhere by \fullref{thm:convex_one_sided_derivatives_exist} and is a subdifferential functional by \fullref{thm:convex_one_sided_derivatives_sublinear}.

  Let \( e_1, \ldots, e_n \) be the canonical basis for \( \BbbR^n \).

  The partial derivatives
  \begin{balign*}
    \frac {\partial f} {\partial x_i} (x)
    \coloneqq
    \lim_{t \to 0} \frac {f(x + t e_i) - f(x)} t
    =
    f_+'(x)(e_i)
  \end{balign*}
  exist, hence the projections of \( f_+'(x) \) along the coordinate exes are linear.

  Define line linear functional
  \begin{equation*}
    l(h) \coloneqq \sum_{i=1}^n h_i \inner{\frac {\partial f} {\partial x_i} (x)} h,
  \end{equation*}
  where \( h_1, \ldots, h_n \) are the coordinates of \( h \) along \( e_1, \ldots, e_n \).

  We will show that \( l \cong f_+' \). Fix \( h \in S_X \). We have
  \begin{balign}\label{thm:rn_continuous_convex_partial_derivatives_imply_gateaux/diff_dominated}
    f_+'(x)(h)
     & =
    f_+'(x)\left(\sum_{i=1}^n h_i e_i \right)
    \overset {\text{sublinearity}} \leq \nonumber      \\ &\leq
    \sum_{i=1}^n f_+'(x)(h_i e_i)
    \overset {\text{linearity along } e_i} = \nonumber \\ &=
    \sum_{i=1}^n h_i f_+'(x)(e_i)
    =
    \sum_{i=1}^n h_i \inner{\frac {\partial f} {\partial x_i} (x)} h.
  \end{balign}

  Thus
  \begin{balign*}
    \inner l h
    =
    -\inner l {-h}
    \overset {\ref{thm:rn_continuous_convex_partial_derivatives_imply_gateaux/diff_dominated}} \leq
    -f_+'(x)(-h)
    \overset {\text{\ref{thm:convex_one_sided_derivative_negative_inequality}}} \leq
    f_+'(x)(h)
    \overset {\ref{thm:rn_continuous_convex_partial_derivatives_imply_gateaux/diff_dominated}} \leq
    \inner l h,
  \end{balign*}
  i.e. \( f_+'(x)(h) = \inner l h \) for all \( h \in S_X \), hence \( f_+'(x) \) is a linear functional and \( f \) is Gateaux differentiable at \( x \).
\end{proof}

\begin{theorem}\label{thm:rn_continuous_convex_gateaux_implies_frechet}\mcite[exer. 1.15(a]{Phelps1993})
  In \( \BbbR^n \), Gateaux differentiability of a continuous convex function \( f: D \to \BbbR \) at a point \( x \in D \) implies Frechet differentiability.
\end{theorem}
\begin{proof}
  Since \( f \) is Gateaux differentiable (\fullref{def:differentiability/gateaux}) at \( x \), the derivative \( f'(x) = f_+'(x) \) is linear.

  Because \( f \) is continuous and convex, it is locally Lipschitz with constant \( L \) in some \( \delta \)-ball with center \( x \).

  Suppose\LEM that \( f \) is not Frechet differentiable at \( x \). Inverting the condition in \fullref{def:differentiability/frechet}, we obtain that there exist \( \varepsilon > 0 \) and a sequence \( \{ h_n \}_n \subseteq B(x, \delta) \setminus \{ 0 \} \) such that \( \norm{h_n} \to 0 \) and yet for all \( n \in \BbbZ_{>0} \),
  \begin{balign}\label{thm:rn_continuous_convex_gateaux_implies_frechet/frechet_assumption}
    \abs{f(x + h_n) - f(x) - \inner{f'(x)} {h_n}} > \varepsilon \norm{h_n}.
  \end{balign}

  Define
  \begin{balign*}
    t_n \coloneqq \norm{h_n}
     &  &
    u_n \coloneqq \frac{h_n} {\norm {h_n}}.
  \end{balign*}

  Obviously \( t_{n_k} \downarrow 0 \). The vectors \( h_n \) are linearly independent since otherwise \( f \) would not be Gateaux differentiable at \( x \), hence \( u_n \) are not all equal.

  Since \( S_{\BbbR^n} \) is compact\USC, by the Bolzano-Weierstrass theorem, there exists a convergent subsequence \( \{ u_{n_k} \}_k \underset {k \to \infty} \to u_0 \) of \( \{ u_n \}_n \). We have

  \begin{balign}\label{thm:rn_continuous_convex_gateaux_implies_frechet/frechet_estimate}
     & \phantom= \abs{\frac {f(x + t_{n_k} u_{n_k}) - f(x)} {t_{n_k}} - \inner{f'(x)} {u_{n_k}}}
    \leq \nonumber
    \abs{\frac {f(x + t_{n_k} u_{n_k}) - f(x + t_{n_k} u_0)} {t_{n_k}}} +                       \\ &+ \abs{\frac {f(x + t_{n_k} u_0) - f(x)} {t_{n_k}} - \inner{f'(x)} {u_0}} + \abs{\inner{f'(x)} {u_0 - u_{n_k}}}
    \leq \nonumber                                                                              \\ &\leq
    L \norm{u_{n_k} - u_0} + \abs{\frac {f(x + t_{n_k} u_0) - f(x)} {t_{n_k}} - \inner{f'(x)} {u_0}} + \norm{f'(x)} \norm{u_0 - u_{n_k}}.
  \end{balign}

  Fix \( \delta > 0 \). Because of the Gateaux differentiable of \( f \) at \( x \), we can pick \( k_0 \) such that
  \begin{equation*}
    \abs{\frac {f(x + t_{n_{k_0}} u_0) - f(x)} {t_{n_{k_0}}} - \inner{f'(x)} {u_0}} < \delta.
  \end{equation*}

  Because \( \{ u_{n_k} \}_k \) converges to \( u_0 \), we can choose \( k_1 \) such that
  \begin{equation*}
    \norm{u_0 - u_{n_{k_1}}} < \delta.
  \end{equation*}

  Thus for \( k > \max \{ k_0, k_1 \} \), \fullref{thm:rn_continuous_convex_gateaux_implies_frechet/frechet_estimate} is bounded by
  \begin{balign*}
    \abs{\frac {f(x + t_{n_k} u_{n_k}) - f(x)} {t_{n_k}} - \inner{f'(x)} {u_{n_k}}}
    \leq
    (L + 1 + \norm{f'(x)}) \delta.
  \end{balign*}

  It suffices to choose \( \delta > 0 \) so that
  \begin{equation*}
    \delta < \frac 1 {L + 1 + \norm{f'(x)}}
  \end{equation*}
  in order to have, for \( k > \max \{ k_0, k_1 \} \),
  \begin{equation*}
    \abs{\frac {f(x + t_{n_k} u_{n_k}) - f(x)} {t_{n_k}} - \inner{f'(x)} {u_{n_k}}} < \varepsilon.
  \end{equation*}

  But this contradicts \fullref{thm:rn_continuous_convex_gateaux_implies_frechet/frechet_assumption}, hence \( f \) is Frechet differentiable at \( x \).
\end{proof}

\begin{corollary}\label{thm:rn_continuous_convex_partial_derivatives_imply_frechet}
  In \( \BbbR^n \), the existence of the partial derivatives at \( x \) for a continuous convex function \( f: D \to \BbbR \) at a point \( x \in D \) is equivalent to Frechet differentiability.
\end{corollary}
\begin{proof}
  A direct consequence of and \fullref{thm:rn_continuous_convex_partial_derivatives_imply_gateaux} and \fullref{thm:rn_continuous_convex_gateaux_implies_frechet}.
\end{proof}

\begin{theorem}\label{thm:rn_continuous_convex_frechet_almost_everywhere}\mcite[exer. 1.17]{Phelps1993}
  In \( \BbbR^n \), continuous convex functions \( f: D \to \BbbR \) are differentiable almost everywhere.
\end{theorem}
\begin{proof}
  For all \( h \in S_X \) and small enough \( t > 0 \) we define
  \begin{balign*}
     & \varphi_h^t: D \to \BbbR
     & \varphi_h^t(x) \coloneqq \frac {f(x + th) - f(x)} t
  \end{balign*}
  and \( \varphi_h(x) \coloneqq f_+'(x)(h) = \lim_{t \downarrow 0} \varphi_h^t(x) \).

  Considered as functions of \( x \), \( \varphi_h^t \) are obviously continuous hence Borel measurable and so \( \varphi_h \) is also Borel measurable.

  Denote by
  \begin{balign*}
    B_h
    \coloneqq
    \{ x \in D \colon -f_+'(x)(-h) < f_+'(x)(h) \}
    =
    \{ x \in D \colon -\varphi_{-h}(x) - \varphi_h(x) < 0 \}
  \end{balign*}
  the set of points \( x \in D \) where the one-sided derivative \( f_+'(x)(h) \) is not linear, given a fixed direction \( h \in S_X \). If \( B_h \) is nonempty, \( f \) is not differentiable at \( x \).

  The sets \( B_h \) are Borel sets since they are the preimages of \( (-\infty, 0) \) under a Borel function. We will show that it is a null set for every direction \( h \).

  Fix \( h \in S_X \). Denote by \( \delta_x \coloneqq \sup \{ t > 0 \colon x + th \in D \} \).

  The function \( t \mapsto f(x + th) \) is a convex function of one variable. By \cite[theorem 1.16]{Phelps1993}, it is differentiable \( \mu_1 \)-almost everywhere in \( [0, \delta_x) \), where \( \mu_m \) is the Lebesgue \( m \)-measure.

  Denote
  \begin{balign*}
     & H \coloneqq \linspan\{ h \} \cong \BbbR^1,
    \\
     & H^\perp \cong \BbbR^{n-1} \text{ - the orthogonal complement of \( H \) in \( \BbbR^n \)},
    \\
     & L_x \coloneqq \{ x + th, 0 \leq t < \delta_x \} - half-open segments in D.
  \end{balign*}

  THe whole domain \( D \) can be represented as \( D = \cup \{ L_x \colon x \in H^\perp \} \).

  We can now use Fubini's theorem to show that \( B_h \) is a null set:
  \begin{balign*}
    \mu_n(B_h)
    =
    \int_{B_h} dz
    =
    \int_{\BbbR^n = H^\perp \oplus H} \Chi_{B_h} (z) dz
    =
    \int_{H^\perp} \int_{L_x} \Chi_{B_h} (y) dy dx
    = \\ =
    \int_{H^\perp} \mu_1(B_h \cap L_x) dx
    =
    \int_{H^\perp} 0 dx
    =
    0.
  \end{balign*}

  Hence for all \( h \in S_X \), \( -f_+'(x)(-h) = f_+'(x)(h) \) for almost all \( x \in D \).

  In particular, if \( e_1, \ldots, e_n \) is the canonical basis of \( \BbbR^n \), the \( i \)-th partial derivative \( \frac{\partial f} {\partial x_i} (x) \) exists only in \( D \ B_{e_i} \).

  The gradient
  \begin{equation*}
    \nabla f(x) = \left( \frac{\partial f} {\partial x_1} (x), \ldots, \frac{\partial f} {\partial x_n} (x) \right)
  \end{equation*}
  then exists in
  \begin{equation*}
    \hat D \coloneqq (D \ B_{e_1}) \cap \ldots \cap (D \ B_{e_n}) = D \setminus \left( \bigcup_{i=1}^n B_{e_i} \right).
  \end{equation*}

  \Fullref{thm:rn_continuous_convex_partial_derivatives_imply_frechet} then implies that \( f \) is Frechet differentiable in \( \hat D \), i.e. almost everywhere in \( D \).
\end{proof}
