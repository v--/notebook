\subsection{Bilinear forms}\label{subsec:bilinear_forms}

\begin{definition}\label{def:bilinear_form}\mcite\cite[249]{Knapp2016BasicAlgebra}
  Let \( M \) and \( N \) be left \( R \)-modules and \( L: M \times N \to R \) be a \hyperref[def:multilinear_function]{multilinear function}. We say that \( L \) is a \term{bilinear form}.

  If \( M = N \), we have the following additional types of bilinear forms:
  \begin{thmenum}
    \ilabel{def:bilinear_form/symmetric} If \( L \) is a \hyperref[def:symmetric_function]{symmetric function}, we say that is is a \term{symmetric bilinear form}

    \ilabel{def:bilinear_form/skew_symmetric} If for all \( x, y \in M \) instead of \( L(x, y) = L(y, x) \) we have \( L(x, y) = -L(y, x) \), we say that \( L \) is \term{skew-symmetric}.

    \ilabel{def:bilinear_form/alternating} If for all \( x \in M \) we have \( L(x, x) = 0 \), we say that \( L \) is \term{alternating}.
  \end{thmenum}
\end{definition}

\begin{proposition}\label{thm:skew_symmetric_iff_alternating}
  Let \( 1 + 1 = 2 \) be a unit in the ring \( R \). Let \( M \) be a left module over \( R \). Then the bilinear form \( L: M \times M \to R \) is \hyperref[def:bilinear_form/alternating]{alternating} if and only if it is \hyperref[def:bilinear_form/skew_symmetric]{skew-symmetric}.

  Alternating implies skew-symmetric even if \( 2 \) is not invertible.
\end{proposition}
\begin{proof}
  \SufficiencySubProof Let \( L \) be alternating. Then
  \begin{balign*}
    L(x + y, x + y) & = L(x, x) + L(x, y) + L(y, x) + L(y, y) \\
    0               & = L(x, y) + L(y, x)                     \\
    L(x, y) = -L(y, x).
  \end{balign*}

  \NecessitySubProof Let \( L \) be skew-symmetric. Then
  \begin{equation*}
    L(x, x) = -L(x, x),
  \end{equation*}
  which implies that \( 2L(x, y) = 0 \). Hence \( L \) is alternating if we are able to divide by \( 2 \).
\end{proof}

\begin{definition}\label{def:bilinear_form_radicals}\mcite\cite[250]{Knapp2016BasicAlgebra}
  Let \( L: M \times N \to R \) be a bilinear form. We define its \term{left radical}
  \begin{equation*}
    \{ x \in M \colon \forall y \in N, \inprod x y = 0 \}
  \end{equation*}
  and \term{right radical}
  \begin{equation*}
    \{ y \in N \colon \forall x \in M, \inprod x y = 0 \}.
  \end{equation*}

  Note that if \( L \) is symmetric or skew-symmetric (which also implies \( M = N \)), the two are identical and we speak simply of the \term{radical} \( \sqrt L \).
\end{definition}

\begin{definition}\label{def:nondegenerate_bilinear_form}\mcite\cite[249]{Knapp2016BasicAlgebra}
  We say that a bilinear form \( L: M \times N \to R \) is \term{nondegenerate} if both its left and right \hyperref[def:bilinear_form_radicals]{radicals} are nontrivial.
\end{definition}

\begin{theorem}\label{thm:bilinear_form_matrix_presentation}
  Fix a commutative unital ring \( R \) and a bilinear form \( L: R^n \times R^m \to R \). Then there exists a matrix \( A \in R^{n \times m} \) such that
  \begin{equation*}
    L(x, y) \coloneqq x^T A y.
  \end{equation*}

  This matrix is called the generalized \term{Gram matrix}.

  In particular, if \( L \) is \hyperref[def:symmetric_function]{symmetric}, so it \( A \).
\end{theorem}
\begin{proof}
  Denote by \( e_1, \ldots, e_n \) the basis of \( R^n \) and by \( f_1, \ldots, f_m \) the basis of \( R^m \).

  Define the matrix \( A = \{ a_{i,j} \}_{i,j=1}^{n,m} \) by
  \begin{equation*}
    a_{i,j} \coloneqq L(e_i, f_j).
  \end{equation*}

  Note that if \( n = m \) and if \( L \) is symmetric, then the matrix \( A \) is obviously symmetric too.

  For any fixed basis vector \( e_i, i = 1, \ldots, n \) of \( R^n \), we have
  \begin{equation*}
    L(e_i, y)
    =
    \sum_{j=1}^m y_i L(e_i, f_j)
    =
    y_i a_{(i,-)},
  \end{equation*}
  where \( a_{(i,-)} \) is the \( i \)-th row of \( A \).

  Thus for an arbitrary \( x \in R^n \)
  \begin{equation*}
    L(x, y)
    =
    \sum_{i=1}^n x_i L(e_i, y)
    =
    \sum_{i=1}^n x_i (a_{(i,-)} y)
    =
    \left( \sum_{i=1}^n x_i a_{(i,-)} \right) y
    =
    x^T A y.
  \end{equation*}
\end{proof}

\begin{corollary}\label{thm:bilinear_forms_isomorphic_to_matrices}
  Fix a commutative unital ring \( R \). The vector space of bilinear forms of type \( R^n \times R^m \to R \) is isomorphic to the matrix space \( A \in R^{n \times m} \).
\end{corollary}

\begin{definition}\label{def:sesquilinear_form}\mcite\cite[258]{Knapp2016BasicAlgebra}
  Let \( V \) be a complex vector space and let \( \overline V \) be its conjugate \hyperref[def:complex_conjucate_vector_space]{transpose}. We call the bilinear form \( L: V \times \overline V \to \BbbC \) a \term{sesquilinear form} (we say that \( L \) is \enquote{semilinear} in its second argument and \enquote{sesqui} means \enquote{one and a half} is Latin).

  Similar to \fullref{def:bilinear_form}, we have
  \begin{thmenum}
    \ilabel{def:sesquilinear_form/hermitian} If for all \( x, y \in V \) we have \( L(x, y) = \overline{L(y, x)} \), we say that \( L \) is \term{Hermitian}.

    \ilabel{def:sesquilinear_form/skew_hermitian} If for all \( x, y \in V \) we have \( L(x, y) = -\overline{L(y, x)} \), we say that \( L \) is \term{skew-Hermitian}.
  \end{thmenum}
\end{definition}

\begin{definition}\label{def:duality_pairing}
  Let \( M \) and \( N \) be left \( R \)-modules. A \term{duality pairing} \( \inprod \cdot \cdot: M \times N \to R \) is a \hyperref[def:nondegenerate_bilinear_form]{nondegenerate} bilinear form.

  See \fullref{def:canonical_duality_pairing} and \fullref{def:locally_convex_duality_pairing}.
\end{definition}

\begin{definition}\label{def:quadratic_form}
  If \( L: M \times M \to R \) be a bilinear \hyperref[def:bilinear_form]{form}, we call the function
  \begin{equation*}
    Q(x) \coloneqq L(x, x)
  \end{equation*}
  a \term{quadratic form} over \( M \).
\end{definition}

\begin{definition}\label{def:quadratic_form_definiteness}
  \todo{Define definiteness of quadratic forms}
\end{definition}

\begin{proposition}\label{thm:bilinear_forms_vs_to_quadratic_forms}
  A \term{quadratic form} \( Q: M \to R \) is a \hyperref[def:homogenous_function]{homogeneous function} of degree \( 2 \). In particular, \( Q(x) = Q(-x) \).
\end{proposition}
\begin{proof}
  Let \( L: M \times M \to R \) be the corresponding bilinear form. Then, by \fullref{def:linear_operator/homogeneity},
  \begin{equation*}
    Q(tx) = L(tx, tx) = t^2 L(x, x) = t^2 Q(x).
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:polarization_identity}\mcite\cite{nLab:polarization_identity}
  Let \( L: M \times M \to R \) be a bilinear \hyperref[def:bilinear_form]{form} and \( Q: M \to R \) be its associated quadratic \hyperref[def:quadratic_form]{form}. Then the \term{polarization identity} holds:
  \begin{equation}\label{thm:polarization_identity/polarization_identity}
    2 L(x, y) + 2 L(y, x) = Q(x + y) - Q(x - y)
  \end{equation}

  The similar looking but slightly less useful parallelogram law also holds:
  \begin{equation}\label{thm:polarization_identity/parallelogram_law}
    2 Q(x) + 2 Q(y) = Q(x + y) + Q(x - y)
  \end{equation}

  If \( 2 = 1 + 1 \) is a unit in \( R \), we can \enquote{recover} from \( Q \) the bilinear form:
  \begin{equation}\label{thm:polarization_identity/symmetrization_definition}
    \hat L(x, y) \coloneqq \frac 1 2 \left[ Q(x + y) - Q(x) - Q(y) \right]
  \end{equation}

  The function \( \hat L \) is \hyperref[def:symmetric_function]{symmetric} and is called the \term{symmetrization} of \( L \). If \( L \) itself is symmetric, \( L = \hat L \).
\end{proposition}
\begin{proof}
  Identities \fullref{thm:polarization_identity/polarization_identity,thm:polarization_identity/parallelogram_law,thm:polarization_identity/symmetrization_definition} all follow from the bilinearity of \( L \), that is,
  \begin{equation*}
    Q(x \pm y)
    =
    L(x, x) \pm L(x, y) \pm L(y, x) + L(y, y)
    =
    [Q(x) + Q(y)] \pm [L(x, y) + L(y, x)].
  \end{equation*}
\end{proof}

\begin{definition}\label{def:orthogonality}
  Let \( R \) be a division \hyperref[def:semiring/division_ring]{ring}, let \( M \) be a left \( R \)-module and let \( L: M \times N \to R \) be a nondegenerate bilinear form. We say that the vectors \( x \in M \) and \( y \in N \) are \term{orthogonal} with respect to \( L \) if
  \begin{equation*}
    L(x, y) = 0.
  \end{equation*}

  For every submodule \( K \subseteq M \) we define its \term{orthogonal complement} with respect to \( L \) as
  \begin{equation*}
    K^\perp \coloneqq \{ x \in M \colon \forall y, L(x, y) = 0 \}
  \end{equation*}
  and analogously for submodules of \( N \).

  Let \( \mscrK \) be an index set and \( \{ x_k \}_{k \in \mscrK} \subseteq M \), \( \{ y_k \}_{k \in \mscrK} \subseteq N \) be two families of vectors indexed by \( I \). We say that these families form a \term{biorthogonal system} with respect to \( L \) if
  \begin{equation*}
    L(x_k, y_\beta) = 0 \text{ follows from } k \neq \beta
  \end{equation*}

  If \( M = N \), we usually consider \term{orthogonal systems} \( \{ x_k \}_{k \in \mscrK} \subseteq M \) where
  \begin{equation*}
    L(x_k, x_\beta) = 0 \iff i \neq j
  \end{equation*}
\end{definition}

\begin{definition}\label{def:inner_product_space}
  An \term{inner product space} is a vector space \( V \) over \( F \) equipped with a positive \hyperref[def:quadratic_form_definiteness]{definite} \hyperref[def:bilinear_form/symmetric]{symmetric} bilinear form \( \inprod \cdot \cdot: V \times V \to F \).

  In the special case where \( F = \BbbC \), by convention we require \( V \) to instead be equipped with a positive definite \hyperref[def:sesquilinear_form/hermitian]{Hermitian} sesquilinear form instead.
\end{definition}

\begin{definition}\label{def:symplectic_vector_space}
  A \term{symplectic vector space} is a vector space \( V \) over \( F \) equipped with a \hyperref[def:bilinear_form/symmetric]{nondegenerate}  \hyperref[def:bilinear_form/alternating]{alternating} bilinear form \( \inprod \cdot \cdot: V \times V \to F \).
\end{definition}

\begin{lemma}\label{thm:inner_product_quadratic_form_is_positive_definite}
  Let \( V \) be a real or complex \hyperref[def:inner_product_space]{inner product space} with product \( \inprod \cdot \cdot \). The function \( Q(x) \coloneqq \inprod x x \) (which is not a quadratic form in the complex case) is positive definite.
\end{lemma}
\begin{proof}
  The real case is trivial. Assume that \( V \) is a complex vector space and that \( \inprod \cdot \cdot \) is Hermitian. This implies that \( \inprod x x = \overline{\inprod x x} \), thus \( \inprod x x \in \BbbR \). Furthermore, since the inner product is positive definite, we have \( Q(x) = \inprod x x \geq 0 \). Thus \( Q \) is nonnegative real valued.

  Since \( \inprod \cdot \cdot \) is positive definite, so is \( Q \).
\end{proof}

\begin{theorem}[Cauchy-Bunyakovsky-Schwarz inequality]\label{thm:cauchy_bunyakovsky_schwarz_inequality}
  Let \( V \) be a real or complex \hyperref[def:inner_product_space]{inner product space} with product \( \inprod \cdot \cdot \). For every \( x, y \in V \) it holds that
  \begin{equation}\label{thm:cauchy_bunyakovsky_schwarz_inequality/inequality}
    {\abs{\inprod x y}}^2 \leq \inprod x x \inprod y y.
  \end{equation}

  Furthermore, equality is achieved if and only if \( x \) and \( y \) are linearly dependent.
\end{theorem}
\begin{proof}
  Note that we use this theorem to prove that the induced norm is a norm so we cannot use the norm here. Associate with \( \inprod \cdot \cdot \) the function \( Q(x) \coloneqq \inprod x x \). By \fullref{thm:inner_product_quadratic_form_is_positive_definite}, \( Q \) is positive definite.

  Fix \( x, y \in V \) and \( t \in \BbbC \). If either vector is zero the statement is trivially true so let both be nonzero. We have
  \begin{balign*}
    Q(x + ty)
     & =
    \inprod {x + ty} {x + ty}
    =    \\ &=
    Q(x) + \overline t \inprod x y + t \inprod y x + \abs{t}^2 Q(y)
    =    \\ &=
    Q(x) + 2\real t \overline{\inprod x y} + \abs{t}^2 Q(y)
  \end{balign*}

  Take \( t \coloneqq - \frac {\inprod x y} {Q(y)} \) so that
  \begin{equation*}
    Q(x + ty)
    =
    Q(x) - 2 \frac {\abs{\inprod x y}^2} {Q(y)} + \frac {\abs{\inprod x y}^2} {Q(y)}
    =
    Q(x) - \frac {\abs{\inprod x y}^2} {Q(y)}
  \end{equation*}

  Since \( Q(x + ty) \geq 0 \), it follows that
  \begin{balign*}
    Q(x) - \frac {\abs{\inprod x y}^2} {Q(y)} & \geq 0                  \\
    Q(x) Q(y)                               & \geq \abs{\inprod x y}^2.
  \end{balign*}

  If \( x \) and \( y \) are linearly dependent, equality obviously holds. Conversely, suppose that equality holds. This implies that
  \begin{equation*}
    Q(x + ty) = 0,
  \end{equation*}
  which by the positive definiteness of \( Q \) means that \( x = -ty \). Thus \( x \) and \( y \) are linearly dependent.
\end{proof}

\begin{definition}\label{def:bilinear_form_induced_norm}
  Let \( V \) be a real or complex \hyperref[def:inner_product_space]{inner product space} with product \( \inprod \cdot \cdot \). We define its induced \hyperref[def:norm]{norm} as
  \begin{balign*}
     & \norm \cdot : V \to \BbbR_{\geq 0}    \\
     & \norm x \coloneqq \sqrt{\inprod x x}.
  \end{balign*}

  If \( V \) is a real inner product space, the induced norm is a square root of the induced quadratic \hyperref[def:quadratic_form]{form} of \( \inprod \cdot \cdot \).
\end{definition}
\begin{proof}
  We will only prove the complex case because the real case is identical but slightly simpler.

  Note that \( \norm \cdot \) is well-defined (that is, positive definite) by \fullref{thm:inner_product_quadratic_form_is_positive_definite}.

  Now we will show that it is a norm.
  \SubProofOf{def:norm/N1} Follows from the positive definiteness of \( \inprod \cdot \cdot \)

  \SubProofOf{def:norm/N2} For \( t \in \BbbC \) and \( x \in V \) we have
  \begin{equation*}
    \norm{tx} = \sqrt{\inprod{tx} {tx}} = \abs{t} \sqrt{\inprod x x} = \abs t \norm x.
  \end{equation*}

  \SubProofOf{def:norm/N3} For \( x, y \in V \) we have
  \begin{balign*}
    \norm{x + y}^2
     & =
    \inprod{x + y} {x + y}
    =                                                            \\ &=
    \inprod x x + \inprod x y + \inprod y x + \inprod y y
    =                                                            \\ &=
    \norm{x}^2 + 2 \real \inprod x y + \norm{y}^2
    \leq                                                         \\ &\leq
    \norm{x}^2 + 2 \abs{\real \inprod x y} + \norm{y}^2
    \overset {\ref{thm:cauchy_bunyakovsky_schwarz_inequality}} = \\ &=
    \norm{x}^2 + 2 \norm x \norm y + \norm{y}^2
    =
    (\norm{x} + \norm{y})^2
  \end{balign*}

  Therefore
  \begin{equation*}
    \norm{x + y} \leq \norm x + \norm y.
  \end{equation*}
\end{proof}
