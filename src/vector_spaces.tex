\section{Linear algebra}\label{sec:linear_algebra}
\subsection{Vector spaces}\label{subsec:vector_spaces}

This subsection is about the algebraic properties of vector spaces. See \fullref{subsec:vector_space_geometry} for \enquote{geometric} concepts like hyperplanes and convexity.

\begin{definition}\label{def:vector_space}
  A \term{vector space} \( (V, +, \cdot) \) is a \hyperref[def:left_module]{left module} over a field \( \BbbK \).

  We call elements of \( \BbbK \) \term{scalars} and elements of \( V \) \term{vectors}.

  The category of vector spaces over \( \BbbK \) is denoted by \( \cat{Vect}_{\BbbK} \).
\end{definition}

\begin{definition}\label{def:vector_field}
  Let \( V \) be a vector space over \( \BbbK \). Functions of the type
  \begin{equation*}
    f: \BbbK \to V
  \end{equation*}
  are called \term{vector fields}. To avoid confusion, \( \BbbK \) is, sometimes referred to as a \term{scalar field}. This convention comes from physics and is dominant in areas that are far from algebraic field theory, hence in practice it does not cause a lot of confusion.
\end{definition}

\begin{remark}\label{rem:real_vector_space}
  Outside of algebra, we are usually only interested in vector spaces over the fields \( \BbbR \) or \( \BbbC \). We call them \term{real vector spaces} and \term{complex vector spaces}, respectively.
\end{remark}

\begin{definition}\label{def:complex_conjucate_vector_space}
  Let \( V \) be a vector space over the complex numbers \( \BbbC \). Its \term{complex conjugate vector space} \( \overline V \) is the same space, but with scalar multiplication defined as
  \begin{equation*}
    t \cdot_{\overline V} x \coloneqq \overline t \cdot_V x.
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:field_extension_is_vector_space}
  Let \( \BbbK \) be a field \hyperref[def:field_extension]{extension} of \( G \). Then \( \BbbK \) is a vector space over \( G \).
\end{proposition}
\begin{proof}
  Since \( \BbbK \) already has the structure of an abelian group, we must only define scalar multiplication
  \begin{balign*}
     & \circ: G \times \BbbK \to \BbbK, \\
     & g \circ f \coloneqq gf,
  \end{balign*}
  where the product in the definition is simply multiplication in \( \BbbK \). The well-definedness of \( \circ \) follows from the well-definedness of multiplication in \( \BbbK \).
\end{proof}

\begin{remark}\label{rem:linear_span_only_for_vector_spaces}
  The definition for linear \hyperref[def:linear_span]{span} applies to general commutative \hyperref[def:left_module]{modules}. However, since \fullref{thm:vector_space_linear_dependence,thm:vector_space_basis} do not apply to general commutative modules, it makes sense to only use linear spans withing the context of vector spaces.
\end{remark}

\begin{definition}\label{def:linear_span}
  For a set \( A \subseteq X \) of vectors, the set of all linear combinations of finite subsets of \( A \) is called its span and is denoted by \( \linspan{A} \).
\end{definition}

\begin{proposition}\label{thm:vector_space_linear_dependence}
  The set \( A \subseteq V \) is linearly dependent in the sense of \fullref{def:left_module_linear_dependence} if and only if there exists a vector \( x \in V \) such that
  \begin{equation*}
    x \in \linspan{A} \setminus \{ x \}.
  \end{equation*}
\end{proposition}
\begin{proof}
  \SufficiencySubProof Let \( A \subseteq M \) and let
  \begin{equation*}
    0_M \coloneqq \sum_{k=1}^n t_k x_k,
  \end{equation*}
  where \( t_1, \ldots, t_n \) have at least one nonzero scalar and where \( x_1, \ldots, x_n \) are nonzero vectors. Without loss of generality, assume that \( t_{n_0} \) is the nonzero scalar. Then
  \begin{balign*}
    0_M             & = \sum_{k=1}^n t_k x_k,                                                                                  \\
    t_{k_0} x_{k_0} & = -\sum_{k=1}^n t_k x_k,                                                                                 \\
    x_{k_0}         & = \sum_{k=1}^n \left(-\frac {t_k} {t_{k_0}} \right) x_k \in \linspan{A} \setminus \left\{ x_{k_0} \right\}.
  \end{balign*}

  \NecessitySubProof Let \( A \subseteq M \) and \( x \in \linspan{A} \setminus \{ 0_M, x \} \). By \fullref{def:linear_combination}, there exist nonzero vectors \( x_1, \ldots, x_n \in A \) and scalars \( t_1, \ldots, t_n \in R \) such that
  \begin{equation*}
    x \coloneqq \sum_{k=1}^n t_k x_k,
  \end{equation*}
  where at least one of \( t_1, \ldots, t_n \) is nonzero.

  Then \( 0_M \) is a nontrivial linear combination of the nonzero vectors \( x_1, \ldots, x_n, x \):
  \begin{equation*}
    0_M = \sum_{k=1}^n t_k x_k - x.
  \end{equation*}
\end{proof}

\begin{definition}\label{affine_independence}
  Given a vector space \( V \) over \( F \), we say that a set \( A \subseteq V \) of vectors is \term{affinely independent} in \( V \) if the set
  \begin{equation*}
    \{ (x, 1) \colon x \in A \}
  \end{equation*}
  is linearly independent in \( V \times F \).
\end{definition}

\begin{proposition}\label{thm:vector_space_basis}
  The set \( B \subseteq V \) is a basis in the sense of \fullref{def:left_module_hamel_basis} if and only if it is linearly independent and
  \begin{equation*}
    V = \linspan{B}.
  \end{equation*}
\end{proposition}
\begin{proof}
  \SufficiencySubProof We will prove the contraposition, that is, if \( \linspan{B} \neq M \), then \( B \) is not a maximal linearly independent set.

  If \( \linspan{B} \subsetneq M \), then there exists a vector \( x \in M \) such that \( x \) is not a linear combination of any subset of \( B \). Thus, \( B \cup \{ x \} \) does not have a nontrivial linear combination that equals zero. Hence, \( B \cup \{ x \} \) is linearly independent.

  \NecessitySubProof Let \( B \subseteq M \) and \( \linspan{B} = M \). Assume that there exists a vector \( x \in M \setminus B \) such that the set \( B \cup \{ x \} \) is linearly independent.

  Then, for any \( b \in B \), the vector \( x + b \) is a linear combination of elements, one of which is independent of \( B \). Thus, by \fullref{thm:vector_space_linear_dependence},
  \begin{equation*}
    M = \linspan{B} \subsetneq \linspan{B \cup \{ x \}} \subseteq M,
  \end{equation*}
  which is a contradiction.
\end{proof}

\begin{theorem}[Every vector space has a basis]\label{thm:every_vector_space_has_a_basis}
  Every vector space has a \hyperref[def:left_module_hamel_basis]{basis}. Equivalently, all vector spaces are \hyperref[def:free_left_module]{free modules}.

  Compare this to finite-dimensional vector spaces of order \( n \) over \( \BbbK \), which are all isomorphic to \( \BbbK^n \).

  In \hyperref[def:zfc]{\logic{ZF}} this theorem is equivalent to the \hyperref[def:zfc/choice]{axiom of choice} --- see \fullref{thm:axiom_of_choice_equivalences/vector_space_bases}.
\end{theorem}
\begin{proof}
  Let \( V \) be a vector space. Assume that it does not have a basis. Let \( \mathcal{B} \) be the family of all linearly independent \hyperref[def:linear_combination]{subsets} of \( V \).

  The family \( \mathcal{B} \) is obviously nonempty since any \hyperref[rem:singleton_sets]{singleton} from \( V \) belongs to \( \mathcal{B} \). The union of any chain \( \mathcal{B}' \subseteq \mathcal{B} \) can then contain only linearly independent elements since otherwise we would have that, some set in \( \mathcal{B}' \) is not linearly independent. Thus, we can apply \fullref{thm:zorns_lemma} to obtain a maximal element \( B \).

  Assume that \( B \) is not a basis, that is,
  \begin{equation*}
    \linspan B \subsetneq V.
  \end{equation*}

  Take \( V \in V \setminus \linspan B \). Then the set \( B \cup \{ v \} \) is linearly independent, which contradicts the assumption that \( B \) is not a basis. Thus, \( B \) is a basis of \( V \) and \( V \) is a free module.
\end{proof}

\begin{definition}\label{def:vector_space_dimension}
  The \hyperref[def:free_left_module]{free module rank} of a vector space \( V \) is called the \term{dimension} \( \dim V \) of \( V \). If \( U \) is a vector subspace of \( V \), we call \( \co\dim_V U \coloneqq \dim(V/U) \) the \term{codimension} of \( U \) relative to \( V \).
\end{definition}

\begin{proposition}\label{thm:linear_maps_form_algebra}
  The set \( \hom(U, V) \) is a vector space.
\end{proposition}
\begin{proof}
  By \fullref{thm:functions_over_ring_form_algebra}, \( \hom(U, V) \) forms an \( \BbbK \)-vector space.
\end{proof}

\begin{remark}\label{rem:functional}
  The term \enquote{functional} does not have a strict meaning. For example, logicians use terms like \enquote{primitive recursive functional} for certain generalized functions. Functions are also ill-defined, see \fullref{rem:function_definition}. Outside of logic, however, the term \enquote{functional} usually refers to a function from a vector space \( V \) to its base field \( \BbbK \). Examples include linear \hyperref[def:linear_operator]{functionals}, like projection \hyperref[def:left_module_basis_projection]{maps} and \hyperref[def:differentiability]{derivatives}, and nonlinear functionals, like the Minkowski \hyperref[def:minkowski_functional]{functionals}.
\end{remark}

\begin{definition}\label{def:eigenpair}
  Let \( f: U \to V \) be a function between vector spaces over \( \BbbK \).

  An \term{eigenpair} of \( f \) consists of an \term{eigenvalue} \( \lambda \in \BbbK \) and an \term{eigenvector} \( x \in U \) such that
  \begin{equation*}
    f(x) = \lambda x.
  \end{equation*}
\end{definition}
