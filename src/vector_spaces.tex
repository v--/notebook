\subsection{Vector spaces}\label{subsec:vector_spaces}

This subsection is about the algebraic properties of vector spaces. See \fullref{subsec:vector_space_geometry} for \enquote{geometric} concepts like hyperplanes and convexity.

\begin{proposition}\label{thm:module_basis_decomposition}
  Let \( B \) be a \hyperref[def:semimodule_basis/independent]{Hamel basis} of the \( R \)-module \( M \). Every member of \( M \) can be represented as a linear combination over \( B \) uniquely up the order of summands. We call this the \term{basis decomposition}.

  Explicitly, for every element \( x \) of \( M \), there exists unique finite set \( V \subseteq M \) and a function \( \tau: V \to R \) such that
  \begin{equation*}
    x = \sum_{k=1}^n t_k v_k,
  \end{equation*}
  where \( v_1, \ldots, v_n \) is any well-ordering of \( V \) and \( t_k = \tau(v_k) \).
\end{proposition}
\begin{proof}
  At least one such representation exists by \fullref{def:semimodule_basis/independent/span_of_basis} and \fullref{def:semimodule_basis/independent/span_and_combinations}.

  Suppose that there are two such representations
  \begin{equation*}
    x = \sum_{k=1}^n t_k v_k = \sum_{k=1}^m s_k w_k.
  \end{equation*}

  Without loss of generality, we can assume that \( n = m \) and \( v_k = w_k \) for every index \( k = 1, \ldots, n \). We can justify this by explicitly adding terms with \( 0_R \) as the coefficients where needed.

  Hence,
  \begin{equation*}
    0 = x - x = \sum_{k=1}^n (t_k - s_k) v_k.
  \end{equation*}

  The vectors \( v_1, \ldots, v_n \) are linearly independent since they belong to the basis \( B \). Hence, only a trivial linear combination can give the zero vector. This implies that \( t_k = s_k \) for every index \( k = 1, \ldots, n \).

  Therefore, the representation of \( x \) as a linear combination over \( B \) is unique up to reordering.
\end{proof}

\begin{definition}\label{def:module_basis_projection}
  Let \( M \) be a left \( R \)-module and let \( B \) be a basis of \( M \). For each \( b \in B \), we define the \text{coordinate projection functional} \( \pi_b: M \to R \) that gives us the unique coefficient in the basis decomposition. Thus, for every \( x \in M \) we have
  \begin{equation*}
    x = \sum_{b \in B} \pi_b(x) b.
  \end{equation*}

  \begin{proposition}\label{thm:module_basis_decomposition}
    Let \( B \) be a \hyperref[def:semimodule_basis/independent]{basis} of the free left \( R \)-module \( M \). Then every member of \( M \) can be represented as a linear combination over \( B \) uniquely up the order of summands.

    Explicitly, for every element \( x \) of \( M \), there exists unique finite set \( V \subseteq M \) and a unique function \( \tau: V \to R \) such that
    \begin{equation*}
      x = \sum_{k=1}^n t_k v_k,
    \end{equation*}
    where \( v_1, \ldots, v_n \) is a well-ordering of \( V \) and \( t_k = \tau(v_k) \).
  \end{proposition}
  \begin{proof}
    At least one such representation exists by \fullref{def:semimodule_basis/independent/span_of_basis} and \fullref{def:semimodule_basis/independent/span_and_combinations}.

    Suppose that there are two such representations
    \begin{equation*}
      x = \sum_{k=1}^n t_k v_k = \sum_{k=1}^m s_k w_k.
    \end{equation*}

    Without loss of generality, we can assume that \( n = m \) and \( v_k = w_k \) for every index \( k = 1, \ldots, n \). We can justify this by explicitly adding terms with \( 0_R \) as the coefficients where needed.

    Hence,
    \begin{equation*}
      0 = x - x = \sum_{k=1}^n (t_k - s_k) v_k.
    \end{equation*}

    The vectors \( v_1, \ldots, v_n \) are linearly independent since they belong to the basis \( B \). Hence, only a trivial linear combination can give the zero vector. This implies that \( t_k = s_k \) for every index \( k = 1, \ldots, n \).

    Therefore, the representation of \( x \) as a linear combination over \( B \) is unique up to reordering.
  \end{proof}

  The sum is well-defined since only finitely many terms are nonzero.

  When the basis \( B \) is finite and ordered:
  \begin{equation*}
    B = \{ b_1, \ldots, b_n \},
  \end{equation*}
  we also write
  \begin{equation*}
    x = \sum_{i=1}^n x_k b_i.
  \end{equation*}
\end{definition}
\begin{proof}
  By \fullref{thm:module_basis_decomposition}, this decomposition is unique given a basis \( B \).
\end{proof}

\begin{proposition}\label{thm:left_module_basis_projections_are_linear}
  The basis projection \hyperref[def:module_basis_projection]{maps} are linear.
\end{proposition}
\begin{proof}
  \SubProofOf{def:semimodule/homomorphism/homogeneity} Let \( t \in R \) and \( x \in M \). We have the unique decompositions
  \begin{balign*}
    x  & = \sum_{b \in B} \pi_b(x) b,  \\
    tx & = \sum_{b \in B} \pi_b(tx) b.
  \end{balign*}

  Since both decompositions have only finitely many terms, their difference also has only finitely many nonzero terms. Thus,
  \begin{equation*}
    0
    =
    tx - tx
    =
    t \left( \sum_{b \in B} \pi_b(x) b \right) - \sum_{b \in B} \pi_b(tx) b
    =
    \sum_{b \in B} (t \pi_b(x) - \pi_b(tx)) b.
  \end{equation*}

  Since the vectors in \( B \) are linearly independent, no nontrivial linear combination can equal the zero vector. Thus, for all \( b \in B \),
  \begin{equation*}
    t \pi_b(x) = \pi_b(tx).
  \end{equation*}

  \SubProofOf{def:semimodule/homomorphism/additivity} Analogous.
\end{proof}

\begin{proposition}\label{thm:left_module_basis_cardinality}\mcite{ProofWiki:bases_of_free_module_have_same_cardinality}
  All bases in a free left module over a commutative unital ring have the same cardinality.
\end{proposition}

\begin{definition}\label{def:vector_field}
  Let \( V \) be a vector space over \( \BbbK \). Functions of the type
  \begin{equation*}
    f: \BbbK \to V
  \end{equation*}
  are called \term{vector fields}. To avoid confusion, \( \BbbK \) is sometimes referred to as a \term{scalar field}. This convention comes from physics and is dominant in areas that are far from algebraic field theory, hence in practice it does not cause a lot of confusion.
\end{definition}

\begin{remark}\label{rem:real_vector_space}
  Outside of algebra, we are usually only interested in vector spaces over the fields \( \BbbR \) or \( \BbbC \). We call them \term{real vector spaces} and \term{complex vector spaces}, respectively.
\end{remark}

\begin{definition}\label{def:complex_conjucate_vector_space}
  Let \( V \) be a vector space over the complex numbers \( \BbbC \). Its \term{complex conjugate vector space} \( \overline V \) is the same space, but with scalar multiplication defined as
  \begin{equation*}
    t \cdot_{\overline V} x \coloneqq \overline t \cdot_V x.
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:field_extension_is_vector_space}
  Let \( \BbbK \) be a field \hyperref[def:field_extension]{extension} of \( G \). Then \( \BbbK \) is a vector space over \( G \).
\end{proposition}
\begin{proof}
  Since \( \BbbK \) already has the structure of an abelian group, we must only define scalar multiplication
  \begin{balign*}
     & \circ: G \times \BbbK \to \BbbK, \\
     & g \circ f \coloneqq gf,
  \end{balign*}
  where the product in the definition is simply multiplication in \( \BbbK \). The well-definedness of \( \circ \) follows from the well-definedness of multiplication in \( \BbbK \).
\end{proof}

\begin{remark}\label{rem:linear_span_only_for_vector_spaces}
  The definition for linear \hyperref[def:semimodule/submodel]{span} applies to general commutative \hyperref[def:module]{modules}. However, since \fullref{thm:vector_space_linear_dependence,thm:vector_space_basis} do not apply to general commutative modules, it makes sense to only use linear spans withing the context of vector spaces.
\end{remark}

\begin{proposition}\label{thm:vector_space_linear_dependence}
  The set \( A \subseteq V \) is linearly dependent in the sense of \fullref{def:semimodule_basis/independent} if and only if there exists a vector \( x \in V \) such that
  \begin{equation*}
    x \in \linspan{A} \setminus \{ x \}.
  \end{equation*}
\end{proposition}
\begin{proof}
  \SufficiencySubProof Let \( A \subseteq M \) and let
  \begin{equation*}
    0_M \coloneqq \sum_{k=1}^n t_k x_k,
  \end{equation*}
  where \( t_1, \ldots, t_n \) have at least one nonzero scalar and where \( x_1, \ldots, x_n \) are nonzero vectors. Without loss of generality, assume that \( t_{n_0} \) is the nonzero scalar. Then
  \begin{balign*}
    0_M             & = \sum_{k=1}^n t_k x_k,                                                                                  \\
    t_{k_0} x_{k_0} & = -\sum_{k=1}^n t_k x_k,                                                                                 \\
    x_{k_0}         & = \sum_{k=1}^n \left(-\frac {t_k} {t_{k_0}} \right) x_k \in \linspan{A} \setminus \left\{ x_{k_0} \right\}.
  \end{balign*}

  \NecessitySubProof Let \( A \subseteq M \) and \( x \in \linspan{A} \setminus \{ 0_M, x \} \). By \fullref{rem:linear_combinations}, there exist nonzero vectors \( x_1, \ldots, x_n \in A \) and scalars \( t_1, \ldots, t_n \in R \) such that
  \begin{equation*}
    x \coloneqq \sum_{k=1}^n t_k x_k,
  \end{equation*}
  where at least one of \( t_1, \ldots, t_n \) is nonzero.

  Then \( 0_M \) is a nontrivial linear combination of the nonzero vectors \( x_1, \ldots, x_n, x \):
  \begin{equation*}
    0_M = \sum_{k=1}^n t_k x_k - x.
  \end{equation*}
\end{proof}

\begin{definition}\label{affine_independence}
  Given a vector space \( V \) over \( F \), we say that a set \( A \subseteq V \) of vectors is \term{affinely independent} in \( V \) if the set
  \begin{equation*}
    \{ (x, 1) \colon x \in A \}
  \end{equation*}
  is linearly independent in \( V \times F \).
\end{definition}

\begin{proposition}\label{thm:vector_space_basis}
  The set \( B \subseteq V \) is a basis in the sense of \fullref{def:hamel_basis} if and only if it is linearly independent and
  \begin{equation*}
    V = \linspan{B}.
  \end{equation*}
\end{proposition}
\begin{proof}
  \SufficiencySubProof We will prove the contraposition, that is, if \( \linspan{B} \neq M \), then \( B \) is not a maximal linearly independent set.

  If \( \linspan{B} \subsetneq M \), then there exists a vector \( x \in M \) such that \( x \) is not a linear combination of any subset of \( B \). Thus, \( B \cup \{ x \} \) does not have a nontrivial linear combination that equals zero. Hence, \( B \cup \{ x \} \) is linearly independent.

  \NecessitySubProof Let \( B \subseteq M \) and \( \linspan{B} = M \). Assume that there exists a vector \( x \in M \setminus B \) such that the set \( B \cup \{ x \} \) is linearly independent.

  Then, for any \( b \in B \), the vector \( x + b \) is a linear combination of elements, one of which is independent of \( B \). Thus, by \fullref{thm:vector_space_linear_dependence},
  \begin{equation*}
    M = \linspan{B} \subsetneq \linspan{B \cup \{ x \}} \subseteq M,
  \end{equation*}
  which is a contradiction.
\end{proof}

\begin{definition}\label{def:vector_space_dimension}
  The \hyperref[def:free_semimodule]{free module rank} of a vector space \( V \) is called the \term{dimension} \( \dim V \) of \( V \). If \( U \) is a vector subspace of \( V \), we call \( \co\dim_V U \coloneqq \dim(V/U) \) the \term{codimension} of \( U \) relative to \( V \).
\end{definition}

\begin{proposition}\label{thm:linear_maps_form_algebra}
  The set \( \hom(U, V) \) is a vector space.
\end{proposition}
\begin{proof}
  By \fullref{thm:functions_over_semimodule}, \( \hom(U, V) \) forms an \( \BbbK \)-vector space.
\end{proof}

\begin{remark}\label{rem:functional}
  The term \enquote{functional} does not have a strict meaning. For example, logicians use terms like \enquote{primitive recursive functional} for certain generalized functions. Functions are also ill-defined, see \fullref{rem:function_definition}. Outside of logic, however, the term \enquote{functional} usually refers to a function from a vector space \( V \) to its base field \( \BbbK \). Examples include linear \hyperref[def:semimodule/homomorphism]{functionals}, like projection \hyperref[def:module_basis_projection]{maps} and \hyperref[def:differentiability]{derivatives}, and nonlinear functionals, like the Minkowski \hyperref[def:minkowski_functional]{functionals}.
\end{remark}

\begin{definition}\label{def:eigenpair}
  Let \( f: U \to V \) be a function between vector spaces over \( \BbbK \).

  An \term{eigenpair} of \( f \) consists of an \term{eigenvalue} \( \lambda \in \BbbK \) and an \term{eigenvector} \( x \in U \) such that
  \begin{equation*}
    f(x) = \lambda x.
  \end{equation*}
\end{definition}
