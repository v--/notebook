\subsection{Vector spaces}\label{subsec:vector_spaces}

This subsection is about the algebraic properties of vector spaces. See \fullref{subsec:vector_space_geometry} for \enquote{geometric} concepts like hyperplanes and convexity.

\begin{definition}\label{def:semimodule_basis}
  Fix an \( R \)-\hyperref[def:semimodule]{semimodule} \( M \) and a set \( A \) in \( M \).

  \begin{thmenum}
    \thmitem{def:semimodule_basis/independent} We say that \( A \) is \term{linearly independent} if no vector in \( A \) is a \hyperref[def:linear_combinatoin]{linear combination} of the rest. Equivalently, for every vector \( e \in A \), we must have \( e \not\in \linspan(A \setminus \set{ e }) \).

    \thmitem{def:semimodule_basis/generating} We say that \( A \) is a \term{generating set} for \( M \) if \( \linspan{ A } = M \).

    The \term{rank} of \( M \) is the minimum of the \hyperref[thm:cardinality_existence]{cardinalities} of generating sets of \( M \). A module with finite rank is said to be \term{finitely generated}.

    \thmitem{def:semimodule_basis/basis} We say that \( A \) is a \term{Hamel basis} or simply \term{basis} for \( M \) if it is a linearly independent generating set.
  \end{thmenum}
\end{definition}

\begin{proposition}\label{thm:def:semimodule_basis/independent/properties}
  For an \( R \)-module \( M \), \hyperref[def:semimodule_basis/independent]{linear (in)dependence} has the following basic properties:
  \begin{thmenum}
    \thmitem{def:semimodule_basis/independent/zero} The zero vector \( 0_M \) is by itself linearly dependent.

    \thmitem{def:semimodule_basis/independent/antimonotonicity} If \( A \) is a linearly \hi{dependent} set and \( A \subseteq B \), then \( A \) is also a linearly dependent set.

    \thmitem{def:semimodule_basis/independent/monotonicity} If \( A \) is a linearly \hi{independent} set and \( A \supseteq B \), then \( B \) is also a linearly independent set.

    \thmitem{def:semimodule_basis/independent/span_and_combinations} The \hyperref[def:semimodule/submodel]{linear span} of a set \( A \) of vectors is the set of (the evaluations of) all \hyperref[def:linear_combination]{linear combinations} of members of \( A \).

    \thmitem{def:semimodule_basis/independent/adding_to_span} For any set of vectors \( A \), if \( x \in \linspan A \) is not in \( A \), then \( A \cup \set{ x } \) is a linearly dependent set.

    \thmitem{def:semimodule_basis/independent/basis_of_span} Every linearly independent set is a Hamel basis for its own span.

    \thmitem{def:semimodule_basis/independent/span_of_basis} If \( B \) is a Hamel basis of \( M \), then \( M = \linspan{ B } \).

    \thmitem{def:semimodule_basis/independent/sum_of_basis} If \( B \) is a Hamel basis of \( M \), then \( M \cong \sum_{x \in B} R \).
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{def:semimodule_basis/independent/zero} Trivial.

  \SubProofOf{def:semimodule_basis/independent/antimonotonicity} Trivial.

  \SubProofOf{def:semimodule_basis/independent/monotonicity} Trivial.

  \SubProofOf{def:semimodule_basis/independent/span_and_combinations} Follows from \fullref{ex:def:first_order_substructure/vector_space}.

  \SubProofOf{def:semimodule_basis/independent/adding_to_span} By \fullref{def:semimodule_basis/independent/span_and_combinations}, there exists a linear combination of members of \( A \) such that
  \begin{equation*}
    x = \sum_{k=1}^n t_k x_k.
  \end{equation*}

  If \( x = 0_M \), then \( A \cup \set{ x } \) is not linearly dependent by \fullref{def:semimodule_basis/independent/zero} and \fullref{def:semimodule_basis/independent/antimonotonicity}.

  If \( x \neq 0_M \), then \( A \cup \set{ x } \) is not linearly independent because \( x \) is a nontrivial linear combination of other vectors of \( A \).

  \SubProofOf{def:semimodule_basis/independent/basis_of_span} Suppose that \( A \) is not a maximal linearly independent set in the module \( \linspan A \). Then there exists some nonzero vector \( x \in \linspan A \) such that \( A \cup \set{ x } \) are again linearly independent.

  By \fullref{def:semimodule_basis/independent/span_and_combinations}, there exists a linear combination in \( A \) such that
  \begin{equation*}
    x = \sum_{k=1}^n t_k x_k.
  \end{equation*}

  Since \( x \) is nonzero, the combination must be nontrivial. But this contradicts the linear independence of \( A \). Therefore, \( A \) is a basis of its span.

  \SubProofOf{def:semimodule_basis/independent/span_of_basis} Clearly \( \linspan{B} \subseteq M \). If we suppose that \( M \setminus \linspan{B} \) is nonempty, then there exists some vector \( x \) that is not a linear combination of members of \( B \) (since it is not in the span). But this also implies that \( B \cup \set{ x } \) is linearly independent, which contradicts our assumption that \( B \) is a Hamel basis.

  The obtained contradicts shows that \( M = \linspan{B} \).

  \SubProofOf{def:semimodule_basis/independent/sum_of_basis} Follows from \fullref{def:semimodule_basis/independent/span_of_basis} by noting that linear combinations are defined abstractly as members of the direct sum \( \sum_{x \in B} R \).
\end{proof}

\begin{proposition}\label{thm:free_module_hamel_basis}
  The \( R \)-module \( M \) is (isomorphic to) a \hyperref[def:free_semimodule]{free \( R \)-module} with basis \( B \) if and only if \( B \) is a \hyperref[def:semimodule_basis/independent]{Hamel basis} of \( M \).
\end{proposition}
\begin{proof}
  \SufficiencySubProof Let \( M \) be a free \( R \)-module with basis \( B \). Every vector in \( M \) is then a function \( \tau: B \to R \) with finite \hyperref[def:function_support]{support}.

  Fix members \( b_1, \ldots, b_n \) of \( B \). For their canonical inclusion, we have \( [\iota(b_k)](r) = 1_R \) if \( r = b_k \) and \( 0_R \) otherwise. Thus, if
  \begin{equation*}
    \tau = \sum_{k=1}^n t_k \iota(b_k) = 0_M,
  \end{equation*}
  then for every index \( m = 1, \ldots, n \) we have
  \begin{equation*}
    \tau(b_m) = \sum_{k=1}^n t_k [\iota(b_k)](b_m) = t_m = 0_R.
  \end{equation*}

  The above holds when \( \tau \) is an arbitrary vector in \( M \). Therefore, \( \iota[B] \) is a maximal linearly independent set.

  \NecessitySubProof Let \( M \) be a module with Hamel basis \( B \). We will show that \( F(B) \cong M \).

  By \fullref{thm:free_semimodule_universal_property}, the identity function \( b \mapsto b \) on \( B \) can be extended to an \( R \)-module homomorphism \( \varphi: F(B) \to M \) such that \( \varphi(\iota(b)) = b \) for every \( b \in B \). By linearity of \( \varphi \),
  \begin{equation*}
    \varphi(\tau) = \sum_{k=1}^n \tau(b_k) b_k,
  \end{equation*}
  where \( b_1, \ldots, b_n \) is any well-ordering on the \hyperref[def:function_support]{support} \( \supp(\tau) \).

  To see that \( \varphi \) is injective, suppose that \( \varphi(\tau) = \varphi(\sigma) \). Then
  \begin{equation*}
    \varphi(\tau) - \varphi(\sigma) = \sum_{k=1}^n [\tau(b_k) - \sigma(b_k)] b_k = 0_{F(B)}.
  \end{equation*}

  Since \( B \) is a linearly independent set, the above is only possible when \( \tau(b_k) - \sigma(b_k) = 0_R \) for every \( k \). Therefore, \( \tau = \sigma \), and \( \varphi \) is injective.

  To see that \( \varphi \) is surjective, let \( x \in M \). By \fullref{def:semimodule_basis/independent/span_of_basis} and \fullref{def:semimodule_basis/independent/span_and_combinations}, it follows that there exists a linear combination of members of the basis \( B \) of \( M \) such that
  \begin{equation*}
    x = \sum_{k=1}^n t_k b_k.
  \end{equation*}

  Then \( \tau(b_k) \coloneqq t_k \) is a member of \( F(B) \) and, furthermore,
  \begin{equation*}
    \varphi(\tau) = \sum_{k=1}^n \tau(b_k) b_k = \sum_{k=1}^n t_k b_k.
  \end{equation*}

  Therefore, \( \varphi \) is also surjective.

  It follows that \( \varphi: F(B) \to M \) is an isomorphism.
\end{proof}

\begin{definition}\label{thm:hamel_basis_decomposition}
  Let \( B \) be a \hyperref[def:semimodule_basis/independent]{Hamel basis} of the \( R \)-module \( M \). For every member \( b \) of the basis, consider the function
\end{definition}

\begin{proposition}\label{thm:module_basis_decomposition}
  Let \( B \) be a \hyperref[def:semimodule_basis/independent]{Hamel basis} of the \( R \)-module \( M \). Every member of \( M \) can be represented as a linear combination over \( B \) uniquely up the order of summands. We call this the \term{basis decomposition}.

  Explicitly, for every element \( x \) of \( M \), there exists unique finite set \( V \subseteq M \) and a function \( \tau: V \to R \) such that
  \begin{equation*}
    x = \sum_{k=1}^n t_k v_k,
  \end{equation*}
  where \( v_1, \ldots, v_n \) is any well-ordering of \( V \) and \( t_k = \tau(v_k) \).
\end{proposition}
\begin{proof}
  At least one such representation exists by \fullref{def:semimodule_basis/independent/span_of_basis} and \fullref{def:semimodule_basis/independent/span_and_combinations}.

  Suppose that there are two such representations
  \begin{equation*}
    x = \sum_{k=1}^n t_k v_k = \sum_{k=1}^m s_k w_k.
  \end{equation*}

  Without loss of generality, we can assume that \( n = m \) and \( v_k = w_k \) for every index \( k = 1, \ldots, n \). We can justify this by explicitly adding terms with \( 0_R \) as the coefficients where needed.

  Hence,
  \begin{equation*}
    0 = x - x = \sum_{k=1}^n (t_k - s_k) v_k.
  \end{equation*}

  The vectors \( v_1, \ldots, v_n \) are linearly independent since they belong to the basis \( B \). Hence, only a trivial linear combination can give the zero vector. This implies that \( t_k = s_k \) for every index \( k = 1, \ldots, n \).

  Therefore, the representation of \( x \) as a linear combination over \( B \) is unique up to reordering.
\end{proof}

\begin{definition}\label{def:module_basis_projection}
  Let \( M \) be a left \( R \)-module and let \( B \) be a basis of \( M \). For each \( b \in B \), we define the \text{coordinate projection functional} \( \pi_b: M \to R \) that gives us the unique coefficient in the basis decomposition. Thus, for every \( x \in M \) we have
  \begin{equation*}
    x = \sum_{b \in B} \pi_b(x) b.
  \end{equation*}

  \begin{proposition}\label{thm:module_basis_decomposition}
    Let \( B \) be a \hyperref[def:semimodule_basis/independent]{basis} of the free left \( R \)-module \( M \). Then every member of \( M \) can be represented as a linear combination over \( B \) uniquely up the order of summands.

    Explicitly, for every element \( x \) of \( M \), there exists unique finite set \( V \subseteq M \) and a unique function \( \tau: V \to R \) such that
    \begin{equation*}
      x = \sum_{k=1}^n t_k v_k,
    \end{equation*}
    where \( v_1, \ldots, v_n \) is a well-ordering of \( V \) and \( t_k = \tau(v_k) \).
  \end{proposition}
  \begin{proof}
    At least one such representation exists by \fullref{def:semimodule_basis/independent/span_of_basis} and \fullref{def:semimodule_basis/independent/span_and_combinations}.

    Suppose that there are two such representations
    \begin{equation*}
      x = \sum_{k=1}^n t_k v_k = \sum_{k=1}^m s_k w_k.
    \end{equation*}

    Without loss of generality, we can assume that \( n = m \) and \( v_k = w_k \) for every index \( k = 1, \ldots, n \). We can justify this by explicitly adding terms with \( 0_R \) as the coefficients where needed.

    Hence,
    \begin{equation*}
      0 = x - x = \sum_{k=1}^n (t_k - s_k) v_k.
    \end{equation*}

    The vectors \( v_1, \ldots, v_n \) are linearly independent since they belong to the basis \( B \). Hence, only a trivial linear combination can give the zero vector. This implies that \( t_k = s_k \) for every index \( k = 1, \ldots, n \).

    Therefore, the representation of \( x \) as a linear combination over \( B \) is unique up to reordering.
  \end{proof}

  The sum is well-defined since only finitely many terms are nonzero.

  When the basis \( B \) is finite and ordered:
  \begin{equation*}
    B = \{ b_1, \ldots, b_n \},
  \end{equation*}
  we also write
  \begin{equation*}
    x = \sum_{i=1}^n x_k b_i.
  \end{equation*}
\end{definition}
\begin{proof}
  By \fullref{thm:module_basis_decomposition}, this decomposition is unique given a basis \( B \).
\end{proof}

\begin{proposition}\label{thm:left_module_basis_projections_are_linear}
  The basis projection \hyperref[def:module_basis_projection]{maps} are linear.
\end{proposition}
\begin{proof}
  \SubProofOf{def:semimodule/homomorphism/homogeneity} Let \( t \in R \) and \( x \in M \). We have the unique decompositions
  \begin{balign*}
    x  & = \sum_{b \in B} \pi_b(x) b,  \\
    tx & = \sum_{b \in B} \pi_b(tx) b.
  \end{balign*}

  Since both decompositions have only finitely many terms, their difference also has only finitely many nonzero terms. Thus,
  \begin{equation*}
    0
    =
    tx - tx
    =
    t \left( \sum_{b \in B} \pi_b(x) b \right) - \sum_{b \in B} \pi_b(tx) b
    =
    \sum_{b \in B} (t \pi_b(x) - \pi_b(tx)) b.
  \end{equation*}

  Since the vectors in \( B \) are linearly independent, no nontrivial linear combination can equal the zero vector. Thus, for all \( b \in B \),
  \begin{equation*}
    t \pi_b(x) = \pi_b(tx).
  \end{equation*}

  \SubProofOf{def:semimodule/homomorphism/additivity} Analogous.
\end{proof}

\begin{proposition}\label{thm:left_module_basis_cardinality}\mcite{ProofWiki:bases_of_free_module_have_same_cardinality}
  All bases in a free left module over a commutative unital ring have the same cardinality.
\end{proposition}
\begin{definition}\label{def:vector_space}
  A \term{vector space} \( (V, +, \cdot) \) is a \hyperref[def:module]{left module} over a field \( \BbbK \).

  We call elements of \( \BbbK \) \term{scalars} and elements of \( V \) \term{vectors}.

  The category of vector spaces over \( \BbbK \) is denoted by \( \cat{Vect}_{\BbbK} \).
\end{definition}

\begin{definition}\label{def:vector_field}
  Let \( V \) be a vector space over \( \BbbK \). Functions of the type
  \begin{equation*}
    f: \BbbK \to V
  \end{equation*}
  are called \term{vector fields}. To avoid confusion, \( \BbbK \) is sometimes referred to as a \term{scalar field}. This convention comes from physics and is dominant in areas that are far from algebraic field theory, hence in practice it does not cause a lot of confusion.
\end{definition}

\begin{remark}\label{rem:real_vector_space}
  Outside of algebra, we are usually only interested in vector spaces over the fields \( \BbbR \) or \( \BbbC \). We call them \term{real vector spaces} and \term{complex vector spaces}, respectively.
\end{remark}

\begin{definition}\label{def:complex_conjucate_vector_space}
  Let \( V \) be a vector space over the complex numbers \( \BbbC \). Its \term{complex conjugate vector space} \( \overline V \) is the same space, but with scalar multiplication defined as
  \begin{equation*}
    t \cdot_{\overline V} x \coloneqq \overline t \cdot_V x.
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:field_extension_is_vector_space}
  Let \( \BbbK \) be a field \hyperref[def:field_extension]{extension} of \( G \). Then \( \BbbK \) is a vector space over \( G \).
\end{proposition}
\begin{proof}
  Since \( \BbbK \) already has the structure of an abelian group, we must only define scalar multiplication
  \begin{balign*}
     & \circ: G \times \BbbK \to \BbbK, \\
     & g \circ f \coloneqq gf,
  \end{balign*}
  where the product in the definition is simply multiplication in \( \BbbK \). The well-definedness of \( \circ \) follows from the well-definedness of multiplication in \( \BbbK \).
\end{proof}

\begin{remark}\label{rem:linear_span_only_for_vector_spaces}
  The definition for linear \hyperref[def:semimodule/submodel]{span} applies to general commutative \hyperref[def:module]{modules}. However, since \fullref{thm:vector_space_linear_dependence,thm:vector_space_basis} do not apply to general commutative modules, it makes sense to only use linear spans withing the context of vector spaces.
\end{remark}

\begin{proposition}\label{thm:vector_space_linear_dependence}
  The set \( A \subseteq V \) is linearly dependent in the sense of \fullref{def:semimodule_basis/independent} if and only if there exists a vector \( x \in V \) such that
  \begin{equation*}
    x \in \linspan{A} \setminus \{ x \}.
  \end{equation*}
\end{proposition}
\begin{proof}
  \SufficiencySubProof Let \( A \subseteq M \) and let
  \begin{equation*}
    0_M \coloneqq \sum_{k=1}^n t_k x_k,
  \end{equation*}
  where \( t_1, \ldots, t_n \) have at least one nonzero scalar and where \( x_1, \ldots, x_n \) are nonzero vectors. Without loss of generality, assume that \( t_{n_0} \) is the nonzero scalar. Then
  \begin{balign*}
    0_M             & = \sum_{k=1}^n t_k x_k,                                                                                  \\
    t_{k_0} x_{k_0} & = -\sum_{k=1}^n t_k x_k,                                                                                 \\
    x_{k_0}         & = \sum_{k=1}^n \left(-\frac {t_k} {t_{k_0}} \right) x_k \in \linspan{A} \setminus \left\{ x_{k_0} \right\}.
  \end{balign*}

  \NecessitySubProof Let \( A \subseteq M \) and \( x \in \linspan{A} \setminus \{ 0_M, x \} \). By \fullref{def:linear_combination}, there exist nonzero vectors \( x_1, \ldots, x_n \in A \) and scalars \( t_1, \ldots, t_n \in R \) such that
  \begin{equation*}
    x \coloneqq \sum_{k=1}^n t_k x_k,
  \end{equation*}
  where at least one of \( t_1, \ldots, t_n \) is nonzero.

  Then \( 0_M \) is a nontrivial linear combination of the nonzero vectors \( x_1, \ldots, x_n, x \):
  \begin{equation*}
    0_M = \sum_{k=1}^n t_k x_k - x.
  \end{equation*}
\end{proof}

\begin{definition}\label{affine_independence}
  Given a vector space \( V \) over \( F \), we say that a set \( A \subseteq V \) of vectors is \term{affinely independent} in \( V \) if the set
  \begin{equation*}
    \{ (x, 1) \colon x \in A \}
  \end{equation*}
  is linearly independent in \( V \times F \).
\end{definition}

\begin{proposition}\label{thm:vector_space_basis}
  The set \( B \subseteq V \) is a basis in the sense of \fullref{def:module_hamel_basis} if and only if it is linearly independent and
  \begin{equation*}
    V = \linspan{B}.
  \end{equation*}
\end{proposition}
\begin{proof}
  \SufficiencySubProof We will prove the contraposition, that is, if \( \linspan{B} \neq M \), then \( B \) is not a maximal linearly independent set.

  If \( \linspan{B} \subsetneq M \), then there exists a vector \( x \in M \) such that \( x \) is not a linear combination of any subset of \( B \). Thus, \( B \cup \{ x \} \) does not have a nontrivial linear combination that equals zero. Hence, \( B \cup \{ x \} \) is linearly independent.

  \NecessitySubProof Let \( B \subseteq M \) and \( \linspan{B} = M \). Assume that there exists a vector \( x \in M \setminus B \) such that the set \( B \cup \{ x \} \) is linearly independent.

  Then, for any \( b \in B \), the vector \( x + b \) is a linear combination of elements, one of which is independent of \( B \). Thus, by \fullref{thm:vector_space_linear_dependence},
  \begin{equation*}
    M = \linspan{B} \subsetneq \linspan{B \cup \{ x \}} \subseteq M,
  \end{equation*}
  which is a contradiction.
\end{proof}

\begin{theorem}[Every vector space has a basis]\label{thm:every_vector_space_has_a_basis}
  Every vector space has a \hyperref[def:module_hamel_basis]{basis}. Equivalently, all vector spaces are \hyperref[def:free_semimodule]{free modules}.

  Compare this to finite-dimensional vector spaces of order \( n \) over \( \BbbK \), which are all isomorphic to \( \BbbK^n \).

  In \hyperref[def:zfc]{\logic{ZF}} this theorem is equivalent to the \hyperref[def:zfc/choice]{axiom of choice} --- see \fullref{thm:axiom_of_choice_equivalences/vector_space_bases}.
\end{theorem}
\begin{proof}
  Let \( V \) be a vector space. Assume that it does not have a basis. Let \( \mathcal{B} \) be the family of all linearly independent \hyperref[def:linear_combination]{subsets} of \( V \).

  The family \( \mathcal{B} \) is obviously nonempty since any \hyperref[rem:singleton_sets]{singleton} from \( V \) belongs to \( \mathcal{B} \). The union of any chain \( \mathcal{B}' \subseteq \mathcal{B} \) can then contain only linearly independent elements since otherwise we would have that some set in \( \mathcal{B}' \) is not linearly independent. Thus, we can apply \fullref{thm:zorns_lemma} to obtain a maximal element \( B \).

  Assume that \( B \) is not a basis, that is,
  \begin{equation*}
    \linspan B \subsetneq V.
  \end{equation*}

  Take \( V \in V \setminus \linspan B \). Then the set \( B \cup \{ v \} \) is linearly independent, which contradicts the assumption that \( B \) is not a basis. Thus, \( B \) is a basis of \( V \) and \( V \) is a free module.
\end{proof}

\begin{definition}\label{def:vector_space_dimension}
  The \hyperref[def:free_semimodule]{free module rank} of a vector space \( V \) is called the \term{dimension} \( \dim V \) of \( V \). If \( U \) is a vector subspace of \( V \), we call \( \co\dim_V U \coloneqq \dim(V/U) \) the \term{codimension} of \( U \) relative to \( V \).
\end{definition}

\begin{proposition}\label{thm:linear_maps_form_algebra}
  The set \( \hom(U, V) \) is a vector space.
\end{proposition}
\begin{proof}
  By \fullref{thm:functions_over_semimodule}, \( \hom(U, V) \) forms an \( \BbbK \)-vector space.
\end{proof}

\begin{remark}\label{rem:functional}
  The term \enquote{functional} does not have a strict meaning. For example, logicians use terms like \enquote{primitive recursive functional} for certain generalized functions. Functions are also ill-defined, see \fullref{rem:function_definition}. Outside of logic, however, the term \enquote{functional} usually refers to a function from a vector space \( V \) to its base field \( \BbbK \). Examples include linear \hyperref[def:semimodule/homomorphism]{functionals}, like projection \hyperref[def:module_basis_projection]{maps} and \hyperref[def:differentiability]{derivatives}, and nonlinear functionals, like the Minkowski \hyperref[def:minkowski_functional]{functionals}.
\end{remark}

\begin{definition}\label{def:eigenpair}
  Let \( f: U \to V \) be a function between vector spaces over \( \BbbK \).

  An \term{eigenpair} of \( f \) consists of an \term{eigenvalue} \( \lambda \in \BbbK \) and an \term{eigenvector} \( x \in U \) such that
  \begin{equation*}
    f(x) = \lambda x.
  \end{equation*}
\end{definition}
