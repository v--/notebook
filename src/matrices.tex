\subsection{Matrices}\label{subsec:matrices}

\begin{definition}\label{def:array}\mimprovised
  Let \( S \) be any nonempty \hyperref[def:set]{set} and \( n_1, \ldots, n_k \) be \hyperref[def:integer_signum]{positive integers}. An \term{array} of shape \( n_1 \times \cdots \times n_k \) is a \hyperref[def:function]{function} with signature
  \begin{equation*}
    A: \set{ 1, 2, \ldots, n_1 } \times \ldots \times \set{ 1, 2, \ldots, n_k } \to S.
  \end{equation*}

  \enquote{Multi-dimensional array} is also used as a term, but we will avoid it because the terminology conflicts with \hyperref[thm:vector_space_dimension]{vector space dimensions}.

  We can regard \enquote{\( n_1 \times \cdots \times n_k \)} simultaneously as a convenient notation and as a \hyperref[def:cartesian_product]{Cartesian product} of finite \hyperref[def:ordinal]{ordinals} (modulo the fact that finite ordinals are zero-based).

  In particular:
  \begin{thmenum}
    \thmitem{def:array/matrix} A two-dimensional array of shape \( m \times n \) is usually called a \term{matrix}. Let \( A \) be an \( m \times n \)-matrix. We will denote \( A \) as
    \begin{equation*}
      A = \seq{ a_{i,j} }_{i,j=1}^{m,n}
    \end{equation*}
    or graphically as the table
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{m,1} & a_{m,2} & \cdots & a_{m,n}
      \end{pmatrix}.
    \end{equation*}

    \thmitem{def:array/square_matrix} A \term{square matrix} of order \( n \) is simply an \( n \times n \) matrix.

    \thmitem{def:array/column_vector} A \term{column vector} of dimension \( m \) is simply a \( m \times 1 \) matrix
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} \\
        \vdots  \\
        a_{m,1}
      \end{pmatrix}.
    \end{equation*}

    \thmitem{def:array/row_vector} A \term{row vector} of dimension \( n \) is simply an \( 1 \times n \) matrix
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,n}
      \end{pmatrix}.
    \end{equation*}
  \end{thmenum}
\end{definition}

\begin{remark}\label{rem:vector_etymology}
  In practice, the terms \enquote{vector}, \enquote{tuple} and \enquote{finite sequence} are used interchangeably. Formally, the concepts differ slightly:

  \begin{itemize}
    \item \enquote{Vector} refers to an element of a \hyperref[def:vector_space]{vector space} or, more generally, a \hyperref[def:semimodule]{semimodule}. \hyperref[def:array/column_vector]{Column vectors} and \hyperref[def:array/row_vector]{row vectors} are important special cases.

    \item Tuples are defined and discussed in \fullref{def:cartesian_product/tuple}. Tuples are technically \hyperref[def:cartesian_product/indexed_family]{indexed families} and the latter are defined without reference to functions, which we use to define both arrays and sequences.

    \item Sequences are defined and discussed in \fullref{def:sequence}. Formally, a finite sequence of length \( n \) is the same as an array of shape \( n \).
  \end{itemize}
\end{remark}

\begin{definition}\label{def:block_matrix}
  A \term{block matrix} is a \enquote{matrix of matrices}. That is, a matrix of the form
  \begin{equation*}
    \begin{pmatrix}
      A_{1,1} & A_{1,2} & \cdots & A_{1,n} \\
      A_{2,1} & A_{2,2} & \cdots & A_{2,n} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      A_{m,1} & A_{m,2} & \cdots & A_{m,n}
    \end{pmatrix},
  \end{equation*}
  where all \( A_{i,j} \) are matrices of compatible dimensions.

  We can write the block matrix
  \begin{equation*}
    \begin{pmatrix}
      A      & \cdots & B      \\
      \vdots & \ddots & \vdots \\
      C      & \cdots & D
    \end{pmatrix}
  \end{equation*}
  as
  \begin{equation*}
    \parens*
      {
        \begin{array}{ccc|c|ccc}
          a_{1,1}   & \cdots & a_{1,n_A}   & \cdots & b_{1,1}   & \cdots & b_{1,n_B} \\
          \vdots    & \ddots & \vdots      & \cdots & \vdots    & \ddots & \vdots \\
          a_{m_A,1} & \cdots & a_{m_A,n_A} & \cdots & b_{m_B,1} & \cdots & b_{m_B,n_B} \\
          \hline
          \vdots    & \vdots & \vdots      & \ddots & \vdots    & \vdots & \vdots \\
          \hline
          c_{1,1}   & \cdots & c_{1,n_C}   & \cdots & d_{1,1}   & \cdots & d_{1,n_D} \\
          \vdots    & \ddots & \vdots      & \cdots & \vdots    & \ddots & \vdots \\
          c_{m_C,1} & \cdots & c_{m_C,n_C} & \cdots & d_{m_D,1} & \cdots & d_{m_D,n_D} \\
        \end{array}
      }.
  \end{equation*}

  Given any matrix \( A = \seq{ a_{i,j} }_{i,j=1}^{n,m} \), we can represent it via its block matrix of rows
  \begin{equation*}
    \parens*
      {
        \begin{array}{c}
          a_{1,\anon*} \\
          \hline
          a_{2,\anon*} \\
          \hline
          \vdots \\
          \hline
          a_{n,\anon*}
        \end{array}
      },
  \end{equation*}
  and its block matrix of columns
  \begin{equation*}
    \parens*
      {
        \begin{array}{c|c|c|c}
          a_{\anon*,1} & a_{\anon*,2} & \cdots & a_{\anon*,m}
        \end{array}
      }
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:matrix_algebra}
  Denote by \( R^{m \times n} \) the set of \( m \times n \) \hyperref[def:array/matrix]{matrices} over the \hyperref[def:semiring]{semiring} \( R \). We define three operations on matrices:
  \begin{thmenum}
    \thmitem{def:matrix_algebra/addition} We define \term{matrix addition} \( +: R^{m \times n} \times R^{m \times n} \to R^{m \times n} \) componentwise as
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,n} \\
        \vdots  & \ddots & \vdots  \\
        a_{m,1} & \cdots & a_{m,n}
      \end{pmatrix}
      +
      \begin{pmatrix}
        b_{1,1} & \cdots & b_{1,n} \\
        \vdots  & \ddots & \vdots  \\
        b_{m,1} & \cdots & b_{m,n}
      \end{pmatrix}
      \coloneqq
      \begin{pmatrix}
        a_{1,1} + b_{1,1} & \cdots & a_{1,n} + b_{1,n} \\
        \vdots            & \ddots & \vdots            \\
        a_{m,1} + b_{m,1} & \cdots & a_{m,n} + b_{m,n}
      \end{pmatrix}.
    \end{equation*}

    \thmitem{def:matrix_algebra/scalar_multiplication} We define \term{scalar multiplication} as
    \begin{equation*}
       \lambda \cdot \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,n} \\
        \vdots  & \ddots & \vdots  \\
        a_{m,1} & \cdots & a_{m,n}
      \end{pmatrix}
      \coloneqq
      \begin{pmatrix}
        \lambda a_{1,1} & \cdots & \lambda a_{1,n} \\
        \vdots          & \ddots & \vdots          \\
        \lambda a_{m,1} & \cdots & \lambda a_{m,n}
      \end{pmatrix}.
    \end{equation*}

    \thmitem{def:matrix_algebra/matrix_multiplication} We define \term{matrix multiplication} in two steps. The complexity of the definition is justified by \fullref{thm:finite_dimensional_operators_are_isomorphic_to_matrices}. First, if \( \seq{ a_{1,j} }_{j=1}^n \) is a \hyperref[def:array/row_vector]{row vector} and \( \seq{ b_{i,1} }_{i=1}^m \) is a \hyperref[def:array/column_vector]{column vector}, we define their \term{inner product} as
    \begin{equation*}
      a \cdot b \coloneqq \sum_{i=1}^n a_i b_i.
    \end{equation*}

    We can now define matrix multiplication \( {}\cdot{}: R^{m \times k} \times R^{k \times n} \to R^{m \times n} \) as
    \begin{equation*}
     \parens*
       {
         \begin{array}{c}
            a_{1,-} \\
            \hline
            a_{2,-} \\
            \hline
            \vdots \\
            \hline
            a_{m,-}
          \end{array}
        }
      \cdot
      \parens*
        {
          \begin{array}{c|c|c|c}
            b_{-,1} & b_{-,2} & \cdots & b_{-,n}
          \end{array}
        }
      \coloneqq
      \begin{pmatrix}
        a_{1,-} \cdot b_{-,1} & a_{1,-} \cdot b_{-,2} & \vdots & a_{1,-} \cdot b_{-,n} \\
        a_{2,-} \cdot b_{-,1} & a_{2,-} \cdot b_{-,2} & \vdots & a_{2,-} \cdot b_{-,n} \\
        \vdots                & \vdots                & \ddots & \vdots                \\
        a_{m,-} \cdot b_{-,1} & a_{m,-} \cdot b_{-,2} & \cdots & a_{m,-} \cdot b_{-,n}
      \end{pmatrix}.
    \end{equation*}
  \end{thmenum}

  Under \hyperref[def:matrix_algebra/addition]{addition} and \hyperref[def:matrix_algebra/scalar_multiplication]{scalar multiplication}, \( R^{m \times n} \) becomes an \( R \)-\hyperref[def:semimodule]{semimodule}. If, additionally, \( n \) and \( m \) are equal, \( R^{n \times n} \) becomes an \( R \)-\hyperref[def:algebra_over_semiring]{algebra} under \hyperref[def:matrix_algebra/matrix_multiplication]{matrix multiplication}.
\end{proposition}
\begin{proof}
  The semimodule structure is inherited by the \hyperref[thm:semiring_is_semimodule]{semimodule structure} on \( R \). We will show that, if \( n = n \), matrix multiplication is associative and bilinear. Fix matrices
  \begin{equation*}
    \begin{aligned}
      A = \seq{ a_{i,j} }_{i,j=1}^{m,k} && B = \seq{ b_{i,j} }_{i,j=1}^{k,l} && C = \seq{ c_{i,j} }_{i,j=1}^{l,n}.
    \end{aligned}
  \end{equation*}

  \SubProofOf[def:magma/associative]{associativity} The \( (i, j) \)-th entry in \( D \coloneqq (AB)C \) is
  \begin{equation*}
    d_{i,j} = \sum_{s=1}^n \parens*{ \sum_{r=1}^n a_{i,r} \cdot b_{r,s} } \cdot c_{s,j}.
  \end{equation*}

  Due to distributivity,
  \begin{equation*}
    d_{i,j}
    =
    \sum_{s=1}^n \sum_{r=1}^n a_{i,r} \cdot b_{r,s} \cdot c_{s,j}
    =
    \sum_{r=1}^n a_{i,r} \cdot \parens*{ \sum_{s=1}^n b_{r,s} \cdot c_{s,j} },
  \end{equation*}
  which is the \( (i, j) \)-th entry in \( A(BC) \).

  Therefore, \( (AB)C = A(BC) \).

  \SubProofOf[def:semimodule/homomorphism/additive]{additivity} Again due to distributivity,
  \begin{equation*}
    \sum_{r=1}^n \parens*{ a_{i,r} + b_{i,r} } \cdot c_{r,j}
    =
    \sum_{r=1}^n a_{i,r} \cdot c_{r,j} + \sum_{r=1}^n b_{i,r} \cdot c_{r,j}.
  \end{equation*}

  Therefore, \( (A + B)C = AC + BC \). The proof that \( A(B + C) = AB + AC \) is analogous.

  \SubProofOf[def:semimodule/homomorphism/homogeneity]{homogeneity} Again due to distributivity,
  \begin{equation*}
    t \cdot \sum_{r=1}^n a_{i,r} \cdot b_{r,j}
    =
    \sum_{r=1}^n (t \cdot a_{i,r}) \cdot b_{r,j}
    =
    \sum_{r=1}^n a_{i,r} \cdot (t \cdot b_{r,j}).
  \end{equation*}

  Therefore, \( t(AB) = (tA)B = A(tB) \).
\end{proof}

\begin{example}\label{ex:matrix_multiplication_is_noncommutative}
  For \( n > 1 \), the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) is a noncommutative ring. Consider the following example:
  \begin{align*}
    \begin{pmatrix}
      0 & 0 \\
      0 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix}
    &=
    \begin{pmatrix}
      0 & 0 \\
      1 & 0
    \end{pmatrix},
    \\
    \begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix}
    \begin{pmatrix}
      0 & 0 \\
      0 & 1
    \end{pmatrix}
    &=
    \begin{pmatrix}
      0 & 0 \\
      0 & 0
    \end{pmatrix}.
  \end{align*}
\end{example}

\begin{proposition}\label{thm:finite_dimensional_operators_are_isomorphic_to_matrices}
  For a \hyperref[def:ring/commutative]{commutative ring} \( R \), the \hyperref[def:matrix_algebra]{matrix algebra} \( R^{m \times n} \) is \hyperref[def:algebra_over_semiring/homomorphism]{isomorphic} to the \hyperref[thm:functions_over_algebra]{linear function algebra} \( \hom(R^n, R^m) \)\footnote{Note that the maps are from \( R^n \) to \( R^m \) and not vice versa}.

  This justifies using juxtaposition for application of linear functions, e.g. \( Lx \) rather than \( L(x) \).
\end{proposition}

\begin{definition}\label{def:matrix_determinant}\mcite[215]{Knapp2016BasicAlgebra}
  Fix the \hyperref[def:matrix_algebra]{matrix space} \( R^{n \times n} \) over a commutative unital ring \( R \). We define its determinant as
  \begin{balign*}
     & \det: R^{n \times n} \to R                                                                  \\
     & \det(\{ a_{i,j} \}_{i,j=1}^n) \coloneqq \sum_{p \in S_n} \sgn(p) \prod_{i=1}^n a_{i,p(i)},
  \end{balign*}
  where \( S_n \) is the \hyperref[def:symmetric_group]{symmetric group} of order \( n \).

  The determinant of a matrix is not invertible, we say that it is \term{singular}. \Fullref{thm:matrix_invertible_iff_nonsingular} gives a strong link between the invertibility of a matrix and the invertibility of its determinant.

  If \( R \) is a field, then only the zero is not invertible and hence only matrices with \( \det A = 0 \) are singular.
\end{definition}

\begin{example}\label{ex:vandermonde_matrix}\mcite[corr. 2.37]{Knapp2016BasicAlgebra}
  The matrix
  \begin{equation*}
    V_n(x_0, x_1, \ldots, x_n)
    \coloneqq
    \begin{pmatrix}
      x_0^0  & x_0^1  & x_0^2  & \cdots & x_0^n  \\
      x_1^0  & x_1^1  & x_1^2  & \cdots & x_1^n  \\
      \vdots & \vdots & \vdots & \ddots & \vdots \\
      x_n^0  & x_n^1  & x_n^2  & \cdots & x_n^n
    \end{pmatrix}.
  \end{equation*}
  is called the \term{Vandermonde matrix} corresponding to \( x_0, \ldots, x_n \).

  Its determinant is
  \begin{equation*}
    \det(V_n) = \prod_{i < j} (r_j - r_i).
  \end{equation*}

  Hence, the determinant is nonzero if and only if all of \( x_0, \ldots, x_n \) are distinct.
\end{example}

\begin{proposition}\label{thm:def:matrix_determinant}\mcite[prop. 5.1]{Knapp2016BasicAlgebra}
  Matrix determinants over the commutative unital ring \( R \) have the following basic properties:
  \begin{thmenum}
    \thmitem{thm:def:matrix_determinant/identity} For the identity matrix \( E_n \in R^n \) we have
    \begin{equation*}
      \det(E_n) = 1.
    \end{equation*}

    \thmitem{thm:def:matrix_determinant/transpose} For the transpose \hyperref[def:matrix_transpose]{matrix} \( A^T \) of \( A \in R^n \), we have
    \begin{equation*}
      \det(A^T) = \det(A).
    \end{equation*}

    \thmitem{thm:def:matrix_determinant/product} For matrices \( A, B \in R^n \) we have
    \begin{equation*}
      \det(A) \det(B) = \det(AB).
    \end{equation*}
  \end{thmenum}
\end{proposition}

\begin{proposition}\label{thm:matrix_invertible_iff_nonsingular}\mcite[cor. 5.5]{Knapp2016BasicAlgebra}
  A matrix over \( R \) is invertible if and only if its determinant is invertible in \( R \).

  In particular, a matrix over a field is invertible if and only if its determinant is nonzero.
\end{proposition}

\begin{definition}\label{def:inverse_matrix}
  Let \( A \) be a square matrix of order \( n \) over a semiring \( R \). We say that \( B \) is an \term{inverse matrix} of \( A \) if
  \begin{equation*}
    AB = BA = E_n.
  \end{equation*}

  An inverse matrix, if it exists, is unique. We denote this inverse of \( A \) by \( A^{-1} \).

  The set of all invertible matrices of order \( n \) over \( R \) is called the \term{general linear group} and is denoted by \( \op{GL}_n(R) \). It forms a group with respect to matrix multiplication.

  If \( R \) is commutative, we also consider the \term{special linear group} \( \op{SL}_n(R) \) of matrices with \hyperref[def:matrix_determinant]{determinant} \( 1 \).
\end{definition}
\begin{proof}
  The inverse is unique by \fullref{thm:monoid_inverse_unique}.
\end{proof}

\begin{proposition}\label{thm:general_linear_group_isomorphic_to_automorphism_group}
  Fix a semiring \( R \). The general linear group \( \op{GL}_n(R) \) is isomorphic to the group of all invertible linear transformations over \( R^n \) under composition.
\end{proposition}
\begin{proof}
  Follows from \fullref{thm:finite_dimensional_operators_are_isomorphic_to_matrices}.
\end{proof}

\begin{definition}\label{def:orthogonal_matrix}
  Let \( R \) be a semiring. We say that the square matrix \( A \) is \term{orthogonal} if \( A^T = A^{-1} \). If \( R \) is a commutative unital ring, the set of all orthogonal matrices of order \( n \) forms a subgroup of \( \op{GL}_n(R) \) called the \term{orthogonal group} \( \op{O}_n(R) \).
\end{definition}

\begin{definition}\label{def:unitary_matrix}
  We say that the complex square matrix \( A \) is \term{unitary} if \( A^\dagger = A^{-1} \). The set of all unitary matrices of order \( n \) is called the unitary group \( \op{U}_n \) and is a subgroup of \( \op{GL}_n(\co) \).
\end{definition}

\begin{definition}\label{def:matrix_column_and_row_space}
  Fix a matrix \( A \in R^{n \times m} \) over a semiring \( R \). We define its \term{row space} as
  \begin{equation*}
    \linspan \{ a_{i,-} \colon i = 1, \ldots, n \}
  \end{equation*}
  and its \term{column space} as
  \begin{equation*}
    \linspan \{ a_{-,j} \colon j = 1, \ldots, m \}.
  \end{equation*}
\end{definition}

\begin{definition}\label{def:matrix_transpose}
  Let \( A = \{ a_{i,j} \}_{i,j=1}^{n,m} \) be a matrix. We define its \term{transpose matrix} by \term{flipping it over its main diagonal}, that is,
  \begin{equation*}
    A^T \coloneqq \begin{pmatrix}
      a_{1,1} & a_{2,1} & \cdots & a_{n,1} \\
      a_{1,2} & a_{2,2} & \cdots & a_{n,2} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      a_{1,m} & a_{2,m} & \cdots & a_{n,m}
    \end{pmatrix}.
  \end{equation*}
\end{definition}

\begin{definition}\label{def:symmetric_matrix}
  A square matrix \( A \) is said to be \term{symmetric} if \( A = A^T \).
\end{definition}

\begin{definition}\label{def:matrix_conjugate_transpose}
  Let \( A \) be a complex matrix. We define its \term{conjugate transpose matrix} as
  \begin{equation*}
    A^\dagger \coloneqq \begin{pmatrix}
      \overline{a_{1,1}} & \overline{a_{2,1}} & \cdots & \overline{a_{n,1}} \\
      \overline{a_{1,2}} & \overline{a_{2,2}} & \cdots & \overline{a_{n,2}} \\
      \vdots       & \vdots       & \ddots & \vdots       \\
      \overline{a_{1,m}} & \overline{a_{2,m}} & \cdots & \overline{a_{n,m}}
    \end{pmatrix}.
  \end{equation*}
\end{definition}

\begin{definition}\label{def:hermitian_matrix}
  If \( A \) is a complex matrix, we say that it is Hermitian if \( A = A^\dagger \).
\end{definition}

\begin{proposition}\label{thm:dual_linear_operator_matrix_transpose}
  Let \( L: F^m \to F^n \) be a linear operator and let \( L^*: {F^n}^* \to {F^m}^* \) be its dual \hyperref[def:dual_linear_operator]{operator}.

  If \( A \in F^{n \times m} \) is the matrix of \( L \), then its \hyperref[def:matrix_transpose]{transpose} \( A^T \) is the matrix of \( L^* \) when regarding \( L^* \) as an operator acting on column vectors.
\end{proposition}
\begin{proof}
  Let \( l \in {F^n}^* \) be a linear functional regarded as a function and \( \vect l \) be the same functional regarded as a column \hyperref[rem:finite_dimensional_dual_space_isomorphism]{vector}. We have
  \begin{equation*}
    L^*(l)
    =
    l \circ L
    =
    (x \mapsto l(L(x)))
    =
    (x \mapsto \vect l^T Ax)
    =
    \vect l^T A.
  \end{equation*}

  Thus,
  \begin{equation*}
    L^*(l) = A^T \vect l,
  \end{equation*}
  i.e. the matrix \( A^T \) corresponds to the dual operator \( L^* \).
\end{proof}

\begin{proposition}\label{thm:column_and_row_spaces_are_images}
  Fix a semiring \( R \). Let \( L: R^m \to R^n \) be a linear map and let \( A \in R^{n \times m} \) be the corresponding \hyperref[thm:finite_dimensional_operators_are_isomorphic_to_matrices]{matrix}. The \hyperref[def:matrix_column_and_row_space]{column space} of \( A \) is isomorphic to the image \( \img(L) \) and the row space is isomorphic to \( \img(L^*) \).
\end{proposition}
\begin{proof}
  The column space of \( A \) lies within \( R^{n \times 1} \), which is isomorphic to \( R^n \). We will assume that it is a subset of \( R^n \) and will prove that it is equal to \( \img(L) \).

  Denote by \( e_1, \ldots, e_m \) the basis of \( R^m \). The \( j \)-th column \( a_{-,j} \) of \( A \) can be represented as
  \begin{equation*}
    A e_j = a_{-,j}.
  \end{equation*}

  Thus, \( a_{-,j} \in \img(L), j = 1, \ldots, m \). Since \( \img(L) \) is a linear subspace of \( R^n \), it contains the linear span of any finite collection of its vectors. Consequently, the column space of \( A \) is a subspace of \( \img(L) \).

  To see the converse, let \( x \in \BbbR^m \). We have
  \begin{equation*}
    L(x) = Ax = \sum_{j=1}^m x_i A e_j = \sum_{j=1}^m a_{-,j}.
  \end{equation*}

  Hence, the image of any vector \( x \) under \( L \) is a linear combination of the columns of \( A \).

  Thus, proves that the column space of \( A \) is equal to \( \img(L) \).

  The proof that the row space is isomorphic to \( \img(L^*) \) is identical, noting that \( A^T \) corresponds to \( L^* \) by \fullref{thm:dual_linear_operator_matrix_transpose}.
\end{proof}
