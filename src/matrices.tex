\subsection{Matrices}\label{subsec:matrices}

Although we will mostly be concerned with matrices over \hyperref[def:field]{fields}, in this subsection we will work over \hyperref[def:ring/commutative]{commutative unital rings}. Some definitions and statements hold even more generally, in arbitrary \hyperref[def:semiring]{semirings}, however we have only developed a suitable theory of \hyperref[def:hamel_basis]{Hamel bases} over commutative rings.

\begin{definition}\label{def:array}\mimprovised
  Let \( S \) be any nonempty \hyperref[def:set]{set} and \( n_1, \ldots, n_k \) be \hyperref[def:integer_signum]{positive integers}. An \term{array} of shape \( n_1 \times \cdots \times n_k \) is a \hyperref[def:function]{function} with signature
  \begin{equation*}
    A: \set{ 1, 2, \ldots, n_1 } \times \ldots \times \set{ 1, 2, \ldots, n_k } \to S.
  \end{equation*}

  \enquote{Multi-dimensional array} is also used as a term, but we will avoid it because the terminology conflicts with \hyperref[thm:vector_space_dimension]{vector space dimensions}.

  We can regard \enquote{\( n_1 \times \cdots \times n_k \)} simultaneously as a convenient notation and as a \hyperref[def:cartesian_product]{Cartesian product} of finite \hyperref[def:ordinal]{ordinals} (modulo the fact that finite ordinals are zero-based).

  In particular:
  \begin{thmenum}
    \thmitem{def:array/matrix} A two-dimensional array of shape \( m \times n \) is usually called a \term{matrix}. Let \( A \) be an \( m \times n \)-matrix. We will denote \( A \) as
    \begin{equation*}
      A = \seq{ a_{i,j} }_{i,j=1}^{m,n}
    \end{equation*}
    or graphically as the table
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{m,1} & a_{m,2} & \cdots & a_{m,n}
      \end{pmatrix}.
    \end{equation*}

    \thmitem{def:array/square_matrix} A \term{square matrix} of order \( n \) is simply an \( n \times n \) matrix.

    \thmitem{def:array/column_vector} A \term{column vector} of dimension \( m \) is simply a \( m \times 1 \) matrix
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} \\
        \vdots  \\
        a_{m,1}
      \end{pmatrix}.
    \end{equation*}

    When \( S \) is a \hyperref[def:semiring]{semiring} \( R \), we often identify the set of all \( m \)-dimensional column vectors with the free semimodule \hyperref[def:standard_basis]{\( R^m \)}.

    \thmitem{def:array/row_vector} A \term{row vector} of dimension \( n \) is simply an \( 1 \times n \) matrix
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,n}
      \end{pmatrix}.
    \end{equation*}
  \end{thmenum}
\end{definition}

\begin{remark}\label{rem:vector_etymology}
  In practice, the terms \enquote{vector}, \enquote{tuple} and \enquote{finite sequence} are used interchangeably. Formally, the concepts differ slightly:

  \begin{itemize}
    \item \enquote{Vector} refers to an element of a \hyperref[def:vector_space]{vector space} or, more generally, a \hyperref[def:semimodule]{semimodule}. \hyperref[def:array/column_vector]{Column vectors} and \hyperref[def:array/row_vector]{row vectors} are important special cases.

    \item Tuples are defined and discussed in \fullref{def:cartesian_product/tuple}. Tuples are technically \hyperref[def:cartesian_product/indexed_family]{indexed families} and the latter are defined without reference to functions, which we use to define both arrays and sequences.

    \item Sequences are defined and discussed in \fullref{def:sequence}. Formally, a finite sequence of length \( n \) is the same as an array of shape \( n \).
  \end{itemize}
\end{remark}

\begin{definition}\label{def:block_matrix}
  A \term{block matrix} is a \enquote{matrix of matrices}. That is, a matrix of the form
  \begin{equation*}
    \begin{pmatrix}
      A_{1,1} & A_{1,2} & \cdots & A_{1,n} \\
      A_{2,1} & A_{2,2} & \cdots & A_{2,n} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      A_{m,1} & A_{m,2} & \cdots & A_{m,n}
    \end{pmatrix},
  \end{equation*}
  where all \( A_{i,j} \) are matrices of compatible dimensions.

  We can write the block matrix
  \begin{equation*}
    \begin{pmatrix}
      A      & \cdots & B      \\
      \vdots & \ddots & \vdots \\
      C      & \cdots & D
    \end{pmatrix}
  \end{equation*}
  as
  \begin{equation*}
    \parens*
      {
        \begin{array}{ccc|c|ccc}
          a_{1,1}   & \cdots & a_{1,n_A}   & \cdots & b_{1,1}   & \cdots & b_{1,n_B} \\
          \vdots    & \ddots & \vdots      & \cdots & \vdots    & \ddots & \vdots \\
          a_{m_A,1} & \cdots & a_{m_A,n_A} & \cdots & b_{m_B,1} & \cdots & b_{m_B,n_B} \\
          \hline
          \vdots    & \vdots & \vdots      & \ddots & \vdots    & \vdots & \vdots \\
          \hline
          c_{1,1}   & \cdots & c_{1,n_C}   & \cdots & d_{1,1}   & \cdots & d_{1,n_D} \\
          \vdots    & \ddots & \vdots      & \cdots & \vdots    & \ddots & \vdots \\
          c_{m_C,1} & \cdots & c_{m_C,n_C} & \cdots & d_{m_D,1} & \cdots & d_{m_D,n_D} \\
        \end{array}
      }.
  \end{equation*}

  Given any matrix \( A = \seq{ a_{i,j} }_{i,j=1}^{n,m} \), we can represent it via its block matrix of rows
  \begin{equation*}
    \parens*
      {
        \begin{array}{c}
          a_{1,\anon*} \\
          \hline
          a_{2,\anon*} \\
          \hline
          \vdots \\
          \hline
          a_{n,\anon*}
        \end{array}
      },
  \end{equation*}
  and its block matrix of columns
  \begin{equation*}
    \parens*
      {
        \begin{array}{c|c|c|c}
          a_{\anon*,1} & a_{\anon*,2} & \cdots & a_{\anon*,m}
        \end{array}
      }
  \end{equation*}
\end{definition}

\begin{definition}\label{def:matrix_diagonal}
  The \term{main diagonal} of the matrix \( A = \seq{ a_{i,j} }_{i,j=1}^{m,n} \) is the sequence \( a_{1,1}, \ldots, a_{i,i}, \ldots, a_{k,k} \), where \( k \coloneqq \min\set{ m, n } \). The \term{antidiagonal} is instead \( a_{1,k}, \ldots, a_{i,k-i}, \ldots, a_{k,n-k} \). These can be visualized as follows:
  \begin{equation*}
    \begin{pmatrix}
      \fbox{\( a_{1,1} \)} & a_{1,2}                & \cdots & a_{1,k-1}                & \fbox{\( a_{k,k} \)} & \cdots \\
      a_{2,1}              & \fbox{\( a_{2,2} \)}   &        & \fbox{\( a_{2,k-1} \)}   & a_{2,k}              &        \\
      \vdots               &                        & \ddots &                          & \vdots               &        \\
      a_{k-1,1}            & \fbox{\( a_{k-1,2} \)} &        & \fbox{\( a_{k-1,k-1} \)} & a_{k-1,k}            & \cdots \\
      \fbox{\( a_{k,1} \)} & a_{k,2}                & \cdots & a_{k,k-1}                & \fbox{\( a_{k,k} \)} &        \\
      \vdots               &                        &        & \vdots                   &                      & \ddots
    \end{pmatrix}
  \end{equation*}

  Over a \hyperref[def:semiring]{semiring}, we say that a square matrix \term{diagonal} if all entries outside the main diagonal are zero. For brevity, we write
  \begin{equation*}
    \op{diag}(a_1, \ldots, a_n)
    \coloneqq
    \begin{pmatrix}
      a_1    & 0      & \cdots & 0      \\
      0      & a_2    & \cdots & 0      \\
      \vdots & \vdots & \ddots & \vdots \\
      0      & 0      & \cdots & a_n
    \end{pmatrix}
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:matrix_algebra}
  Denote by \( R^{m \times n} \) the set of \( m \times n \) \hyperref[def:array/matrix]{matrices} over the \hyperref[def:semiring]{semiring} \( R \). We define three operations on matrices:
  \begin{thmenum}
    \thmitem{def:matrix_algebra/addition} We define \term{matrix addition} \( +: R^{m \times n} \times R^{m \times n} \to R^{m \times n} \) componentwise as
    \begin{equation*}
      \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,n} \\
        \vdots  & \ddots & \vdots  \\
        a_{m,1} & \cdots & a_{m,n}
      \end{pmatrix}
      +
      \begin{pmatrix}
        b_{1,1} & \cdots & b_{1,n} \\
        \vdots  & \ddots & \vdots  \\
        b_{m,1} & \cdots & b_{m,n}
      \end{pmatrix}
      \coloneqq
      \begin{pmatrix}
        a_{1,1} + b_{1,1} & \cdots & a_{1,n} + b_{1,n} \\
        \vdots            & \ddots & \vdots            \\
        a_{m,1} + b_{m,1} & \cdots & a_{m,n} + b_{m,n}
      \end{pmatrix}.
    \end{equation*}

    With addition, \( R^{m \times n} \) becomes an \hyperref[def:monoid/commutative]{commutative monoid} with neutral element the \term{zero matrix}
    \begin{equation}\label{eq:def:matrix_algebra/matrix_multiplication/zero}
      \begin{pmatrix}
        0       & 0      & \cdots & 0      \\
        0       & 0      & \cdots & 0      \\
        \vdots  & \vdots & \ddots & \vdots \\
        0       & \vdots & \cdots & 0
      \end{pmatrix}.
    \end{equation}

    \thmitem{def:matrix_algebra/scalar_multiplication} We define \term{scalar multiplication} \( \cdot: R \times R^{m \times n} \to R^{m \times n} \) as
    \begin{equation*}
       \lambda \cdot \begin{pmatrix}
        a_{1,1} & \cdots & a_{1,n} \\
        \vdots  & \ddots & \vdots  \\
        a_{m,1} & \cdots & a_{m,n}
      \end{pmatrix}
      \coloneqq
      \begin{pmatrix}
        \lambda a_{1,1} & \cdots & \lambda a_{1,n} \\
        \vdots          & \ddots & \vdots          \\
        \lambda a_{m,1} & \cdots & \lambda a_{m,n}
      \end{pmatrix}.
    \end{equation*}

    Under \hyperref[def:matrix_algebra/addition]{addition} and \hyperref[def:matrix_algebra/scalar_multiplication]{scalar multiplication}, \( R^{m \times n} \) becomes an \( R \)-\hyperref[def:semimodule]{semimodule}.

    \thmitem{def:matrix_algebra/matrix_multiplication} We define \term{matrix multiplication} in two steps. The definition is justified by \fullref{thm:matrix_and_linear_function_algebras}. First, if \( \seq{ a_{1,j} }_{j=1}^n \) is a \hyperref[def:array/row_vector]{row vector} and \( \seq{ b_{i,1} }_{i=1}^m \) is a \hyperref[def:array/column_vector]{column vector}, we define their \term{inner product} as
    \begin{equation*}
      a \cdot b \coloneqq \sum_{i=1}^n a_i b_i.
    \end{equation*}

    We can now define matrix multiplication \( \odot: R^{m \times k} \times R^{k \times n} \to R^{m \times n} \) as
    \begin{equation*}
     \parens*
       {
         \begin{array}{c}
            a_{1,-} \\
            \hline
            a_{2,-} \\
            \hline
            \vdots \\
            \hline
            a_{m,-}
          \end{array}
        }
      \odot
      \parens*
        {
          \begin{array}{c|c|c|c}
            b_{-,1} & b_{-,2} & \cdots & b_{-,n}
          \end{array}
        }
      \coloneqq
      \begin{pmatrix}
        a_{1,-} \cdot b_{-,1} & a_{1,-} \cdot b_{-,2} & \vdots & a_{1,-} \cdot b_{-,n} \\
        a_{2,-} \cdot b_{-,1} & a_{2,-} \cdot b_{-,2} & \vdots & a_{2,-} \cdot b_{-,n} \\
        \vdots                & \vdots                & \ddots & \vdots                \\
        a_{m,-} \cdot b_{-,1} & a_{m,-} \cdot b_{-,2} & \cdots & a_{m,-} \cdot b_{-,n}
      \end{pmatrix}.
    \end{equation*}

    If \( n \) and \( m \) are equal, \( R^{n \times n} \) becomes an \( R \)-\hyperref[def:algebra_over_semiring]{algebra} under \hyperref[def:matrix_algebra/matrix_multiplication]{matrix multiplication} with multiplicative identity the \term{identity matrix} of order \( n \)
    \begin{equation}\label{eq:def:matrix_algebra/matrix_multiplication/identity}
      \op{diag}(\underbrace{ 1, \cdots, 1 }_{n \T*{ones}})
      =
      \begin{pmatrix}
        1       & 0      & \cdots & 0      \\
        0       & 1      & \cdots & 0      \\
        \vdots  & \vdots & \ddots & \vdots \\
        0       & \vdots & \cdots & 1
      \end{pmatrix}.
    \end{equation}
  \end{thmenum}
\end{proposition}
\begin{proof}
  The semimodule structure is inherited by the \hyperref[thm:semiring_is_semimodule]{semimodule structure} on \( R \). We will show that, if \( n = n \), matrix multiplication is associative and bilinear. Fix matrices
  \begin{equation*}
    \begin{aligned}
      A = \seq{ a_{i,j} }_{i,j=1}^{m,k} && B = \seq{ b_{i,j} }_{i,j=1}^{k,l} && C = \seq{ c_{i,j} }_{i,j=1}^{l,n}.
    \end{aligned}
  \end{equation*}

  \SubProofOf[def:magma/associative]{associativity} The \( (i, j) \)-th entry in \( D \coloneqq (AB)C \) is
  \begin{equation*}
    d_{i,j} = \sum_{s=1}^n \parens*{ \sum_{r=1}^n a_{i,r} \cdot b_{r,s} } \cdot c_{s,j}.
  \end{equation*}

  Due to distributivity,
  \begin{equation*}
    d_{i,j}
    =
    \sum_{s=1}^n \sum_{r=1}^n a_{i,r} \cdot b_{r,s} \cdot c_{s,j}
    =
    \sum_{r=1}^n a_{i,r} \cdot \parens*{ \sum_{s=1}^n b_{r,s} \cdot c_{s,j} },
  \end{equation*}
  which is the \( (i, j) \)-th entry in \( A(BC) \).

  Therefore, \( (AB)C = A(BC) \).

  \SubProofOf[def:semimodule/homomorphism/additive]{additivity} Again due to distributivity,
  \begin{equation*}
    \sum_{r=1}^n \parens*{ a_{i,r} + b_{i,r} } \cdot c_{r,j}
    =
    \sum_{r=1}^n a_{i,r} \cdot c_{r,j} + \sum_{r=1}^n b_{i,r} \cdot c_{r,j}.
  \end{equation*}

  Therefore, \( (A + B)C = AC + BC \). The proof that \( A(B + C) = AB + AC \) is analogous.

  \SubProofOf[def:semimodule/homomorphism/homogeneity]{homogeneity} Again due to distributivity,
  \begin{equation*}
    t \cdot \sum_{r=1}^n a_{i,r} \cdot b_{r,j}
    =
    \sum_{r=1}^n (t \cdot a_{i,r}) \cdot b_{r,j}
    =
    \sum_{r=1}^n a_{i,r} \cdot (t \cdot b_{r,j}).
  \end{equation*}

  Therefore, \( t(AB) = (tA)B = A(tB) \).
\end{proof}

\begin{example}\label{ex:matrix_multiplication_is_noncommutative}
  For \( n > 1 \), the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) is a noncommutative ring. Consider the following example:
  \begin{align*}
    \begin{pmatrix}
      0 & 0 \\
      0 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix}
    &=
    \begin{pmatrix}
      0 & 0 \\
      1 & 0
    \end{pmatrix},
    \\
    \begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix}
    \begin{pmatrix}
      0 & 0 \\
      0 & 1
    \end{pmatrix}
    &=
    \begin{pmatrix}
      0 & 0 \\
      0 & 0
    \end{pmatrix}.
  \end{align*}
\end{example}

\begin{remark}\label{rem:matrices_as_functions}
  Let \( R \) be a \hyperref[def:ring/commutative]{commutative ring} and let \( e_1, \ldots, e_n \) be the \hyperref[def:standard_basis]{standard basis} of \( R \). The \hyperref[def:basis_decomposition]{coordinate projections} \( \pi_{e_1}, \ldots, \pi_{e_n} \) allow us to identify \( R^n \) with the module \( R^{n \times 1} \) of \hyperref[def:array/column_vector]{column vectors} by regarding the vector \( x \) from \( R^n \) as the column vector
  \begin{equation*}
    \begin{pmatrix}
      \pi_{e_1}(x) \\
      \vdots \\
      \pi_{e_n}(x)
    \end{pmatrix}.
  \end{equation*}

  Under this identification, the columns on the identity matrix \eqref{eq:def:matrix_algebra/matrix_multiplication/identity} are precisely the column vectors of the \hyperref[def:standard_basis]{standard basis}.

  Let \( A = \seq{ a_{ij} }_{i,j=1}^{m,n} \) be an \( m \times n \) matrix over \( R \). If we regard \( R^n \) as a set of column vectors, then \hyperref[thm:matrix_algebra/matrix_multiplication]{matrix multiplication} allows us to regard \( A \) as the function \( x \mapsto Ax \), which maps column vectors from \( R^n \) to column vectors in \( R^m \).

  This justifies using juxtaposition for application of linear maps, e.g. \( Lx \) rather than \( L(x) \).

  Conversely, let \( e_1, \ldots, e_n \) be the standard basis of \( R^n \) and \( f_1, \ldots, f_m \) --- of \( R^m \). The linear map \( L: R^n \to R^m \) corresponds to the following matrix:
  \begin{equation*}
    \begin{pmatrix}
      \pi_{e_1}(L f_1) & \cdots & \pi_{e_1}(L f_1) \\
      \vdots           & \ddots & \vdots       \\
      \pi_{e_n}(L f_m) & \cdots & \pi_{e_n}(L f_m)
    \end{pmatrix}.
  \end{equation*}
\end{remark}

\begin{proposition}\label{thm:matrix_and_linear_function_algebras}
  For a \hyperref[def:ring/commutative]{commutative ring} \( R \), the \hyperref[def:matrix_algebra]{matrix algebra} \( R^{m \times n} \) is \hyperref[def:algebra_over_semiring/homomorphism]{isomorphic} to the \hyperref[thm:functions_over_algebra]{linear function algebra} \( \hom(R^n, R^m) \)\footnote{Note that the maps are from \( R^n \) to \( R^m \) and not vice versa}.
\end{proposition}
\begin{proof}
  Follows from our discussion in \fullref{rem:matrices_as_functions} due to linearity.
\end{proof}

\begin{remark}\label{rem:double_index_maps}
  We want to be able to map single indices to double indices and vice versa, for example for the purpose of \fullref{thm:matrix_spaces_are_free_modules}. As an example, we want to be able to \enquote{linearize} an \( m \times n \) matrix such as the \( 2 \times 3 \) matrix
  \begin{equation}\label{eq:rem:double_index_maps/example/matrix}
    \begin{pmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6
    \end{pmatrix}
  \end{equation}
  into the tuple
  \begin{equation}\label{eq:rem:double_index_maps/example/row_major}
    (1, 2, 3, 4, 5, 6)
  \end{equation}
  and vice versa. This is called \term{row-major order} of the elements of a matrix. The \term{column-major order} would instead be
  \begin{equation}\label{eq:rem:double_index_maps/example/column_major}
    (1, 4, 2, 5, 3, 6).
  \end{equation}

  Let \( m \) and \( n \) be \hyperref[def:integer_signum]{positive integers}. We will explicitly define functions for linearizing a matrix like \eqref{eq:rem:double_index_maps/example/matrix} into its row-major order \eqref{eq:rem:double_index_maps/example/row_major}. Consider the sets
  \begin{align*}
    S &\coloneqq \overbrace{ \set{ 1, \ldots, mn - 1, mn } }^{\T{single indices}}
    \\
    D &\coloneqq \underbrace{ \set{ 1, \ldots, m } \times \set{ 1, \ldots, n } }_{\T{double indices}}
  \end{align*}
  and the mutually inverse operations
  \begin{align}
    &\begin{aligned}\label{eq:rem:double_index_maps/sharp}
      &\sharp: S \to D \\
      &\sharp(k) \coloneqq \parens[\Big]{ \quot(k - 1, m) + 1, \rem(k - 1, m) + 1 } \\
    \end{aligned}
    \\[0.5\baselineskip]
    &\begin{aligned}\label{eq:rem:double_index_maps/flat}
      &\flat: D \to S \\
      &\flat(i, j) \coloneqq (i - 1) \cdot m + (j - 1) + 1.
    \end{aligned}
  \end{align}

  The operation \( \sharp \) encodes the matrix \eqref{eq:rem:double_index_maps/example/matrix} into its row-major order \eqref{eq:rem:double_index_maps/example/row_major} and \( \flat \) does the opposite. Both operations are trivial except for the shifting needed in to allow us to use \hyperref[def:euclidean_domain]{remainders and quotients}.

  We can easily verify that \( \sharp \) is a \hyperref[def:morphism_invertibility/left_invertible]{left inverse} of \( \flat \) (note that \( j < m \)):
  \begin{align*}
    \sharp(\flat(i, j))
    &=
    \sharp\parens[\Big]{ (i - 1) \cdot m + (j - 1) + 1 }
    = \\ &=
    \parens[\Big]{ \quot(\cdots, m) + 1, \rem(\cdots, m) + 1 }
    = \\ &=
    \parens[\Big]{ (i - 1) + 1, (j - 1) + 1 }
    = \\ &=
    (i, j).
  \end{align*}

  We can just as easily verify that \( \flat \) is a \hyperref[def:morphism_invertibility/right_invertible]{right inverse} of \( \sharp \):
  \begin{align*}
    \flat(\sharp(k))
    &=
    \flat\parens[\Big]{ \quot(k, m) + 1, \rem(k, m) + 1 }
    = \\ &=
    \quot(k, m) \cdot m + \rem(k, m)
    = \\ &=
    k.
  \end{align*}

  Hence, \( \sharp \) is fully invertible with inverse \( \flat \). By \fullref{thm:function_invertibility_categorical/fully_invertible}, it is bijective.
\end{remark}

\begin{proposition}\label{thm:matrix_spaces_are_free_modules}
  The \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{m \times n} \) is isomorphic as a \hyperref[def:semimodule]{semimodule} to \( R^{mn} \).
\end{proposition}
\begin{proof}
  \Fullref{rem:double_index_maps} gives us a semimodule isomorphism between \( m \times n \) matrices and \( mn \)-dimensional column vectors when extended to linear maps via \fullref{thm:free_semimodule_universal_property}.
\end{proof}

\begin{definition}\label{def:matrix_determinant}\mcite[215]{Knapp2016BasicAlgebra}
  The \term{determinant} for the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) over the \hyperref[def:semiring/commutative]{commutative semiring} \( R \) is the function
  \begin{equation}\label{eq:def:matrix_determinant}
    \begin{aligned}
      &\det: R^{n \times n} \to R \\
      &\det(\seq{ a_{i,j} }_{i,j=1}^n) \coloneqq \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n a_{i,\sigma(i)},
    \end{aligned}
  \end{equation}
  where \( S_n \) is the \hyperref[def:symmetric_group]{symmetric group} and \( \sgn \) is the \hyperref[def:permutation_parity]{sign} of the permutation \( \sigma \).

  See the proof of \fullref{thm:determinant_on_columns} for a justification of the definition.
\end{definition}

\begin{definition}\label{def:symmetric_function}\mimprovised
  Given a function \( f: X^n \to Y \), where \( X \) and \( Y \) are \hyperref[def:set]{plain sets}, we say that \( f \) is \term{symmetric} if, for any \hyperref[def:symmetric_group/permutation]{permutation} \( \sigma \in S_n \), we have
  \begin{equation*}
    f(x_1, \ldots, x_n) = f(x_{\sigma(1)}, \ldots, x_{\sigma(n)}).
  \end{equation*}

  A permutation can be decomposed into \hyperref[def:symmetric_group/transposition]{transpositions} due to \fullref{thm:permutation_decomposition}. Hence, the above condition reduces to the simpler condition of \( f \) being invariant with respect to swapping any two arguments. That is,
  \begin{equation*}
    f(\ldots, x_{i-1}, \fbox{\( x_i \)}, x_{i+1}, \cdots, x_{j-1}, \fbox{\( x_j \)}, x_{j+1}, \ldots)
    =
    f(\ldots, x_{i-1}, \fbox{\( x_j \)}, x_{i+1}, \cdots, x_{j-1}, \fbox{\( x_i \)}, x_{j+1}, \ldots).
  \end{equation*}

  In the case where \( n = 2 \), this reduces to the simple condition
  \begin{equation*}
    f(x, y) = f(y, x).
  \end{equation*}

  Symmetric functions should not be confused with symmetric binary relations defined in \fullref{def:binary_relation/symmetric}.
\end{definition}

\begin{definition}\label{def:antisymmetric_function}\mimprovised
  Given a function \( f: X^n \to Y \), where \( X \) is a \hyperref[def:set]{plain set} and \( Y \) is an \hyperref[rem:additive_magma]{additive group}, we say that \( f \) is \term{antisymmetric} if, for any \hyperref[def:symmetric_group/permutation]{permutation} \( \sigma \in S_n \), we have
  \begin{equation*}
    f(x_1, \ldots, x_n) = \sgn(\sigma) \cdot f(x_{\sigma(1)}, \ldots, x_{\sigma(n)}).
  \end{equation*}

  A permutation can be decomposed into \hyperref[def:symmetric_group/transposition]{transpositions} due to \fullref{thm:permutation_decomposition}. Hence, the above condition reduces to the simpler condition of \( f \) changing sign when swapping any two arguments. That is,
  \begin{equation*}
    f(\ldots, x_{i-1}, \fbox{\( x_i \)}, x_{i+1}, \cdots, x_{j-1}, \fbox{\( x_j \)}, x_{j+1}, \ldots)
    =
    -f(\ldots, x_{i-1}, \fbox{\( x_j \)}, x_{i+1}, \cdots, x_{j-1}, \fbox{\( x_i \)}, x_{j+1}, \ldots).
  \end{equation*}

  In the case where \( n = 2 \), this reduces to the simple condition
  \begin{equation*}
    f(x, y) = -f(y, x).
  \end{equation*}

  Antisymmetric functions should not be confused with antisymmetric binary relations defined in \fullref{def:binary_relation/antisymmetric}.
\end{definition}

\begin{definition}\label{def:alternating_function}\mimprovised
  Given a commutative ring \( R \), and \( R \)-module \( M \) and a \hyperref[def:multilinear_function]{multilinear function} \( f: M \to R \), we say that \( f \) is \term{alternating} if, \( x_i = x_j \) implies that
  \begin{equation*}
    f(x_1, \ldots, x_i, \ldots, x_j, \ldots x_n) = 0.
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:alternating_multilinear_is_antisymmetric}
  If a \hyperref[def:multilinear_function]{multilinear map} is \hyperref[def:alternating_function]{alternating}, it is \hyperref[def:antisymmetric_function]{antisymmetric}. The converse holds if \( 2 \) is a unit.
\end{proposition}
\begin{proof}
  \SufficiencySubProof If \( f \) is an alternating multilinear map, then
  \begin{equation*}
    0
    =
    f(\cdots, x_i + x_j, \cdots, x_i + x_j, \cdots)
    =
    f(\cdots, x_i, \cdots, x_j, \cdots)
    +
    f(\cdots, x_j, \cdots, x_i, \cdots).
  \end{equation*}

  Therefore,
  \begin{equation*}
    f(\cdots, x_i, \cdots, x_j, \cdots)
    =
    -f(\cdots, x_j, \cdots, x_i, \cdots).
  \end{equation*}

  \NecessitySubProof If \( f \) is an antisymmetric multilinear map, then
  \begin{equation*}
    0
    =
    f(\cdots, x_i + x_i, x_i, \cdots)
    =
    2 f(\cdots, x_i, x_i, \cdots).
  \end{equation*}

  If \( 2 \) is a unit, this implies
  \begin{equation*}
    f(\cdots, x_i, x_i, \cdots) = 0.
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:determinant_on_columns}
  In the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) over the commutative ring \( R \), the determinant function \( \det: R^{n \times n} \to R \) can be regarded as a function that maps \( n \) column vectors from \( R^n \) to \( R \). That is,
  \begin{equation}\label{eq:thm:determinant_on_columns}
    \det(x_1, \cdots, x_n) = \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n \pi_{\sigma(i)} (x_i).
  \end{equation}

  The determinant is an \hyperref[def:alternating_function]{alternating} \hyperref[def:multilinear_function]{multilinear function} on columns. Furthermore, it is the unique alternating multilinear function \( f(x_1, \ldots, x_n) \) such that \( f(e_1, \ldots, e_n) = 1 \), where \( e_1, \ldots, e_n \) are vectors of the \hyperref[def:standard_basis]{standard basis} in \( R^n \).
\end{proposition}
\begin{proof}
  Let \( \pi_1, \ldots, \pi_n \) be the \hyperref[def:basis_decomposition]{projection functionals} corresponding to the \hyperref[def:standard_basis]{standard basis} \( e_1, \ldots, e_n \).

  \SubProof{Proof of multilinearity} Due to linearity of the coordinate projection functionals \( \pi_i \) and due to distributivity in \( R \), for every \( j \) we have
  \begin{align*}
    &\phantom{{}={}}
    \det(\cdots, x_{j-1}, ty + rz, x_{j+1}, \cdots)
    = \\ &=
    \sum_{\sigma \in S_n} \sgn(\sigma) \cdot \pi_{\sigma(j)} (ty + rz) \prod_{i \neq j} \pi_{\sigma(i)} (x_i)
    = \\ &=
    t \sum_{\sigma \in S_n} \sgn(\sigma) \cdot \pi_{\sigma(j)} (y) \prod_{i \neq j} \pi_{\sigma(i)} (x_i) + r \sum_{\sigma \in S_n} \sgn(\sigma) \cdot \pi_{\sigma(j)} (z) \prod_{i \neq j} \pi_{\sigma(i)} (x_i)
    = \\ &=
    t \cdot \det(\cdots, y, \cdots) + r \cdot \det(\cdots, z, \cdots).
  \end{align*}

  \SubProof{Proof of alternation} If \( x_i = x_j \), then for every even (resp. odd) permutation \( \sigma \), the permutation \( \cycle{i, j} \bincirc \sigma \) is odd (resp. even), and hence they cancel out in the sum \eqref{eq:thm:determinant_on_columns}. This holds for every permutation, hence it remains for the determinant to be zero.

  \SubProof{Proof of \( \det(I_n) = 1 \)} Note that
  \begin{equation*}
    \prod_{i=1}^n \pi_i (e_{\sigma(i)}) \neq 0
  \end{equation*}
  if and only if \( i = \sigma(i) \) for every \( i = 1, \ldots, n \). This only holds for the identity permutation, hence
  \begin{equation*}
    \det(e_1, \ldots, e_n) = \prod_{i=1}^n \pi_i(e_i) = \prod_{i=1}^n 1 = 1.
  \end{equation*}

  \SubProof{Proof of uniqueness} Suppose that \( f(x_1, \ldots, x_n) \) is an alternating multilinear function such that \( f(e_1, \ldots, e_n) = 1 \).

  For an arbitrary column vector \( x_j \) in \( R^n \), we have
  \begin{equation*}
    x_j = \sum_{i=1}^n \pi_i(x_j) \cdot e_i.
  \end{equation*}

  Then
  \begin{align*}
    f(x_1, \ldots, x_n)
    &=
    f\parens*{ \sum_{i_1=1}^n \pi_{i_1}(x_1) \cdot e_{i_1}, \ldots, \sum_{i_n=1}^n \pi_{i_n}(x_n) \cdot e_{i_n} }
    = \\ &=
    \sum_{i_1=1}^n \pi_{i_1}(x_1) \cdots \sum_{i_n=1}^n \pi_{i_n}(x_n) f(e_{i_1}, \ldots, e_{i_n})
    = \\ &=
    \sum_{\sigma \in S_n} \pi_{\sigma(i)}(x_i) f(e_{\sigma(1)}, \ldots, e_{\sigma(n)}).
  \end{align*}

  The last step is valid because \( f \) is \hyperref[def:alternating_function]{alternating} and thus \( f(e_{i_1}, \cdots, e_{i_n}) \) is zero when not all of \( i_1, \ldots, i_n \) are distinct, and they are necessarily distinct if the indices are given by a permutation from \( S_n \).

  Finally, since \( f \) is \hyperref[def:antisymmetric_function]{antisymmetric} due to \fullref{thm:alternating_multilinear_is_antisymmetric},
  \begin{equation*}
    f(e_{\sigma(1)}, \ldots, e_{\sigma(n)}) = \sgn(\sigma) \underbrace{f(e_1, \ldots, e_n)}_{1 \T*{by assumption}} = \sgn(\sigma).
  \end{equation*}

  Therefore,
  \begin{equation*}
    f(x_1, \ldots, x_n) = \det(x_1, \ldots, x_n).
  \end{equation*}
\end{proof}

\begin{definition}\label{def:transpose_matrix}
  The \term{transpose matrix} of
  \begin{equation*}
    A = \begin{pmatrix}
      a_{1,1} & a_{1,2} & \cdots & a_{1,n} \\
      a_{2,1} & a_{2,2} & \cdots & a_{2,n} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      a_{m,1} & a_{m,2} & \cdots & a_{m,n}
    \end{pmatrix}
  \end{equation*}
  is defined as
  \begin{equation*}
    A^T = \begin{pmatrix}
      a_{1,1} & a_{1,2} & \cdots & a_{n,1} \\
      a_{2,1} & a_{2,2} & \cdots & a_{n,2} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      a_{1,m} & a_{2,m} & \cdots & a_{n,m}
    \end{pmatrix}.
  \end{equation*}

  A matrix that is equal to its transpose is called \term{symmetric}.
\end{definition}

\begin{proposition}\label{thm:def:matrix_determinant}
  In the \hyperref[thm:matrix_algebra]{matrix algebra} \( R^{n \times n} \) over the commutative ring \( R \), the \hyperref[def:matrix_determinant]{determinant} as function on matrices has the following basic properties:
  \begin{thmenum}
    \thmitem{thm:def:matrix_determinant/transpose} \( \det(A^T) = \det(A) \).
    \thmitem{thm:def:matrix_determinant/homogeneous} \( \det(tA) = t^n \cdot \det(A) \).
    \thmitem{thm:def:matrix_determinant/homomorphism}\mcite[sec. 6.7]{Тыртышников2004Лекции} \( \det(AB) = \det(A) \cdot \det(B) \).

    That is, \( \det: R^{n \times n} \to R \) is a \hyperref[def:monoid/homomorphism]{monoid homomorphism} from the \hyperref[def:semiring]{multiplicative monoid} of the ring \( R^{n \times n} \) to the multiplicative monoid of \( R \).
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:def:matrix_determinant/transpose} The inverse of any permutation in \( S_n \) is also a permutation in \( S_n \), hence
  \begin{equation*}
    \det(A^T)
    =
    \sum_{\sigma \in S_n} \sgn(\sigma) \prod_{i=1}^n a_{\sigma(i),i}
    =
    \sum_{\sigma \in S_n} \sgn(\sigma^{-1}) \prod_{i=1}^n a_{i,\sigma^{-1}(i)}
    =
    \det(A).
  \end{equation*}

  \SubProofOf{thm:def:matrix_determinant/homogeneous} Follows from \fullref{thm:determinant_on_columns}.

  \SubProofOf{thm:def:matrix_determinant/homomorphism} The \( j \)-th column of the product \( C = AB \) is
  \begin{equation*}
    c_{\anon*,j}
    =
    \sum_{i=1}^n b_{i,j} a_{\anon*,i}
    =
    \begin{pmatrix}
      \sum_{i=1}^n a_{1,i} b_{i,j} \\
      \vdots \\
      \sum_{i=1}^n a_{n,i} b_{i,j} \\
    \end{pmatrix}.
  \end{equation*}

  Since the determinant is a multilinear function on columns,
  \begin{balign*}
    \det(c_{\anon*,1}, \cdots, c_{\anon*,n})
    &=
    \det\parens*{ \sum_{i_1=1}^n b_{i_1,1} a_{\anon*,i_1}, \cdots, \sum_{i_n=1}^n b_{i_n,n} a_{\anon*,i_n} }
    = \\ &=
    \sum_{i_1=1}^n b_{i_1,1} \det\parens*{ a_{\anon*,i_1}, \cdots, \sum_{i_n=1}^n b_{i_n,n} a_{\anon*,i_n} }
    = \\ &=
    \sum_{i_1=1}^n b_{i_1,1} \cdots \sum_{i_n=1}^n b_{i_n,n} \det(a_{\anon*,i_1}, \cdots, a_{\anon*,i_n})
    = \\ &=
    \sum_{i_1=1}^n \cdots \sum_{i_n=1}^n b_{i_1,1} \cdots b_{i_n,n} \det(a_{\anon*,i_1}, \cdots, a_{\anon*,i_n}).
  \end{balign*}

  Since the determinant is \hyperref[def:alternating_function]{alternating} on columns, \( \det(a_{\anon*,i_1}, \cdots, a_{\anon*,i_n}) \) is zero when not all of \( i_1, \ldots, i_n \) are distinct. They are necessarily distinct if the indices are given by a permutation from \( S_n \). Therefore,
  \begin{balign*}
    \det(AB)
    &=
    \sum_{\sigma \in S_n} \prod_{i=1}^n a_{i,\sigma(i)} \cdot \sigma(a_{\anon*, \sigma(1)}, \ldots, a_{\anon*, \sigma(n)})
    = \\ &=
    \sum_{\sigma \in S_n} \prod_{i=1}^n a_{i,\sigma(i)} \cdot \sgn(\sigma) \cdot \sigma(a_{\anon*, 1}, \ldots, a_{\anon*, n})
    = \\ &=
    \det(B) \det(A).
  \end{balign*}
\end{proof}

\begin{definition}\label{def:triangular_matrix}
  An \term{upper triangular matrix} over a semiring is one with zeros below its \hyperref[def:matrix_diagonal]{main diagonal}. More precisely, \( U = \seq{ u_{i,j} }_{i,j=1}^{m,n} \) is an upper triangular matrix if \( u_{i,j} = 0 \) when \( i > j \).

  Similarly, a \term{lower triangular matrix} is one with zeros above its main diagonal.

  A matrix that is either upper or lower triangular is simply referred to as \enquote{triangular}.

  A matrix that is both upper and lower triangular is a \hyperref[def:matrix_diagonal]{diagonal matrix}.
\end{definition}

\begin{proposition}\label{thm:def:triangular_matrix}
  \hyperref[def:triangular_matrix]{Triangular matrices} have the following basic properties:
  \begin{thmenum}
    \thmitem{thm:def:triangular_matrix/product} The \hyperref[def:matrix_algebra/matrix_multiplication]{product} of upper (resp. lower) triangular matrices is upper (resp. lower).

    Consequently, the product of diagonal matrices is a diagonal matrix.

    \thmitem{thm:def:triangular_matrix/determinant} The \hyperref[def:matrix_determinant]{determinant} of an upper or lower triangular square matrix is the product of the elements on its \hyperref[def:matrix_diagonal]{main diagonal}.
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:def:triangular_matrix/product} Let \( A \) be an \( m \times k \) upper triangular matrix and \( B \) be a \( k \times n \) upper triangular matrix. The \( (i, j) \)-th element of \( C = AB \) is
  \begin{equation*}
    \sum_{l=1}^k a_{i,l} b_{l,j}.
  \end{equation*}

  Since \( A \) and \( B \) are upper triangular, we have \( b_{l,j} = 0 \) whenever \( l > j \) and \( a_{i,l} = 0 \) whenever \( l < i \). Thus, \( a_{i,l} b_{l,j} = 0 \) if either condition holds. If \( i > j \), then either \( l > j \) or \( l < j < i \), implying that \( a_{i,l} b_{l,j} = 0 \). Therefore, \( AB \) is also upper triangular.

  The proof for lower triangular matrices is analogous.

  \SubProofOf{thm:def:triangular_matrix/determinant} Let \( A \) be an \( n \times n \) upper triangular matrix. Let \( \sigma \in S_n \) be any permutation. Then \( a_{i,\sigma(i)} = 0 \) when \( i > \sigma(i) \). Hence, the only permutation for which the product \( \prod_{i=1}^n a_{i,\sigma(i)} \) is nonzero is the identity permutation. Therefore,
  \begin{equation*}
    \det(A) = \prod_{i=1}^n a_{i,i}.
  \end{equation*}
\end{proof}

\begin{definition}\label{def:inverse_matrix}
  Let \( A \) be an \( m \times n \) matrix over a semiring \( R \). We say that \( B \) is a \term{left inverse matrix} of \( A \) if \( BA \) is the identity matrix \( I_n \) and a \term{right inverse matrix} if \( AB \) is \( I_n \).

  These are precisely the left and right inverse linear maps in the correspondence described in \fullref{thm:matrix_and_linear_function_algebras}.

  If \( B \) is both a left and a right inverse of \( A \), we say that it is a \term{two-sided inverse} or simply \term{inverse}. An inverse matrix, if it exists, is unique. We denote this inverse of \( A \) by \( A^{-1} \).

  We say that \( A \) is \term{invertible} if an inverse exists, and \term{singular} otherwise.
\end{definition}
\begin{proof}
  The inverse is unique by \fullref{thm:monoid_inverse_unique}.
\end{proof}

\begin{definition}\label{def:elementary_matrix}
  We introduce the following three types of \hyperref[def:inverse_matrix]{invertible} \( n \times n \) matrices, collectively known as \term{elementary matrices}:
  \begin{thmenum}
    \thmitem{def:elementary_matrix/permutation} A \term{permutation matrix} is obtained by permuting the columns \( e_1, \ldots, e_n \) of the identity matrix \( I_n \). Every permutation \( \sigma \in S_n \) corresponds to exactly one permutation matrix, namely
    \begin{equation*}
      \parens*
      {
        \begin{array}{c|c|c}
          e_{\sigma(1)} & \cdots & e_{\sigma(n)}
        \end{array}
      }.
    \end{equation*}

    The inverse of a permutation matrix is the matrix corresponding to its inverse permutation.

    \thmitem{def:elementary_matrix/scaling} A \term{scaling matrix} is a \hyperref[def:matrix_diagonal]{diagonal matrix} with at most one entry that is a \hyperref[def:divisibility/unit]{unit} possible different from \( 1 \). That is,
    \begin{equation*}
      \begin{pmatrix}
        1      & \cdots & 0       & 0      & 0      & \cdots & 0      \\
        \vdots & \ddots &         & \vdots &        &        & \vdots \\
        0      &        & 1       & 0      &        &        & 0      \\
        0      & \cdots & 0       & a      & 0      & \cdots & 0      \\
        0      &        &         & 0      & 1      &        & 0      \\
        \vdots &        &         & \vdots &        & \ddots & \vdots \\
        0      & \cdots & 0       & 0      & 0      & \cdots & 1
      \end{pmatrix}
    \end{equation*}
    for some unit \( a \neq 0 \).

    The inverse of the above matrix is the same matrix with \( a \) replaced by its multiplicative inverse \( a^{-1} \).

    \thmitem{def:elementary_matrix/transvection} A \term{transvection matrix} is obtained by adding a single off-diagonal entry to the identity matrix. Such a matrix has the form
    \begin{equation*}
      \begin{pmatrix}
        1      & \cdots & 0       & \cdots & 0      & \cdots & 0      \\
        \vdots & \ddots &         &        &        &        & \vdots \\
        0      &        & 1       & 0      &        &        &        \\
        \vdots &        & \vdots  & 1      & 0      & \cdots & 0      \\
        0      & \cdots & a       & \cdots & 1      &        &        \\
        \vdots &        & \vdots  &        &        & \ddots & \vdots \\
        0      & \cdots & 0       & \cdots & 0      & \cdots & 1
      \end{pmatrix}.
    \end{equation*}

    The inverse of this matrix is the same matrix with \( a \) replaced by its additive inverse \( -a \).
  \end{thmenum}
\end{definition}

\begin{proposition}\label{thm:def:elementary_matrix}
  \hyperref[def:elementary_matrix]{Elementary matrices} have the following basic properties:
  \begin{thmenum}
    \thmitem{thm:def:elementary_matrix/permutation_product} The product of \hyperref[def:elementary_matrix/permutation]{permutation matrices} is a permutation matrix.

    \thmitem{thm:def:elementary_matrix/permutation_determinant} The \hyperref[def:matrix_determinant]{determinant} of a permutation matrix is the \hyperref[def:permutation_parity]{sign} of the \hyperref[def:symmetric_group/permutation]{permutation}.

    \thmitem{thm:def:elementary_matrix/transvection_product} The product of the \hyperref[def:elementary_matrix/transvection]{transvection matrices} \( A \) and \( B \) with nonzero off-diagonal entries \( a_{i_A,j_A} \) and \( b_{i_B,j_B} \), correspondingly, is an identity matrix with nonzero off-diagonal entries \( a_{i_A,j_A} \) and \( b_{i_B,j_B} \).
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:def:elementary_matrix/permutation_product} Trivial.

  \SubProofOf{thm:def:elementary_matrix/permutation_determinant} As discussed in the proof of \fullref{thm:determinant_on_columns}, the determinant of any permutation of the vectors of the standard basis is the sign of the permutation.

  \SubProofOf{thm:def:elementary_matrix/transvection_product} The \( (i, j) \)-th entry of the product \( C = AB \) is
  \begin{equation*}
    c_{i,j} = \sum_{k=1}^n a_{i,k} b_{k,j}.
  \end{equation*}

  \begin{itemize}
    \item If \( i = j \), \( c_{i,j} \) is clearly \( 1 \).
    \item If \( i = i_A \) and \( j = j_A \), then
    \begin{equation*}
      a_{i_A,k} b_{k,j_A} = \begin{cases}
        a_{i_A,j_A}, &i_A = j_A \\
        0,           &i_A \neq j_A
      \end{cases}.
    \end{equation*}

    Thus, \( c_{i_A,j_A} = a_{i_A,j_A} \).

    \item Analogously, \( c_{i_B,j_B} = b_{i_B,j_B} \).
    \item Otherwise, for \( k = 1, \ldots, n \), either \( a_{i,k} \) or \( b_{k,j} \) is zero, hence \( c_{i,j} \) also is.
  \end{itemize}
\end{proof}
