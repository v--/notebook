\subsection{Modules}\label{subsec:modules}

\begin{definition}\label{def:module}
  A \term{module} is a \hyperref[def:semimodule]{semimodule} over a \hyperref[def:ring]{ring} rather than a \hyperref[def:semiring]{semiring}.

  This makes the identity law \eqref{eq:def:semimodule/operation/scalar_multiplication_action/identity} redundant, but most metamathematical properties remain the same. The first-order theory is identical to the \hyperref[def:semimodule/theory]{theory of semimodules}.

  For a fixed ring \( R \), we denote the \hyperref[def:category_of_small_first_order_models]{category of \( \mscrU \)-small models} by \( \ucat{Mod}_R \).
\end{definition}

\begin{theorem}[Homomorphism theorem for modules]\label{thm:homomorphism_theorem_for_modules}
  Every \hyperref[def:module/homomorphism]{module homomorphism} \( \varphi: M \to N \) induces an isomorphism
  \begin{equation*}
    M / \ker \varphi \cong \img \varphi.
  \end{equation*}

  Compare this to \fullref{thm:homomorphism_theorem_for_groups} and \fullref{thm:homomorphism_theorem_for_rings}.
\end{theorem}
\begin{proof}
  \Fullref{thm:homomorphism_theorem_for_groups} defines an explicit additive group isomorphism \( \phi: M / \ker \varphi \to \img \varphi \). The module structure simply restricts \( \ker \varphi \) to be a submodule rather than an arbitrary subgroup of \( M \).
\end{proof}

\begin{proposition}\label{thm:abelian_group_is_module}
  We have an \hyperref[rem:category_similarity/isomorphism]{isomorphism of categories} \( \hyperref[def:abelian_group]{\cat{Ab}} \cong \hyperref[def:module]{\cat{Mod}_\BbbZ} \).

  More concretely, every abelian group \( G \) is a left semimodule over \( \BbbZ \) with scalar multiplication given by \hyperref[rem:additive_magma/multiplication]{recursively defined multiplication}
  \begin{equation}\label{eq:thm:abelian_group_is_module/operation}
    \begin{aligned}
      &\cdot: \BbbZ \times G \to G \\
      &n \cdot x \coloneqq \begin{cases}
        0_G,           &n = 0, \\
        n \cdot x + x, &n > 1, \\
        -(n \cdot x),  &n < 1.
      \end{cases}
    \end{aligned}
  \end{equation}

  Conversely, in every semimodule over \( \BbbZ \), scalar multiplication matches the recursively defined multiplication.

  Compare this result to \fullref{thm:commutative_monoid_is_semimodule}.
\end{proposition}
\begin{proof}
  Trivial refinement of \fullref{thm:commutative_monoid_is_semimodule}.
\end{proof}

\begin{definition}\label{def:linear_combination}
  Let \( M \) be a left \( R \)-module. Like \hyperref[def:polynomial]{polynomials}, we define linear combinations to be tuples \( (t_1, t_2, \ldots, t_n) \) of scalars from \( R \). Unlike with polynomials, we are not interested in defining operations on them, but rather defining the function
  \begin{equation}\label{def:linear_combination/function}
    (x_1, \ldots, x_n) \mapsto \sum_{k=1}^n t_k x_k.
  \end{equation}

  The scalars \( t_1, \ldots, t_n \) are called the \term{coefficients} of the linear combination. A linear combination is said to be \term{trivial} if all coefficients are equal to \( 0_R \).

  For convenience, given set of vectors \( x_1, \ldots, x_n \in M \), we also call the sum \( \sum_{k=1}^n t_k x_k \) a linear combination.

  In the special case where \( R \) is a superring of \( \BbbR \), we define the following special types of linear combinations:
  \begin{thmenum}
    \thmitem{def:linear_combination/affine} an \term{affine combination} if \( \sum_{k=1}^n t_k = 1 \).
    \thmitem{def:linear_combination/conic} a \term{conic combination} if all of the coefficients are nonnegative real numbers.
    \thmitem{def:linear_combination/convex} a \term{convex combination} if it is both affine and conic.
  \end{thmenum}
\end{definition}

\begin{definition}\label{def:module_linear_dependence}
  Let \( M \) be a left \( R \)-module and let \( A \subseteq M \). We say that the set \( A \) is \term{linearly independent} if for any linear \hyperref[def:linear_combination/function]{combination}, the equality
  \begin{equation*}
    \sum_{i=1}^n t_i x_k = 0_M
  \end{equation*}
  for \( x_1, \ldots, x_n \in A \) implies that the combination is trivial.

  We say that the vectors \( x_1, \ldots, x_n \) are linearly independent if the corresponding set \( \{ x_1, \ldots, x_n \} \) is linearly independent.

  We say that \( A \) is \term{linearly dependent} if it is not linearly independent.
\end{definition}

\begin{definition}\label{def:module_hamel_basis}
  A subset \( B \) of the left \( R \)-module \( M \) is called a \term{Hamel basis} or simply \term{basis} of \( M \) if \( B \) is a \hyperref[def:partially_ordered_set_extremal_points/maximal_and_minimal_element]{minimal} (with respect to set inclusion) linearly independent subset of \( M \).
\end{definition}

\begin{proposition}\label{thm:free_module_is_free_functor}
  The functor \( F: \cat{Set} \to \cat{Mod}_R \), defined pointwise in \fullref{def:free_semimodule}, is \hyperref[def:category_adjunction]{free}.
\end{proposition}

\begin{proposition}\label{thm:left_module_basis_decomposition}
  Let \( B \) be a basis of the free left \( R \)-module \( M \). Then each element \( u \) of \( M \) can be uniquely (up to a permutation) represented as a linear \hyperref[def:linear_combination]{combination} of elements of \( B \).
\end{proposition}
\begin{proof}
  Let
  \begin{equation*}
    u = \sum_{i=1}^n t_i v_i
  \end{equation*}
  and
  \begin{equation*}
    u = \sum_{j=1}^m s_i w_i
  \end{equation*}
  be two representations of \( u \) as a linear combination over \( B \).

  Define the function
  \begin{balign*}
     & t: M \to R                                \\
     & t(v) \coloneqq \begin{cases}
      t_i, & v = v_i,                          \\
      0,   & v \not\in \{ v_1, \ldots, v_n \}.
    \end{cases}
  \end{balign*}
  and analogously for \( s: M \to R \). Define the set
  \begin{equation*}
    B' \coloneqq \{ v_1, \ldots, v_n, w_1, \ldots, w_m \}.
  \end{equation*}

  Thus,
  \begin{equation*}
    u = \sum_{b \in B'} t(b) b = \sum_{b \in B'} s(b) b
  \end{equation*}
  and
  \begin{equation*}
    0 = u - u = \sum_{b \in B'} (t(b) - s(b)) b.
  \end{equation*}

  The set \( B' \) is linearly independent as a subset of the basis \( B \), hence only a trivial linear combination can be the zero vector. This gives us
  \begin{equation*}
    t(b) = s(b), b \in B'.
  \end{equation*}

  In particular, the two decompositions of \( u \) along \( B \) are identical up to a permutation.
\end{proof}

\begin{definition}\label{def:module_basis_projection}
  Let \( M \) be a left \( R \)-module and let \( B \) be a basis of \( M \). For each \( b \in B \), we define the \text{coordinate projection functional} \( \pi_b: M \to R \) that gives us the unique coefficient in the basis decomposition. Thus, for every \( x \in M \) we have
  \begin{equation*}
    x = \sum_{b \in B} \pi_b(x) b.
  \end{equation*}

  The sum is well-defined since only finitely many terms are nonzero.

  When the basis \( B \) is finite and ordered:
  \begin{equation*}
    B = \{ b_1, \ldots, b_n \},
  \end{equation*}
  we also write
  \begin{equation*}
    x = \sum_{i=1}^n x_k b_i.
  \end{equation*}
\end{definition}
\begin{proof}
  By \fullref{thm:left_module_basis_decomposition}, this decomposition is unique given a basis \( B \).
\end{proof}

\begin{proposition}\label{thm:left_module_basis_projections_are_linear}
  The basis projection \hyperref[def:module_basis_projection]{maps} are linear.
\end{proposition}
\begin{proof}
  \SubProofOf{def:semimodule/homomorphism/homogeneity} Let \( t \in R \) and \( x \in M \). We have the unique decompositions
  \begin{balign*}
    x  & = \sum_{b \in B} \pi_b(x) b,  \\
    tx & = \sum_{b \in B} \pi_b(tx) b.
  \end{balign*}

  Since both decompositions have only finitely many terms, their difference also has only finitely many nonzero terms. Thus,
  \begin{equation*}
    0
    =
    tx - tx
    =
    t \left( \sum_{b \in B} \pi_b(x) b \right) - \sum_{b \in B} \pi_b(tx) b
    =
    \sum_{b \in B} (t \pi_b(x) - \pi_b(tx)) b.
  \end{equation*}

  Since the vectors in \( B \) are linearly independent, no nontrivial linear combination can equal the zero vector. Thus, for all \( b \in B \),
  \begin{equation*}
    t \pi_b(x) = \pi_b(tx).
  \end{equation*}

  \SubProofOf{def:semimodule/homomorphism/additivity} Analogous.
\end{proof}

\begin{theorem}\label{thm:linear_map_iff_function_on_basis}
  Let \( M \) and \( N \) be left \( R \)-modules and let \( B \) be a basis of \( M \). Then there exists a bijection between the \hyperref[def:function]{functions} from \( B \) to \( N \) and the module \hyperref[def:semimodule/homomorphism]{homomorphisms} from \( M \) to \( N \), such that each linear map is an \hyperref[def:multi_valued_function/restriction]{extension} of the corresponding function.
\end{theorem}
\begin{proof}
  Let \( \varphi: M \to N \) be a homomorphism. Define the function
  \begin{balign*}
     & f: B \to N                 \\
     & f(b) \coloneqq \varphi(b).
  \end{balign*}

  Now define the linear map
  \begin{balign*}
     & \hat \varphi: M \to N                                   \\
     & \hat \varphi(x) \coloneqq \sum_{b \in B} \pi_b(x) f(b).
  \end{balign*}

  Since the projections \( \pi_b(x) \) are linear functions by \fullref{thm:left_module_basis_projections_are_linear} and since we only use the value of \( f \) on fixed vectors, it follows that \( \hat \varphi \) is also linear.

  It remains to show that \( \varphi = \hat \varphi \). For each \( x \in M \), by linearity of \( \varphi \) we have
  \begin{equation*}
    \hat \varphi(x)
    =
    \sum_{b \in B} \pi_b(x) f(b)
    =
    \sum_{b \in B} \pi_b(x) \varphi(b)
    =
    \varphi(x).
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:left_module_basis_cardinality}\mcite{ProofWiki:bases_of_free_module_have_same_cardinality}
  All bases in a free left module over a commutative unital ring have the same cardinality.
\end{proposition}

\begin{definition}\label{def:algebra_over_ring}\mcite[408]{Knapp2016BasicAlgebra}
  Let \( R \) be a commutative unital ring. We say that the left \( R \)-module \( A \) is an \( R \)-\term{algebra} if we define an additional bilinear \term{vector multiplication} operation
  \begin{equation*}
    \odot: A \times A \to A
  \end{equation*}
  such that for \( x, y \in M \) and \( t \in R \)
  \begin{equation*}
    t \cdot (x \odot y) = (t \cdot x) \odot y = x \odot (t \cdot y).
  \end{equation*}

  Both vector and scalar multiplication are usually denoted by juxtaposition.

  If \( \odot \) is associative, commutative, unital or invertible, we add this prefix to \( A \), e.g. A is a commutative algebra if \( \odot \) is commutative.
\end{definition}

\begin{proposition}\label{thm:functions_over_ring_form_algebra}
  Let \( X \) be an arbitrary nonempty set and \( R \) be a commutative unital ring. Define
  \begin{equation*}
    A \coloneqq \cat{Set}(X, R)
  \end{equation*}
  to be the set of all functions from \( X \) to \( R \). Then \( A \) is an \( R \)-algebra with the operations being defined pointwise, that is,
  \begin{balign*}
    [f + g](x)     & \coloneqq f(x) + g(x)     \\
    [f \odot g](x) & \coloneqq f(x) \circ g(x) \\
    [rf](x)        & \coloneqq r f(x)
  \end{balign*}

  We call the algebra \( A \) the \term{algebra of functions} from \( X \) to \( R \).

  If \( X \) itself has a ring structure, we consider the set of ring \hyperref[thm:ring_homomorphism_simpler_conditions]{homomorphisms}
  \begin{equation*}
    \cat{Ring}(X, R),
  \end{equation*}
  which is usually a strict subset of \( \cat{Set}(X, R) \). This set is usually denoted by \( \hom(X, R) \).

  If \( R \) is a module, but not necessarily a ring, then \( \cat{Set}(X, R) \) is a only module since we do not necessarily have multiplication. See \fullref{def:semimodule/homomorphism}.
\end{proposition}
