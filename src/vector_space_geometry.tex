\subsection{Vector space geometry}\label{subsec:vector_space_geometry}

We will denote by \( \BbbK \) either the field of \hyperref[def:real_numbers]{real numbers} or the \hyperref[def:real_numbers]{complex numbers}. We will mostly work over the real numbers, but some important definitions and theorems hold more generally. Unless otherwise noted, \( X \) is a vector space over \( \BbbK \).

\begin{remark}\label{rem:real_field_extensions}
  When speaking about vector spaces, we usually restrict ourselves to vector spaces over \( \BbbR \) or, at most, \( \BbbC \). This restriction may seem arbitrary, however important concepts like \hyperref[def:geometric_ray]{rays} or \hyperref[def:convex_hull]{convexity} requires the field to be an extension of \( \BbbR \), and it just so happens that, by \fullref{thm:fundamental_theorem_of_algebra} and \fullref{thm:no_finite_extensions_of_closed_fields}, the only nontrivial finite \hyperref[def:field/submodel]{field extension} of \( \BbbR \) is \( \BbbC \). It is technically possible to work with infinite field extensions, however in practice vector spaces over \( \BbbC \) are esoteric enough.

  Considering only \( \BbbR \) and \( \BbbC \) leads to certain concepts being defined for complex vector spaces and then real vector spaces become a special case. This is formalized via \hyperref[def:complexification]{complexification}. For example, inner products are defined in \fullref{def:inner_product_space} differently for real and complex vector spaces, however we can transition between them due to \fullref{thm:complexification_universal_property} and \fullref{thm:complexification_of_symmetric_bilinear_form}.
\end{remark}

\begin{definition}\label{def:geometric_shape}
  A \term{geometric shape} is an informal notion that refers to certain special subsets of a vector space, usually defined in a coordinate-independent manner. Shapes in two-dimensional spaces are called \term{figures} and shapes in three dimensions are called \term{surfaces}.

  When two geometric shapes intersect, we say that they are \term{incident}. This relates to \hyperref[def:hypergraph/incidence]{(hyper)graph incidence} via \hyperref[def:quiver_geometric_realization]{geometric realizations}.

  We are interested in the following:
  \begin{thmenum}
    \thmitem{def:geometric_shape/parametric} Parametric shape, defined in \fullref{def:parametric_shape}, are a topological construction.
    \thmitem{def:geometric_shape/algebraic} Affine varieties, defined in \fullref{def:affine_algebraic_set}, are an algebraic construction.
    \thmitem{def:geometric_shape/geometric} Manifolds, defined in \fullref{def:topological_manifold}, are a geometric construction.
  \end{thmenum}

  All the above have a concept of dimension. Shapes of dimension \( 1 \) are called \term{curves}.
\end{definition}

\begin{definition}\label{def:point}
  A \term{point} is a simple \hyperref[def:geometric_shape]{geometric shape} comprising a singleton subset of a topological space. We use the convention \fullref{rem:singleton_sets} and, unless the distinction is important, we do not distinguish between singleton sets and their only element --- see \fullref{def:simplex/point} for an example.

  Within vector spaces points are also called vectors, which is justified by \fullref{def:euclidean_plane_coordinate_system}.

  Scalar multiplication, when applied to points, is also called \term{dilation}.
\end{definition}

\begin{definition}\label{def:affine_hull}\mimprovised
  We say that the \hyperref[rem:linear_combinations]{linear combination}
  \begin{equation*}
    t_1 x_1 + \ldots + t_n x_n
  \end{equation*}
  with real coefficients is an \term{affine combination} if
  \begin{equation*}
    t_1 + \cdots + t_n = 1.
  \end{equation*}

  The \term{affine hull} of the set \( S \) is the set of all affine combinations of members of \( S \). This can be expressed succinctly by saying that, for any real number \( \lambda \) and any vectors \( x \) and \( y \), the hull must contain
  \begin{equation}\label{eq:def:affine_hull/combination}
    \lambda x + (1 - \lambda) y.
  \end{equation}

  The affine hull is a \hyperref[def:closure_operator]{closure operator}.

  Geometrically, the affine hull of \( S \) is the smallest translated vector subspace of \( X \). Unlike the linear span, the affine hull does not necessarily contain the zero vector because it is not itself a vector subspace, but a translation of one. See \cref{fig:def:convex_hull}.
\end{definition}
\begin{defproof}
  We will show that the affine hull operator \( A: \pow(X) \to \pow(X) \) is a closure operator.

  \SubProofOf[def:inflationary_function]{inflation} Obviously \( S \subseteq A(S) \).

  \SubProofOf[def:magma/idempotent]{idempotence} Consider the affine combination \( \lambda x + (1 - \lambda) y \) of vectors from \( A(S) \). Suppose that \( x = t_x u_x + (1 - t_x) v_x \) and \( y = t_y u_y + (1 - t_y) v_y \), where \( u_x \), \( u_y \), \( v_x \) and \( v_y \) are vectors from \( S \).

  Then
  \begin{equation*}
    \lambda x + (1 - \lambda) y
    =
    \lambda t_x u_x + \lambda (1 - t_x) v_x + (1 - \lambda) t_y u_y + (1 - \lambda) (1 - t_y) v_y.
  \end{equation*}

  This is an affine combination of members of \( S \). Therefore, \( A(A(S)) = A(S) \).

  \SubProofOf[eq:def:partially_ordered_set/homomorphism/nonstrict]{monotonicity} If \( S_1 \subseteq S_2 \), then \( A(S_2) \) contains the affine combinations of the members of \( S_1 \) in addition to others. Hence, \( A(S_1) \subseteq A(S_2) \).
\end{defproof}

\begin{definition}\label{def:affine_operator}
  We say that the function \( f: X \to Y \) between vector spaces over \( \BbbK \) is an \term{affine operator} if any of the following equivalent conditions hold:
  \begin{thmenum}
    \thmitem{def:affine_operator/translation} The map \( T(x) \coloneqq f(x) - f(0) \) is a linear operator.
    \thmitem{def:affine_operator/combination} \( f \) preserves \hyperref[def:affine_hull]{affine combinations}. That is, for any real number \( \lambda \),
    \begin{equation}\label{eq:def:affine_operator/combination}
      f\parens[\Big]{ \lambda x + (1 - \lambda) y } = \lambda f(x) + (1 - \lambda) f(y).
    \end{equation}
  \end{thmenum}
\end{definition}
\begin{defproof}
  \ImplicationSubProof{def:affine_operator/translation}{def:affine_operator/combination} Suppose that \( T \) is linear. Then for an arbitrary real number \( \lambda \),
  \begin{balign*}
    f(\lambda x + (1 - \lambda) y)
    &=
    T(\lambda x + (1 - \lambda) y) + f(0)
    = \\ &=
    \lambda T(x) + (1 - \lambda) T(y) + f(0)
    = \\ &=
    \lambda \parens[\Big]{ f(x) - f(0) } + (1 - \lambda) \parens[\Big]{ f(y) - f(0) } + f(0)
    = \\ &=
    \lambda f(x) + (1 - \lambda) f(y).
  \end{balign*}

  We can cancel \( f(0) \) to obtain \eqref{eq:def:affine_operator/combination}.

  \ImplicationSubProof{def:affine_operator/combination}{def:affine_operator/translation} Suppose that \eqref{eq:def:affine_operator/combination} holds. Then
  \begin{balign*}
    T(x + y)
    &=
    f(x + y) - f(0)
    = \\ &=
    f(1 \cdot x - 1 \cdot 0 + 1 \cdot y - 1 \cdot 0 + 1 \cdot 0) - f(0)
    = \\ &=
    \parens[\Big]{ f(x) - f(0) } + \parens[\Big]{ f(y) - f(0) } + f(0) - f(0)
    = \\ &=
    T(x) + T(y).
  \end{balign*}

  Also,
  \begin{equation*}
    T(\lambda x)
    =
    f(\lambda x + (1 - \lambda) 0) - f(0)
    =
    \lambda f(x) - \lambda f(0)
    =
    \lambda T(x).
  \end{equation*}

  This shows that \( T \) is a linear operator.
\end{defproof}

\begin{proposition}\label{thm:isometry_iff_affine_orthogonal_operator}\mcite[thm. 7.1]{Treil2017}
  The endofunction \( f: X \to X \) over the real inner product space \( X \) is an \hyperref[def:isometry]{isometry} if and only if it is an \hyperref[def:affine_operator]{affine operator} whose linear part \( T(x) \coloneqq f(x) - f(0) \) is \hyperref[def:unitary_operator]{orthogonal}.
\end{proposition}
\begin{proof}
  \SufficiencySubProof Suppose that \( f \) is an isometry.

  First note that
  \begin{equation*}
    \inprod { f(x) } { f(y) } = \inprod x y.
  \end{equation*}

  Indeed, we have
  \begin{equation*}
    \norm{ f(x) - f(y) } = \norm{ f(x) } + \norm{ f(y) } - 2 \inprod { f(x) } { f(y) }
  \end{equation*}
  and
  \begin{equation*}
    \norm{ x - y } = \norm x + \norm y - 2 \inprod x y.
  \end{equation*}

  Therefore,
  \begin{equation*}
    \inprod { f(x) } { f(y) }
    =
    \frac{ \norm{ f(x) - f(y) } - \norm{ f(x) } - \norm{ f(y) } } 2
    =
    \frac{ \norm{ x - y } - \norm{ x } - \norm{ y } } 2
    =
    \inprod x y.
  \end{equation*}

  Now we can show additivity of \( T \):
  \begin{balign*}
    &\phantom{{}={}}
    \norm{ T(x + y) - T(x) - T(y) }^2
    = \\ &=
    \norm{ f(x + y) - f(0) - f(x) + f(0) - f(y) + f(0) }^2
    = \\ &=
    \norm{ \parens{ f(x + y) - f(x) } - \parens{ f(y) - f(0) } }^2
    = \\ &=
    \norm{ f(x + y) - f(x) }^2 + \norm{ f(y) - f(0) }^2 - 2 \inprod{ f(x + y) - f(x) } { f(y) - f(0) }
    = \\ &=
    \norm{ x + y - x }^2 + \norm{y}^2 - 2 \inprod{x + y} y + 2 \inprod x y - 2 \inprod {x + y} 0 + 2 \inprod x 0
    = \\ &=
    2 \norm{y}^2 - 2 \inprod x y - 2 \norm{y}^2 + 2 \inprod x y.
  \end{balign*}

  The norm is zero, hence \( T(x + y) = T(x) + T(y) \).

  Similarly,
  \begin{balign*}
    &\phantom{{}={}}
    \norm{ T(\lambda x) - \lambda T(x) }^2
    = \\ &=
    \norm{ f(\lambda x) - f(0) - \lambda f(x) + \lambda f(0) }^2
    = \\ &=
    \norm{ f(\lambda x) - f(0) }^2 + \norm{ \lambda f(x) - \lambda f(0) }^2 - 2 \inprod{ f(\lambda x) - f(0) } { \lambda f(x) - \lambda f(0) }
    = \\ &=
    \norm{\lambda x}^2 + \lambda^2 \norm{x}^2 - 2 \lambda \inprod{ f(\lambda x) - f(0) } { f(x) - f(0) }
    = \\ &=
    2 \lambda^2 \norm{x}^2 - 2 \lambda^2 \inprod x x.
  \end{balign*}

  This norm is also zero, hence \( T(\lambda x) = \lambda T(x) \).

  Finally, we must show that \( T \) is a unitary operator:
  \begin{balign*}
    \inprod{ T(x) }{ T(y) }
    &=
    \inprod{ f(x) - f(0) }{ f(y) - f(0) }
    = \\ &=
    \inprod{ f(x) }{ f(y) } - \inprod{ f(x) }{ f(0) } - \inprod{ f(0) }{ f(y) } + \inprod{ f(0) }{ f(0) }
    = \\ &=
    \inprod x y.
  \end{balign*}

  \NecessitySubProof Suppose that \( f(x) = Tx + f_0 \) for some unitary operator \( T \) and some vector \( f_0 \). Then
  \begin{balign*}
    \norm{ f(x) - f(y) }^2
    &=
    \norm{ Tx - f_0 - Ty + f_0 }^2
    = \\ &=
    \norm{ T(x - y) }^2
    = \\ &=
    \inprod{ T(x - y) }{ T(x - y) }
    = \\ &=
    \inprod{ x - y }{ T^{-1} T(x - y) }
    = \\ &=
    \norm{ x - y }^2.
  \end{balign*}
\end{proof}

\begin{definition}\label{def:euclidean_space}\mimprovised
  An \term{Euclidean space} is a finite-dimensional real \hyperref[def:inner_product_space]{inner product space}, most commonly the \hyperref[def:sequence_space]{tuple space} \( \BbbR^n \) with the dot product \( (x, y) \mapsto x^T y \).
\end{definition}

\begin{remark}\label{rem:euclidean_space_etymology}
  The term \enquote{Euclidean space} may have different meanings depending on the context. For example, \cite[sec. 24.1]{Тыртышников2004Лекции} defines Euclidean spaces as possibly infinite-dimensional real inner product spaces, while \cite[2.19]{Rudin1987RealAndComplex} restricts them to tuple spaces with the dot product. \enquote{The} Euclidean space may refer to the three-dimensional generalization of the Euclidean plane discussed in \fullref{def:euclidean_plane}.
\end{remark}

\begin{lemma}\label{thm:dot_product_and_outer_product}
  For vectors \( x \), \( y \) and \( z \) in \( \BbbK^n \), we have
  \begin{equation*}
    x^T y z = y z^T x.
  \end{equation*}
\end{lemma}
\begin{proof}
  The \( k \)-th coordinate of \( x^T y z \) is \( (x^T y) z_k \).

  The \( k \)-th coordinate of \( y z^T x \) is
  \begin{equation*}
    \sum_{i=1}^n (y_i z_k) x_i
    =
    z_k \sum_{i=1}^n y_i x_i
    =
    (x^T y) z_k.
  \end{equation*}
\end{proof}

\begin{definition}\label{def:rigid_motion}\mimprovised
  A \term{rigid motion} is an \hyperref[def:isometry]{isometry} in an \hyperref[def:euclidean_space]{Euclidean space}. \Fullref{thm:isometry_iff_affine_orthogonal_operator} provides an equivalent characterization: a rigid motion is an affine operator whose linear part is \hyperref[def:unitary_operator]{orthogonal}. Thus, given a rigid motion \( f: X \to X \), there exists an orthogonal operator \( T: X \to X \) and a vector \( d \) such that
  \begin{equation*}
    f(x) = Tx + d.
  \end{equation*}

  The following are common rigid motions:
  \begin{thmenum}
    \thmitem{def:rigid_motion/translation} The \term{translation} along the \term{direction} \( d \) is
    \begin{equation*}
      f(x) = x + d.
    \end{equation*}

    \begin{figure}[!ht]
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__translation__2d.pdf}
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__translation__3d.pdf}
      \hfill
      \hfill
      \caption{Translation of the unit square in \( \BbbR^2 \) and unit cube in \( \BbbR^3 \).}\label{fig:def/rigid_motion/translation}
    \end{figure}

    The term \enquote{translation} generalizes to an arbitrary \hyperref[def:magma]{magma} via
    \begin{equation*}
      f(x) \coloneqq v \cdot x.
    \end{equation*}

    A \term{homothety} is a translation preceded by a \hyperref[def:point]{dilation}. It is no longer a rigid motion in general.

    \thmitem{def:rigid_motion/rotation} A \term{rotation} is an \hyperref[def:orthogonal_operator]{orthogonal} operator with \hyperref[def:matrix_determinant]{determinant} \( 1 \).

    \begin{figure}[!ht]
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__rotation__2d.pdf}
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__rotation__3d.pdf}
      \hfill
      \hfill
      \caption{Rotation of the unit square in \( \BbbR^2 \) and unit cube in \( \BbbR^3 \).}\label{fig:def/rigid_motion/rotation}
    \end{figure}

    \thmitem{def:rigid_motion/reflection} The \term{Householder reflection} with \term{normal vector} \( v \) is, assuming \( \norm v = 1 \),
    \begin{equation*}
      f(x) \coloneqq x - (2 \inprod x v) v.
    \end{equation*}

    In \( \BbbR^n \) this can be expressed in matrix form as
    \begin{equation*}
      f(x)
      =
      x - (2 x^T v) v
      \reloset {\ref{thm:dot_product_and_outer_product}} =
      x - 2 v v^T x
      =
      (I_n - 2 v v^T) x.
    \end{equation*}

    \begin{figure}[!ht]
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__reflection__2d.pdf}
      \hfill
      \includegraphics[align=c]{output/def__rigid_motion__reflection__3d.pdf}
      \hfill
      \hfill
      \caption{Reflection of the unit square in \( \BbbR^2 \) and unit cube in \( \BbbR^3 \).}\label{fig:def/rigid_motion/reflection}
    \end{figure}
  \end{thmenum}
\end{definition}

\begin{definition}\label{def:zero_locus}\mimprovised
  The \term{zero locus} or \term{set of zeros} of a function \( f: S \to X \) from some set \( S \) into a vector space \( X \) (or, more generally, a monoid \( X \)) is the \hyperref[thm:def:function/preimage]{preimage}
  \begin{equation*}
    f^{-1}(0_X) = \set{ x \in X \given f(x) = 0_X }.
  \end{equation*}
\end{definition}

\begin{definition}\label{def:geometric_line}\mimprovised
  \term{Lines} are particularly important and simple \hyperref[def:geometric_shape]{curves}. Let \( X \) be a vector space over \( \BbbK \).

  \begin{thmenum}
    \thmitem{def:geometric_line/subspace} A line is a \hyperref[def:rigid_motion/translation]{translation} of a vector subspace of \( X \). That is, a line is a set \( L \) in \( X \) such that, for some point \( v_0 \), the set
    \begin{equation*}
      L - v_0 = \set{ v - v_0 \given v \in L }
    \end{equation*}
    is a subspace of \( X \).

    \thmitem{def:geometric_line/algebraic} A line is an \hyperref[def:affine_algebraic_set/curve]{algebraic curve} in \( X \) given by a polynomial of degree one.

    \thmitem{def:geometric_line/parametric} A line with \term{directional vector} \( d \) and \term{origin} \( o \) is the parametric curve
    \begin{equation*}
      \begin{aligned}
        &l: \BbbK \to X   \\
        &l(t) \coloneqq o + t d.
      \end{aligned}
    \end{equation*}
  \end{thmenum}
\end{definition}

\begin{remark}\label{rem:figure_through_origin}
  Rather than dealing with general lines as in \fullref{def:geometric_line}, it is common to take the simpler definition of a line through the origin, i.e. a subspace of dimension one. A general line is then simply defined as a translation of a line. This is very common for \hyperref[def:geometric_cone]{cones}, for example --- in general, unless noted otherwise, it is safe to assume that cones pass through the origin. \hyperref[def:hyperplane]{Hyperplanes}, on the other hand, are often said to be \enquote{linear} or \enquote{affine} based on whether they pass through the origin.
\end{remark}

\begin{definition}\label{def:geometric_ray}\mimprovised
  Let \( X \) be a real vector space. We define the \term{positive closed ray} with \term{origin} \( o \) and \term{direction} \( d \) as the \hyperref[def:parametric_curve]{curve}
  \begin{equation*}
    \begin{aligned}
       &l^+: [0, \infty) \to X \\
       &l^+(t) \coloneqq o + td.
    \end{aligned}
  \end{equation*}

  The corresponding \term{open ray} is the similar parametric curve but mapping from \( (0, \infty) \) rather than \( [0, \infty) \).

  Analogously, the \term{negative closed ray} with origin \( o \) and direction \( x \) is
  \begin{equation*}
    \begin{aligned}
       &l^-: [0, \infty) \to X \\
       &l^-(t) \coloneqq o - td.
    \end{aligned}
  \end{equation*}

  As discussed in \fullref{rem:figure_through_origin}, unless explicitly noted otherwise, we may assume that the vertex of the ray is the origin.

  Compare this to the purely geometric \fullref{def:euclidean_plane/ray}.
\end{definition}

\begin{definition}\label{def:conic_hull}\mimprovised
  Let \( X \) be a vector space over \( \BbbK \). We say that the \hyperref[rem:linear_combinations]{linear combination}
  \begin{equation*}
    t_1 x_1 + \ldots + t_n x_n
  \end{equation*}
  with real coefficients is a \term{conic combination} if all of its coefficients are nonnegative.

  The \term{conic hull} of the set \( S \) is the set of all conic combinations of members of \( S \). This is a \hyperref[def:closure_operator]{closure operator}.

  Geometrically, the conic hull of \( S \) is the smallest \hyperref[def:geometric_cone]{cone} that is closed under addition. See \cref{fig:def:convex_hull}.
\end{definition}
\begin{proof}
  The proof that the conic hull is a closure operator is similar to that of affine hulls in \fullref{def:affine_hull}, except that it is simpler.
\end{proof}

\begin{definition}\label{def:geometric_cone}\mcite[20]{Clarke2013}
  An open (resp. closed) \term{cone} in a real vector space is a union of open (resp. closed) \hyperref[def:geometric_ray]{rays} with a common vertex.

  Despite the name a cone is not necessarily a \hyperref[def:conic_hull]{conic set} --- see \cref{fig:def:geometric_cone}. \Fullref{thm:def:convex_hull/conic_cone} gives a necessary and sufficient condition for this.

  \begin{figure}[!ht]
    \centering
    \includegraphics{output/def__geometric_cone.pdf}
    \caption{One cone consisting of two rays and one consisting of two rays and their conic hull.}\label{fig:def:geometric_cone}
  \end{figure}
\end{definition}

\begin{definition}\label{def:hyperplane}\mcite[41]{Clarke2013}
  Dually to \hyperref[def:geometric_line]{lines}, another particularly important \hyperref[def:geometric_shape]{shape} is a \term{hyperplane}.

  \begin{thmenum}
    \thmitem{def:hyperplane/subspace} A \term{linear hyperplane} is simply a subspace of \( X \) of \hyperref[thm:vector_space_dimension]{codimension} one. An \term{affine hyperplane} is a \hyperref[def:rigid_motion/translation]{translation} of a linear hyperplane.

    \thmitem{def:hyperplane/kernel} Linear hyperplanes are simply \hyperref[def:zero_locus]{zero loci} (actually \hyperref[def:module/kernel]{kernels}) of \hyperref[def:semimodule/homomorphism]{linear functionals} and affine hyperplanes are zero loci of \hyperref[def:affine_operator]{affine functionals}.

    We use the convention from \fullref{rem:dual_space_bilinear_form} for writing \( \inprod l x \) rather than \( l(x) \) or \( lx \) for linear functionals.
  \end{thmenum}
\end{definition}

\begin{example}\label{ex:hyperplanes}
  Affine \hyperref[def:hyperplane]{hyperplanes} in \( \BbbR^2 \) are \hyperref[def:geometric_line]{lines} and affine hyperplanes in \( \BbbR^3 \) are planes.

  Linear \hyperref[def:hyperplane]{hyperplanes} in \( \BbbR^2 \) are the lines passing through the origin and linear hyperplanes in \( \BbbR^3 \) are the planes incident to the origin.
\end{example}

\begin{definition}\label{def:half_space}\mcite[41]{Clarke2013}
  Real vector spaces over have the concept of \term{half-spaces}. Given a \hyperref[def:hyperplane]{hyperplane} \( H \) of the real vector space \( X \), defined by the affine functional \( f(x) = \inprod l x - a \), its closed half-spaces are defined as
  \begin{equation*}
    H^+ \coloneqq \set{ f(x) \geq 0 } = \set{ \inprod l x \geq a }
  \end{equation*}
  and
  \begin{equation*}
    H^- \coloneqq \set{ f(x) \leq 0 } = \set{ \inprod l x \leq a }
  \end{equation*}

  If the inequalities are strict, we instead obtain \term{open half-spaces}.
\end{definition}

\begin{definition}\label{def:polyhedron}\mimprovised
  A \term{polyhedron} in a real vector space is an intersection of \hyperref[def:half_space]{half-spaces}.
\end{definition}

\begin{definition}\label{def:hyperplane_separation}\mimprovised
  We say that the sets \( A \) and \( B \) in a real or complex vector space are \term{separated} by the linear functional \( l \) if there exists a real number \( c \) such that, for every \( x \) in \( A \) and \( y \) in \( B \),
  \begin{equation}\label{eq:def:hyperplane_separation/normal}
    \real \inprod l x < c \leq \real \inprod l y.
  \end{equation}

  See \fullref{rem:linear_functionals_over_c} for a justification of only considering the real part of \( l \).

  The asymmetry in the inequality \eqref{eq:def:hyperplane_separation/normal} can be inverted by considering \( -l \) and \( -c \).

  We say that \( A \) and \( B \) are \term{strongly separated} by \( l \) if both inequalities in \eqref{eq:def:hyperplane_separation/normal} are strict:
  \begin{equation}\label{eq:def:hyperplane_separation/strong}
    \real \inprod l x < c < \real \inprod l y.
  \end{equation}

  We can regard \( l \) as a hyperplane as in \fullref{def:hyperplane/kernel}, which justifies the terminology \enquote{hyperplane separation}. It is more correct, however, to say that they are separated by the affine hyperplane \( l(x) + c \).

  In the real case, \eqref{eq:def:hyperplane_separation/strong} is equivalent to requiring that \( A \) and \( B \) are contained in opposite open half-spaces, while \eqref{eq:def:hyperplane_separation/normal} allows one of the half-spaces to be closed.
\end{definition}

\begin{definition}\label{def:convex_hull}\mimprovised
  We say that a \hyperref[rem:linear_combinations]{linear combination} is \term{convex} if it is both an \hyperref[def:affine_hull]{affine} and a \hyperref[def:conic_hull]{conic} combination.

  The \term{convex hull} of the set \( S \) is the set of all convex combinations of members of \( S \). This can be expressed succinctly by saying that, for any number \( \lambda \) in the unit interval and any vectors \( x \) and \( y \), the hull must contain
  \begin{equation}\label{eq:def:convex_hull/combination}
    \lambda x + (1 - \lambda) y.
  \end{equation}

  The convex hull is a \hyperref[def:closure_operator]{closure operator}. A set that coincides with its convex hull is called a \term{convex set}. The geometric interpretation of convex sets is given in \fullref{thm:def:convex_hull/line_segments} and drawn in \cref{fig:def:convex_hull}.

  \begin{figure}[!ht]
    \centering
    \includegraphics[page=1]{output/def__convex_hull.pdf}
    \caption{The \hyperref[def:affine_hull]{affine}, \hyperref[def:conic_hull]{conic} and \hyperref[def:convex_hull]{convex} hulls of \( \set{ A, B, C } \).}\label{fig:def:convex_hull}
  \end{figure}
\end{definition}

\begin{definition}\label{def:line_segment}
  Similar to \hyperref[def:geometric_line]{lines} and \hyperref[def:geometric_ray]{rays}, we can define segments. A \term{line segment} between the points \( x \) and \( y \) in \( X \) is the parametric curve
  \begin{equation*}
    \begin{aligned}
       &s: [0, 1] \to X \\
       &s(t) \coloneqq (1 - t) x + t y.
    \end{aligned}
  \end{equation*}
\end{definition}

\begin{proposition}\label{thm:def:convex_hull}
  \hyperref[def:convex_hull]{Convex sets} have the following basic properties:

  \begin{thmenum}
    \thmitem{thm:def:convex_hull/line_segments} A set is convex if and only if it contains the entire \hyperref[def:line_segment]{line} between any two of its points.

    \thmitem{thm:def:convex_hull/conic_cone} A \hyperref[def:geometric_cone]{cone} is a \hyperref[def:convex_hull]{conic set} if and only if it is \hyperref[def:convex_hull]{convex}.

    \thmitem{thm:def:convex_hull/closed_under_intersections} Any nonempty intersection of convex sets is convex.
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:def:convex_hull/line_segments} Trivial.

  \SubProofOf{thm:def:convex_hull/conic_cone}
  \SufficiencySubProof* Convex combinations are conic, hence all conic sets are also convex sets.

  \NecessitySubProof* Fix a convex cone \( C \) and a conic combination
  \begin{equation*}
    t_1 x_1 + \cdots + t_n x_n
  \end{equation*}
  in \( C \). Define
  \begin{equation*}
    r_k \coloneqq \frac {t_k} {t_1 + \cdots + t_n}.
  \end{equation*}

  Then
  \begin{equation*}
    r_1 x_1 + \cdots + r_n x_n
  \end{equation*}
  is a convex combination. Hence, it belongs to \( C \). But \( C \) is also a cone, hence
  \begin{equation*}
    t_1 x_1 + \cdots + t_n x_n = (t_1 + \cdots + t_n) \parens[\Big]{ r_1 x_1 + \cdots + r_n x_n }
  \end{equation*}
  is in \( C \).

  \SubProofOf{thm:def:convex_hull/closed_under_intersections} Trivial.
\end{proof}

\begin{definition}\label{def:affine_dependence}\mimprovised
  Let \( X \) be a vector space over \( \BbbR \) and let \( B \) be a subset of \( X \). We say that the elements of \( B \) are \term{affinely independent} if there exists a nontrivial linear combination
  \begin{equation*}
    t_1 b_1 + \cdots + t_n b_n
  \end{equation*}
  that sums to zero and where
  \begin{equation*}
    t_1 + \cdots + t_n = 0.
  \end{equation*}

  Relaxing the last condition, we obtain the definition of linear dependence.
\end{definition}

\begin{definition}\label{def:simplex}
  A \( k \)-\term{simplex} is the convex \hyperref[def:convex_hull/hull]{hull} of \( k + 1 \) \hyperref[def:affine_dependence]{affinely independent} vectors called the \term{vertices} of the simplex. The convex hull of any subset of the vertices is called a \term{face} of the simplex.

  \begin{thmenum}
    \thmitem{def:simplex/point} A \( 0 \)-simplex is a \hyperref[def:point]{point}.
    \thmitem{def:simplex/line_segment} A \( 1 \)-simplex is a line segment as defined in \fullref{def:convex_hull/line_segment}.
    \thmitem{def:simplex/triangle} A \( 2 \)-simplex is a triangle as defined in \fullref{def:triangle}.
    \thmitem{def:simplex/tetrahedron} A \( 3 \)-simplex is called a \term{tetrahedron}.
  \end{thmenum}
\end{definition}

\begin{definition}\label{def:k_cell}
  A \( k \)-cell is a \hyperref[def:cartesian_product]{Cartesian product} of \( k \) nonempty \hyperref[def:partially_ordered_set_interval/closed]{closed intervals} of real numbers.

  \begin{thmenum}
    \thmitem{def:k_cell/point} A \( 0 \)-cell is a \hyperref[def:point]{point}.
    \thmitem{def:k_cell/interval} A \( 1 \)-cell is a closed interval.
    \thmitem{def:k_cell/rectangle} A \( 2 \)-cell is called a \term{rectangle}. If a rectangle \( R \) is a product of two copies of the same interval, i.e. if \( R = [a, b]^2 \), we say that \( R \) is a \term{square} with side \( b - a \).
    \thmitem{def:k_cell/parallelepiped} A \( 3 \)-cell is called a \term{parallelepiped}. If \( R = [a, b]^3 \), we say that \( R \) is a \term{cube} with side \( b - a \).
  \end{thmenum}
\end{definition}

\begin{definition}\label{def:neighborhood_set_types}
  The following topology-independent definitions are often used for neighborhoods in a topological vector space \( X \):

  \begin{thmenum}
    \thmitem{def:neighborhood_set_types/absorbing} \( A \) is \term{absorbing} if \( \bigcup_{k=0}^\infty kA = X \).
    \thmitem{def:neighborhood_set_types/symmetric} \( A \) is \term{symmetric} if \( -A = A \).
    \thmitem{def:neighborhood_set_types/balanced} \( A \) is \term{balanced} if \( tA \subseteq A \) for any \( t \in [0, 1] \).
  \end{thmenum}
\end{definition}

\begin{definition}\label{def:collinear_complanar}
  The geometric version of \hyperref[def:linear_dependence]{linear independence} has two special names: we say that the set \( A \subseteq X \) of any vector space \( X \) is \term{collinear} (on the same line) if \( \dim(\linspan(A)) \leq 1 \) and \term{complanar} (on the same plane) if \( \dim(\linspan(A)) \leq 2 \).
\end{definition}
