\subsection{Diagonalization}\label{subsec:diagonalization}

In this subsection, we restrict ourselves to fields rather than arbitrary rings.

\begin{definition}\label{def:eigenpair}\mcite[def. 4.17]{Rudin1991Functional}
  Let \( T: V \to V \) be a \hyperref[def:semimodule/homomorphism]{linear endomorphism} over the \hyperref[def:vector_space]{vector space} \( V \) over \( \BbbK \).

  An \term{eigenpair} \( (\lambda, x) \) consists of an \term{eigenvalue} \( \lambda \in \BbbK \) and a \hi{nonzero} \term{eigenvector} \( x \in V \) such that
  \begin{equation*}
    Tx = \lambda x.
  \end{equation*}

  We say that \( \lambda \) is an eigenvalue of \( T \) if it is part of at least one eigenpair; analogously, we say that \( x \) is an eigenvector if it is part of at least one eigenpair.

  In function spaces, the term \enquote{eigenfunction} is sometimes used instead of \enquote{eigenvector}.
\end{definition}

\begin{example}\label{ex:def:eigenpair}
  We list some examples of \hyperref[def:eigenpair]{eigenpairs}:
  \begin{thmenum}
    \thmitem{ex:def:eigenpair/zero} The zero matrix over \( \BbbK \) of degree \( n \) has the entire vector space \( \BbbK^n \), without the zero vector, as its eigenvectors corresponding to the eigenvalue \( 0 \).

    \thmitem{ex:def:eigenpair/identity} The identity matrix \( I_n \) over \( \BbbK \) of degree \( n \) also has \( \BbbK^n \setminus \set{ \vect 0 } \) as its eigenvectors, however they correspond to the eigenvalue \( 1 \) rather than \( 0 \).

    \thmitem{ex:def:eigenpair/2112} Consider the matrix
    \begin{equation*}
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}.
    \end{equation*}

    The following two are eigenpairs:
    \begin{align*}
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
      \begin{pmatrix}
        1 \\ 1
      \end{pmatrix}
      =
      3
      \begin{pmatrix}
        1 \\ 1
      \end{pmatrix}
      &&
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
      \begin{pmatrix}
        1 \\ -1
      \end{pmatrix}
      =
      1
      \begin{pmatrix}
        1 \\ -1
      \end{pmatrix}
    \end{align*}

    As we shall show in \fullref{ex:def:eigenspace/2112}, these are not the only eigenpairs for \( A \).

    \thmitem{ex:def:eigenpair/exponent} In suitable function spaces like \( C^\infty(\BbbR) \), the function \( e^{\lambda x} \) is en eigenvector of the \hyperref[def:differentiability]{differentiation} operator \( D_x \) corresponding to the eigenvalue \( \lambda \) because
    \begin{equation*}
      D_x e^{\lambda x} = \lambda e^{\lambda x}.
    \end{equation*}

    In particular, the case \( \lambda = 0 \) corresponds to the constant function \( e^0 = 1 \), hence
    \begin{equation*}
      D_x 1 = 0 \cdot 1.
    \end{equation*}
  \end{thmenum}
\end{example}

\begin{remark}\label{rem:eigenpairs_via_invertibility}
  The \hyperref[def:eigenpair]{eigenpair equation}
  \begin{equation*}
    Tx = \lambda x
  \end{equation*}
  can be rewritten as
  \begin{equation*}
    (T - \lambda \cdot \id) x = \vect 0.
  \end{equation*}

  We can regard the above as a \hyperref[rem:system_of_equations]{system of equations}. By \fullref{thm:homogeneous_linear_equations_solutions}, there exists a nonzero solution, i.e. an eigenvector corresponding to \( \lambda \), if and only if the map
  \begin{equation*}
    T - \lambda \cdot \id
  \end{equation*}
  is an isomorphism.

  If \( V \) is finite-dimensional, by \fullref{thm:square_matrix_left_invertible_iff_right_invertible}, this map is injective if and only if it is surjective. Otherwise, it may fail to be either injective or surjective.
\end{remark}

\begin{definition}\label{def:eigenspace}\mcite[sec. 31.2]{Тыртышников2004Лекции}
  The \term{eigenspace} of an \hyperref[def:eigenpair]{eigenvalue} of \( T: V \to V \) is the set of all corresponding \hyperref[def:eigenpair]{eigenvectors}, along with the zero vector. It is a vector subspace of \( V \) as a consequence of \fullref{thm:degrees_of_freedom}. The \hyperref[thm:vector_space_dimension]{dimension} of the eigenspace is called the \term{geometric multiplicity} of the eigenvalue.
\end{definition}

\begin{example}\label{ex:def:eigenspace}
  We list some examples of \hyperref[def:eigenspace]{eigenspaces}:
  \begin{thmenum}
    \thmitem{ex:def:eigenspace/zero} We discussed in \fullref{ex:def:eigenpair/zero} how, with respect to the zero matrix, the entire space is the eigenspace of zero. Hence, the geometric multiplicity of zero is \( n \).

    \thmitem{ex:def:eigenspace/identity} Similarly, with respect to the identity matrix, the entire space is the eigenspace of \( 1 \).

    \thmitem{ex:def:eigenspace/2112} We discussed in \fullref{ex:def:eigenpair/2112} how the vector \( (1, 1) \) corresponds to the eigenvalue \( 3 \). The entire eigenspace is
    \begin{equation*}
      \set{ (t, t) \given t \in \BbbK }.
    \end{equation*}

    The eigenspace of \( 1 \) is
    \begin{equation*}
      \set{ (-t, t) \given t \in \BbbK }.
    \end{equation*}

    \thmitem{ex:def:eigenspace/1101} The eigenspace of \( 1 \) with respect to
    \begin{equation*}
      \begin{pmatrix}
        1 & 1 \\
        0 & 1
      \end{pmatrix}
    \end{equation*}
    is
    \begin{equation*}
      \set{ (t, 0) \given t \in \BbbK }.
    \end{equation*}

    We will show in \fullref{ex:def:characteristic_polynomial/1101} that this is the only eigenvalue of the matrix.

    \thmitem{ex:def:eigenspace/exponent} The eigenspace of \( \lambda \) with respect to differentiation is
    \begin{equation*}
      \set{ t e^\lambda \given t \in \BbbK }.
    \end{equation*}

    Hence, every scalar is an eigenvalue with geometric multiplicity \( 1 \).
  \end{thmenum}
\end{example}

\begin{proposition}\label{thm:eigenspace_direct_sum}

\end{proposition}

\begin{definition}\label{def:characteristic_polynomial}\mimprovised
  Let \( T: V \to V \) be a linear endomorphism over the \( n \)-dimensional vector space \( V \) over \( \BbbK \). Regard \( T \) as an endomorphism over the module \( \BbbK[\Lambda]^n \) over the \hyperref[def:polynomial_algebra]{polynomial ring} \( \BbbK[\Lambda] \). The \term{characteristic polynomial} of \( T \) is defined as
  \begin{equation*}
    p_T(\Lambda) \coloneqq \det(T - \Lambda \id).
  \end{equation*}

  The \term{algebraic multiplicity} of \( \lambda \in \BbbK \) is the \hyperref[def:polynomial_root]{root multiplicity} of \( \lambda \) in \( p_A \).
\end{definition}

\begin{proposition}\label{thm:eigenvalues_and_characteristic_polynomials}
  The \hyperref[def:eigenpair]{eigenvalues} of a square matrix are precisely the \hyperref[def:polynomial_root]{roots} of its \hyperref[def:characteristic_polynomial]{characteristic polynomial}.
\end{proposition}
\begin{proof}
  Follows from \fullref{rem:eigenpairs_via_invertibility}.
\end{proof}

\begin{remark}\label{rem:characteristic_polynomial}
  We are, for the most part, only interested in the roots of a \hyperref[def:characteristic_polynomial]{characteristic polynomial}. For this reason, it is sometimes alternatively defined as
  \begin{equation*}
    \det(\Lambda \id - T).
  \end{equation*}

  This definition is used, for example, in \cite[74]{Knapp2016BasicAlgebra}. This ensures that the polynomial is \hyperref[def:monic_polynomial]{monic}.

  We prefer
  \begin{equation*}
    \det(T - \Lambda \id)
  \end{equation*}
  because it is the more popular definition and because it directly generalizes the determinant of \( A \).
\end{remark}

\begin{example}\label{ex:def:characteristic_polynomial}
  We list some examples of \hyperref[def:characteristic_polynomial]{characteristic polynomials}:
  \begin{thmenum}
    \thmitem{ex:def:characteristic_polynomial/zero} The characteristic polynomial of the \( n \times n \) zero matrix is \( (-\Lambda)^n \). Its only root is \( 0 \). Both the geometric and the algebraic multiplicities of \( 0 \) are \( n \).

    \thmitem{ex:def:characteristic_polynomial/identity} Similarly, the characteristic polynomial of the identity matrix \( I_n \) is \( (1 - \Lambda)^n \). Hence, the only eigenvalue of \( I_n \) is \( 1 \) and both of its multiplicities are \( n \).

    \thmitem{ex:def:characteristic_polynomial/2112} We continue \fullref{ex:def:eigenspace/2112}. The matrix
    \begin{equation*}
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
    \end{equation*}
    has characteristic polynomial
    \begin{equation*}
      (2 - \Lambda)^2 - 1 = \Lambda^2 - 4\Lambda + 3.
    \end{equation*}

    Its roots are
    \begin{equation*}
      \frac {4 \pm \sqrt{16 - 12}} 2 = 2 \pm 1.
    \end{equation*}

    Both roots have geometric and algebraic multiplicities \( 1 \).

    \thmitem{ex:def:characteristic_polynomial/1101} The matrix
    \begin{equation*}
      \begin{pmatrix}
        1 & 1 \\
        0 & 1
      \end{pmatrix}
    \end{equation*}
    has characteristic polynomial
    \begin{equation*}
      (1 - \Lambda)^2.
    \end{equation*}

    The only eigenvalue is hence \( 1 \), and it has algebraic multiplicity \( 2 \). We discussed in \fullref{def:eigenspace/1101} that the corresponding geometric multiplicity is \( 1 \).
  \end{thmenum}
\end{example}

\begin{proposition}\label{thm:geometric_vs_algebraic_multiplicity}\mcite{MathSE:geometric_multiplicity_is_bounded_by_algebraic}
  The \hyperref[def:characteristic_polynomial]{geometric multiplicity} of an eigenvalue does not exceed its \hyperref[def:characteristic_polynomial]{algebraic multiplicity}.
\end{proposition}
\begin{proof}
  Suppose that the geometric multiplicity of \( \lambda \) with respect to \( T \) is \( m \). Then there exist linearly independent eigenvalues \( x_1, \ldots, x_m \) such that, for \( k = 1, \ldots, m \),
  \begin{equation*}
    T x_k = \lambda x_k.
  \end{equation*}

  \Fullref{thm:def:vector_space/expansion} allows us to expand this to a basis of \( \BbbK^n \) via some vectors \( x_{m+1}, \ldots, x_n \). With respect to this basis, the operator \( T \) has the matrix
  \begin{equation*}
    A = \begin{pmatrix}
      \lambda I_k & B \\
      0           & C
    \end{pmatrix},
  \end{equation*}
  for suitable matrices \( B \) and \( C \).

  The characteristic polynomial of this matrix is
  \begin{equation*}
    p_A(\Lambda) = (\Lambda - \lambda)^k p_C(\Lambda).
  \end{equation*}

  Therefore, the algebraic multiplicity of \( A \) is at least \( k \).
\end{proof}

\begin{definition}\label{def:point_spectrum}\mcite[10.32]{Rudin1991Functional}
  The set of all \hyperref[def:eigenpair]{eigenvalues} of a linear endomorphism is called its \term{point spectrum}.
\end{definition}

\begin{definition}\label{def:diagonalizable_matrix}\mimprovised
  A square matrix is called \term{diagonalizable} if it is \hyperref[def:similar_matrices]{similar} to a diagonal matrix.
\end{definition}

\begin{proposition}\label{thm:diagonalizable_matrix_iff_eigenvectors}
  An \( n \times n \) square matrix \( A \) is \hyperref[def:diagonalizable_matrix]{diagonalizable} via \( D = P^{-1} A P \) if and only if there exist \( n \) linearly independent \hyperref[def:eigenpair]{eigenvectors} \( e_1, \ldots, e_n \) such that
  \begin{equation*}
    P = \parens*
    {
      \begin{array}{c|c|c}
        e_1 & \cdots & e_n
      \end{array}
    }.
  \end{equation*}
\end{proposition}
\begin{proof}
  \NecessitySubProof Let \( e_1, \ldots, e_n \) be linearly independent eigenvectors corresponding to the eigenvalues \( \lambda_1, \ldots, \lambda_n \). Let \( P \) be the matrix whole columns are these eigenvectors and let
  \begin{equation*}
    D \coloneqq \op{diag}(\lambda_1, \ldots, \lambda_n).
  \end{equation*}

  Then
  \begin{balign*}
    P D
    &=
    \parens*
      {
        \begin{array}{c}
          \vect \lambda_1^T \\ \hline \vdots \\ \hline \vect \lambda_n^T
        \end{array}
      }
    \parens*
      {
        \begin{array}{c|c|c}
          e_1 & \cdots & e_n
        \end{array}
      }
    = \\ &=
    \sum_{k=1}^n \vect \lambda_k^T e_k
    = \\ &=
    \parens*
      {
        \begin{array}{c|c|c|c}
          \lambda_1 e_1 & \vect 0 & \cdots & \vect 0
        \end{array}
      }
    +
    \parens*
      {
        \begin{array}{c|c|c|c}
          \vect 0 & \lambda_2 e_2 & \cdots & \vect 0
        \end{array}
      }
    +
    \cdots
    +
    \parens*
      {
        \begin{array}{c|c|c|c}
          \vect 0 & \vect 0 & \cdots & \lambda_n e_n
        \end{array}
      }
    = \\ &=
    \parens*
      {
        \begin{array}{c|c|c|c}
          \lambda_1 e_1 & \lambda_2 e_2 & \cdots & \lambda_n e_n
        \end{array}
      }
    = \\ &=
    \parens*
      {
        \begin{array}{c|c|c|c}
          A e_1 & A e_2 & \cdots & A e_n
        \end{array}
      }
    = \\ &=
    A P.
  \end{balign*}

  \SufficiencySubProof Similar to the necessity proof except that \( P D = A P \) is the assumption and \( (\lambda_k, e_k) \) being eigenpairs is the conclusion.
\end{proof}

\begin{definition}\label{def:adjoint_operator}\mcite[sec. 38.1]{Тыртышников2004Лекции}
  We say that the linear operator \( T^*: V \to U \) between \hyperref[def:inner_product_space]{inner product spaces} is \term{adjoint} to \( T: U \to V \) if for every \( x \in U \) and \( y \in V \) we have
  \begin{equation*}
    \inprod {Tx} y = \inprod x {T^* y}.
  \end{equation*}

  If \( U = V \) and \( T = T^* \), we say that the operator is \term{self-adjoint} or \term{symmetric}. In the case of a complex inner product space, we say that \( T \) is \enquote{\term{Hermitian}} instead of \enquote{symmetric}.

  Note that this definition holds for linear operators and the concept differs from the similar concepts for bilinear forms. The inner product itself is symmetric or Hermitian by definition.
\end{definition}

\begin{proposition}\label{thm:conjugate_transpose}
  The \hyperref[def:conjugate_transpose]{conjugate transpose} \( A^* \) of the matrix \( A \) corresponds to the \hyperref[def:adjoint_operator]{adjoint operator} of \( A \) (with respect to the \hyperref[def:inner_product_space]{dot product}).
\end{proposition}
\begin{proof}
  \begin{equation*}
    \inprod {Ax} {y}
    =
    (Ax)^* y
    =
    (x^* A^*) y
    =
    x^* (A^* y)
    =
    \inprod {x} {A^* y}.
  \end{equation*}
\end{proof}

\begin{definition}\label{def:invariant_subspace}\mcite[218]{Knapp2016BasicAlgebra}
  We say that the subspace \( U \) of \( V \) is \term{invariant} under the linear operator \( T: V \to V \) if \( T(U) \subseteq U \). Invariant subspaces allow us to restrict \( T \) to an endomorphism on \( U \).
\end{definition}

\begin{proposition}\label{thm:def:invariant_subspace}
  Let \( T: V \to V \) be a linear operator. \hyperref[def:invariant_subspace]{Invariant} under \( T \) subspaces of \( V \) have the following basic properties:
  \begin{thmenum}
    \thmitem{thm:def:invariant_subspace/kernel} The kernel of \( T \) is invariant.
    \thmitem{thm:def:invariant_subspace/image} The image of \( T \) is invariant.
    \thmitem{thm:def:invariant_subspace/complement} If \( T \) is \hyperref[def:adjoint_operator]{self-adjoint} and \( U \) is invariant under \( T \), so is its \hyperref[def:orthogonal_complement]{orthogonal complement} \( U^\perp \).
  \end{thmenum}
\end{proposition}
\begin{proof}
  \SubProofOf{thm:def:invariant_subspace/kernel} If \( x \in \ker T \), then \( Tx = 0 \in \ker T \).

  \SubProofOf{thm:def:invariant_subspace/image} If \( y \in \img T \), then \( Ty \in \img T \) because \( \img T \) is a subspace of \( V \).

  \SubProofOf{thm:def:invariant_subspace/complement} Let \( x \in U \) and \( y \in U^\perp \). We know that \( Tx \in U \) and thus \( \inprod {Tx} y = 0 \). If \( T \) is self-adjoint, then
  \begin{equation*}
    0 = \inprod {Tx} y = \inprod x {Ty},
  \end{equation*}
  and thus \( Ty \in U^\perp \).
\end{proof}

\begin{definition}\label{def:unitary_matrix}
  We say that a square real (resp. complex) matrix is \term{orthogonal} (resp. \term{unitary}) if any of the following conditions hold:
  \begin{thmenum}
    \thmitem{def:unitary_matrix/transpose} The \hyperref[def:transpose_matrix]{transpose matrix} (resp. \hyperref[def:conjugate_transpose]{conjugate transpose}) is also the \hyperref[def:inverse_matrix]{inverse matrix}.
    \thmitem{def:unitary_matrix/columns} The columns of the matrix are pairwise \hyperref[def:orthogonality]{orthonormal} with respect to the \hyperref[def:inner_product_space]{dot product}.
  \end{thmenum}
\end{definition}
\begin{proof}
  \EquivalenceSubProof{def:unitary_matrix/transpose}{def:unitary_matrix/columns} Let
  \begin{equation*}
    A = \begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}.
  \end{equation*}

  Then
  \begin{equation*}
    A A^*
    =
    \begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix}
    \begin{pmatrix} a_1^* & \cdots & a_n^* \end{pmatrix}
    =
    \begin{pmatrix}
      a_1 a_1^*  & \cdots & a_1 a_n^* \\
      \vdots     & \ddots & \vdots \\
      a_n a_1^*  & \cdots & a_n a_n^*
    \end{pmatrix}.
  \end{equation*}

  Hence, \( A A^* = I_n \) if and only if
  \begin{equation*}
    a_i a_j^* = \begin{cases}
      1, &i = j \\
      0, &i \neq j
    \end{cases}.
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:hermitian_operator_eigenvalues_are_real}
  The \hyperref[def:eigenpair]{eigenvalues} of a \hyperref[def:adjoint_operator]{self-adjoint linear operator} are real.
\end{proposition}
\begin{proof}
  For every eigenpair \( (\lambda, x) \) of \( T: V \to V \) we have
  \begin{equation*}
    \lambda \inprod x x
    =
    \inprod {\lambda x} x
    =
    \inprod {L x} x
    =
    \inprod x {L x}
    =
    \inprod x {\lambda x}
    =
    \overline \lambda \inprod x x.
  \end{equation*}

  Since inner products are positive definite, \( \inprod x x \) is a positive real number and hence we can cancel it to obtain the equality \( \lambda = \overline \lambda \). Hence, if \( \lambda \) is an eigenvalue, it is a real number.
\end{proof}

\begin{theorem}[Finite-dimensional spectral theorem]\label{thm:spectral_theorem}
  Let \( V \) be a real (resp. complex) vector space of dimension \( n \).

  \begin{thmenum}
    \thmitem{thm:spectral_theorem/basis} Every \hyperref[def:transpose_matrix]{symmetric} (resp. \hyperref[def:conjugate_transpose]{Hermitian}) endomorphism on \( V \) induces an \hyperref[def:orthogonality]{orthonormal} basis of eigenvectors.

    \thmitem{thm:spectral_theorem/eigenvalues} Every symmetric (resp. Hermitian) endomorphism on \( V \) has \( n \) real eigenvalues (counting multiplicity).

    \thmitem{thm:spectral_theorem/matrix} Every symmetric (resp. Hermitian) \( n \times n \) matrix \( A \) is \hyperref[def:diagonalizable_matrix]{diagonalizable} via an \hyperref[def:unitary_matrix]{orthogonal} (resp. \hyperref[def:unitary_matrix]{unitary}) matrix. More precisely, we can decompose \( A \) as
    \begin{equation*}
      A = P^{-1} D P,
    \end{equation*}
    where \( D \) is a real diagonal matrix of eigenvalues and \( P \) is a unitary matrix whose columns are eigenvalues of \( A \).

    Furthermore, the \( k \)-th column of \( P \) is an eigenvector of the \( (k, k) \)-th entry of \( D \).
  \end{thmenum}
\end{theorem}
\begin{proof}
  We will only consider Hermitian matrices since the proof for symmetric matrices is identical.

  \SubProofOf{thm:spectral_theorem/basis} We will use induction on \( n \) to show that every Hermitian endomorphism on an \( n \)-dimensional space induces an orthonormal basis of of eigenvectors. The case \( n = 1 \) is obvious.

  Suppose that the statement holds for \( n - 1 \), let \( V \) be an \( n \)-dimensional space and let \( T \) be an endomorphism on \( V \). By \fullref{thm:fundamental_theorem_of_algebra}, the characteristic polynomial has at least one eigenvalue. Let \( (\lambda, x) \) be an eigenpair, let \( e_1 \coloneqq \ifrac x {\norm x} \) and let
  \begin{equation*}
    U \coloneqq \linspan\set{ e_1 }.
  \end{equation*}

  The space \( U \) is invariant under \( T \) because
  \begin{equation*}
    T(t e_1) = t(T e_1) = (t\lambda) e_1.
  \end{equation*}

  By \fullref{thm:def:invariant_subspace/complement}, its \hyperref[def:orthogonality]{orthogonal complement} \( U^\perp \) is also invariant under \( T \). We can thus restrict \( T \) to \( U^\perp \). Furthermore, \( \dim U^\perp = n - 1 \) by \fullref{thm:direct_sum_with_orthogonal_complement} and \fullref{thm:rank_of_direct_sum}. Hence, by the inductive hypothesis, there exists an orthonormal basis \( e_2, \ldots, e_n \) of \( U^\perp \) of eigenvectors of \( T\restr_{U^\perp} \).

  Therefore, \( e_1, \ldots, e_n \) is an orthonormal basis of \( \BbbC^n = U \oplus U^\perp \) consisting of eigenvectors of \( T \).

  \SubProofOf{thm:spectral_theorem/eigenvalues}
  \SubProof*{Complex numbers} Every characteristic polynomial of degree \( n \) over \( \BbbC \) has \( n \) roots, counting multiplicity. For every eigenpair \( (\lambda, x) \) of \( T: V \to V \) we have
  \begin{equation*}
    \lambda \inprod x x
    =
    \inprod {\lambda x} x
    =
    \inprod {L x} x
    =
    \inprod x {L x}
    =
    \inprod x {\lambda x}
    =
    \overline \lambda \inprod x x.
  \end{equation*}

  Since inner products are positive definite, \( \inprod x x \) is a positive real number and hence we can cancel it to obtain the equality \( \lambda = \overline \lambda \). Hence, if \( \lambda \) is an eigenvalue, it is a real number.

  \SubProof*{Real numbers} If \( V \) is instead a real vector space, we embed \( V \) into \( \BbbC^n \), find the eigenvalues of the embedding of \( T \) and then conclude that they are real numbers and hence eigenvalues of \( T \) itself.

  \SubProofOf{thm:spectral_theorem/matrix} Follows from \fullref{thm:spectral_theorem/basis}, \fullref{thm:diagonalizable_matrix_iff_eigenvectors} and \fullref{thm:spectral_theorem/eigenvalues}.
\end{proof}
