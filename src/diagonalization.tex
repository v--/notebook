\subsection{Diagonalization}\label{subsec:diagonalization}

In this subsection, we restrict ourselves to fields rather than arbitrary rings.

\begin{definition}\label{def:eigenpair}\mcite[def. 4.17]{Rudin1991Functional}
  Let \( L: V \to V \) be a \hyperref[def:semimodule/homomorphism]{linear endomorphism} over the \hyperref[def:vector_space]{vector space} \( V \) over \( \BbbK \).

  An \term{eigenpair} \( (\lambda, x) \) consists of an \term{eigenvalue} \( \lambda \in \BbbK \) and a \hi{nonzero} \term{eigenvector} \( x \in V \) such that
  \begin{equation*}
    Lx = \lambda x.
  \end{equation*}

  We say that \( \lambda \) is an eigenvalue of \( L \) if it is part of at least one eigenpair; analogously, we say that \( x \) is an eigenvector if it is part of at least one eigenpair.

  In function spaces, the term \enquote{eigenfunction} is sometimes used instead of \enquote{eigenvector}.
\end{definition}

\begin{example}\label{ex:def:eigenpair}
  We list some examples of \hyperref[def:eigenpair]{eigenpairs}:
  \begin{thmenum}
    \thmitem{ex:def:eigenpair/zero} The zero matrix over \( \BbbK \) of degree \( n \) has the entire vector space \( \BbbK^n \), without the zero vector, as its eigenvectors corresponding to the eigenvalue \( 0 \).

    \thmitem{ex:def:eigenpair/identity} The identity matrix \( I_n \) over \( \BbbK \) of degree \( n \) also has \( \BbbK^n \setminus \set{ \vect 0 } \) as its eigenvectors, however they correspond to the eigenvalue \( 1 \) rather than \( 0 \).

    \thmitem{ex:def:eigenpair/2112} Consider the matrix
    \begin{equation*}
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}.
    \end{equation*}

    The following two are eigenpairs:
    \begin{align*}
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
      \begin{pmatrix}
        1 \\ 1
      \end{pmatrix}
      =
      3
      \begin{pmatrix}
        1 \\ 1
      \end{pmatrix}
      &&
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
      \begin{pmatrix}
        1 \\ -1
      \end{pmatrix}
      =
      1
      \begin{pmatrix}
        1 \\ -1
      \end{pmatrix}
    \end{align*}

    As we shall show in \fullref{ex:def:eigenspace/2112}, these are not the only eigenpairs for \( A \).

    \thmitem{ex:def:eigenpair/exponent} In suitable function spaces like \( C^\infty(\BbbR) \), the function \( e^{\lambda x} \) is en eigenvector of the \hyperref[def:differentiability]{differentiation} operator \( D_x \) corresponding to the eigenvalue \( \lambda \) because
    \begin{equation*}
      D_x e^{\lambda x} = \lambda e^{\lambda x}.
    \end{equation*}

    In particular, the case \( \lambda = 0 \) corresponds to the constant function \( e^0 = 1 \), hence
    \begin{equation*}
      D_x 1 = 0 \cdot 1.
    \end{equation*}
  \end{thmenum}
\end{example}

\begin{remark}\label{rem:eigenpairs_via_invertibility}
  The \hyperref[def:eigenpair]{eigenpair equation}
  \begin{equation*}
    Lx = \lambda x
  \end{equation*}
  can be rewritten as
  \begin{equation*}
    (L - \lambda \cdot \id) x = \vect 0.
  \end{equation*}

  We can regard the above as a \hyperref[rem:system_of_equations]{system of equations}. By \fullref{thm:homogeneous_linear_equations_solutions}, there exists a nonzero solution, i.e. an eigenvector corresponding to \( \lambda \), if and only if the map
  \begin{equation*}
    L - \lambda \cdot \id
  \end{equation*}
  is an isomorphism.

  If \( V \) is finite-dimensional, by \fullref{thm:square_matrix_left_invertible_iff_right_invertible}, this map is injective if and only if it is surjective. Otherwise, it may fail to be either injective or surjective.
\end{remark}

\begin{definition}\label{def:eigenspace}\mcite[sec. 31.2]{Тыртышников2004Лекции}
  The \term{eigenspace} of an \hyperref[def:eigenpair]{eigenvalue} of \( L: V \to V \) is the set of all corresponding \hyperref[def:eigenpair]{eigenvectors}, along with the zero vector. It is a vector subspace of \( V \) as a consequence of \fullref{thm:degrees_of_freedom}. The \hyperref[thm:vector_space_dimension]{dimension} of the eigenspace is called the \term{geometric multiplicity} of the eigenvalue.
\end{definition}

\begin{example}\label{ex:def:eigenspace}
  We list some examples of \hyperref[def:eigenspace]{eigenspaces}:
  \begin{thmenum}
    \thmitem{ex:def:eigenspace/zero} We discussed in \fullref{ex:def:eigenpair/zero} how, with respect to the zero matrix, the entire space is the eigenspace of zero. Hence, the geometric multiplicity of zero is \( n \).

    \thmitem{ex:def:eigenspace/identity} Similarly, with respect to the identity matrix, the entire space is the eigenspace of \( 1 \).

    \thmitem{ex:def:eigenspace/2112} We discussed in \fullref{ex:def:eigenpair/2112} how the vector \( (1, 1) \) corresponds to the eigenvalue \( 3 \). The entire eigenspace is
    \begin{equation*}
      \set{ (t, t) \given t \in \BbbK }.
    \end{equation*}

    The eigenspace of \( 1 \) is
    \begin{equation*}
      \set{ (-t, t) \given t \in \BbbK }.
    \end{equation*}

    \thmitem{ex:def:eigenspace/1101} The eigenspace of \( 1 \) with respect to
    \begin{equation*}
      \begin{pmatrix}
        1 & 1 \\
        0 & 1
      \end{pmatrix}
    \end{equation*}
    is
    \begin{equation*}
      \set{ (t, 0) \given t \in \BbbK }.
    \end{equation*}

    We will show in \fullref{ex:def:characteristic_polynomial/1101} that this is the only eigenvalue of the matrix.

    \thmitem{ex:def:eigenspace/exponent} The eigenspace of \( \lambda \) with respect to differentiation is
    \begin{equation*}
      \set{ t e^\lambda \given t \in \BbbK }.
    \end{equation*}

    Hence, every scalar is an eigenvalue with geometric multiplicity \( 1 \).
  \end{thmenum}
\end{example}

\begin{proposition}\label{thm:eigenspace_direct_sum}

\end{proposition}

\begin{definition}\label{def:characteristic_polynomial}\mimprovised
  The \term{characteristic polynomial} of a square matrix \( A \) is
  \begin{equation*}
    p_A(\Lambda) \coloneqq \det(A - \Lambda I_n).
  \end{equation*}

  The \term{algebraic multiplicity} of \( \lambda \in \BbbK \) is the \hyperref[def:polynomial_root]{root multiplicity} of \( \lambda \) in \( p_A \).
\end{definition}

\begin{proposition}\label{thm:eigenvalues_and_characteristic_polynomials}
  The \hyperref[def:eigenpair]{eigenvalues} of a square matrix are precisely the \hyperref[def:polynomial_root]{roots} of its \hyperref[def:characteristic_polynomial]{characteristic polynomial}.
\end{proposition}
\begin{proof}
  Follows from \fullref{rem:eigenpairs_via_invertibility}.
\end{proof}

\begin{remark}\label{rem:characteristic_polynomial}
  We are, for the most part, only interested in the roots of a \hyperref[def:characteristic_polynomial]{characteristic polynomial}. For this reason, it is sometimes alternatively defined as
  \begin{equation*}
    \det(\Lambda I_n - A).
  \end{equation*}

  This definition is used, for example, in \cite[74]{Knapp2016BasicAlgebra}. This ensures that the polynomial is \hyperref[def:monic_polynomial]{monic}.

  We prefer
  \begin{equation*}
    \det(A - \Lambda I_n)
  \end{equation*}
  because it is the more popular definition and because it directly generalizes the determinant of \( A \).
\end{remark}

\begin{example}\label{ex:def:characteristic_polynomial}
  We list some examples of \hyperref[def:characteristic_polynomial]{characteristic polynomials}:
  \begin{thmenum}
    \thmitem{ex:def:characteristic_polynomial/zero} The characteristic polynomial of the \( n \times n \) zero matrix is \( (-\lambda)^n \). Its only root is \( 0 \). Both the geometric and the algebraic multiplicities of \( 0 \) are \( n \).

    \thmitem{ex:def:characteristic_polynomial/identity} Similarly, the characteristic polynomial of the identity matrix \( I_n \) is \( (1 - \lambda)^n \). Hence, the only eigenvalue of \( I_n \) is \( 1 \) and both of its multiplicities are \( n \).

    \thmitem{ex:def:characteristic_polynomial/2112} We continue \fullref{ex:def:eigenspace/2112}. The matrix
    \begin{equation*}
      \begin{pmatrix}
        2 & 1 \\
        1 & 2
      \end{pmatrix}
    \end{equation*}
    has characteristic polynomial
    \begin{equation*}
      (2 - \lambda)^2 - 1 = \lambda^2 - 4\lambda + 3.
    \end{equation*}

    Its roots are
    \begin{equation*}
      \frac {4 \pm \sqrt{16 - 12}} 2 = 2 \pm 1.
    \end{equation*}

    Both roots have geometric and algebraic multiplicities \( 1 \).

    \thmitem{ex:def:characteristic_polynomial/1101} The matrix
    \begin{equation*}
      \begin{pmatrix}
        1 & 1 \\
        0 & 1
      \end{pmatrix}
    \end{equation*}
    has characteristic polynomial
    \begin{equation*}
      (1 - \lambda)^2.
    \end{equation*}

    The only eigenvalue is hence \( 1 \), and it has algebraic multiplicity \( 2 \). We discussed in \fullref{def:eigenspace/1101} that the corresponding geometric multiplicity is \( 1 \).
  \end{thmenum}
\end{example}

\begin{proposition}\label{thm:similar_matrices_characteristic_polynomial}
  \hyperref[def:characteristic_polynomial]{Characteristic polynomial} are invariant under \hyperref[def:similar_matrices]{matrix similarity}.
\end{proposition}
\begin{proof}
  Suppose that \( A = P^{-1} B P \). Then
  \begin{equation*}
    A - \lambda I_n = P^{-1} B P - \lambda I_n = P^{-1} (B - \lambda I_n) P.
  \end{equation*}

  The determinants of \( A - \lambda I_n = B - \lambda I_n \) are identical by \fullref{thm:similar_matrices_and_determinants}.
\end{proof}

\begin{lemma}\label{thm:kernel_of_complex_matrix}
  For a complex matrix \( A \),
  \begin{equation*}
    \ker A = \ker A^* A.
  \end{equation*}
\end{lemma}
\begin{proof}
  If \( Ax = 0 \), obviously \( A^* A x = 0 \). Hence,
  \begin{equation*}
    \ker A \subseteq \ker A^* A.
  \end{equation*}

  Conversely, if \( A^* A x = 0 \), then
  \begin{equation*}
    x^* A^* A x = (Ax)^* (Ax) = 0.
  \end{equation*}

  Since the dot product is nondegenerate, it follows that
  \begin{equation*}
    Ax = 0.
  \end{equation*}

  Therefore,
  \begin{equation*}
    \ker A^* A \subseteq \ker A.
  \end{equation*}
\end{proof}

\begin{proposition}\label{thm:geometric_vs_algebraic_multiplicity}\mcite{MathSE:geometric_multiplicity_is_bounded_by_algebraic}
  The \hyperref[def:characteristic_polynomial]{geometric multiplicity} of an eigenvalue does not exceed its \hyperref[def:characteristic_polynomial]{algebraic multiplicity}.
\end{proposition}
\begin{proof}
  Suppose that the geometric multiplicity of \( \lambda \) with respect to \( L \) is \( m \). Then there exist linearly independent eigenvalues \( x_1, \ldots, x_m \) such that, for \( k = 1, \ldots, m \),
  \begin{equation*}
    L x_k = \lambda x_k.
  \end{equation*}

  \Fullref{thm:def:vector_space/expansion} allows us to expand this to a basis of \( \BbbK^n \) via some vectors \( x_{m+1}, \ldots, x_n \). With respect to this basis, the operator \( L \) has the matrix
  \begin{equation*}
    A = \begin{pmatrix}
      \lambda I_k & B \\
      0           & C
    \end{pmatrix},
  \end{equation*}
  for suitable matrices \( B \) and \( C \).

  The characteristic polynomial of this matrix is
  \begin{equation*}
    p_A(\Lambda) = (\Lambda - \lambda)^k p_C(\Lambda).
  \end{equation*}

  Therefore, the algebraic multiplicity of \( A \) is at least \( k \).
\end{proof}

\begin{definition}\label{def:point_spectrum}\mcite[10.32]{Rudin1991Functional}
  The set of all \hyperref[def:eigenpair]{eigenvalues} of a linear endomorphism is called its \term{point spectrum}.
\end{definition}

\begin{definition}\label{def:diagonalizable_matrix}\mimprovised
  A square matrix is called \term{diagonalizable} if it is \hyperref[def:similar_matrices]{similar} to a diagonal matrix.
\end{definition}

\begin{proposition}\label{thm:diagonalizable_matrix_iff_eigenvectors}
  An \( n \times n \) square matrix \( A \) is \hyperref[def:diagonalizable_matrix]{diagonalizable} via \( D = P^{-1} A P \) if and only if there exist \( n \) linearly independent \hyperref[def:eigenpair]{eigenvectors} \( e_1, \ldots, e_n \) such that
  \begin{equation*}
    P = \begin{pmatrix}
      e_1 & \cdots & e_n
    \end{pmatrix}.
  \end{equation*}
\end{proposition}
\begin{proof}
  \NecessitySubProof Let \( e_1, \ldots, e_n \) be linearly independent eigenvectors corresponding to the eigenvalues \( \lambda_1, \ldots, \lambda_n \). Let \( P \) be the matrix whole columns are these eigenvectors and let
  \begin{equation*}
    D \coloneqq \op{diag}(\lambda_1, \ldots, \lambda_n).
  \end{equation*}

  Then
  \begin{balign*}
    P D
    &=
    \parens*
      {
        \begin{array}{c}
          \vect \lambda_1^T \\ \hline \vdots \\ \hline \vect \lambda_n^T
        \end{array}
      }
    \parens*
      {
        \begin{array}{c|c|c}
          e_1 & \cdots & e_n
        \end{array}
      }
    = \\ &=
    \sum_{k=1}^n \vect \lambda_k^T e_k
    = \\ &=
    \parens*
      {
        \begin{array}{c|c|c|c}
          \lambda_1 e_1 & \vect 0 & \cdots & \vect 0
        \end{array}
      }
    +
    \parens*
      {
        \begin{array}{c|c|c|c}
          \vect 0 & \lambda_2 e_2 & \cdots & \vect 0
        \end{array}
      }
    +
    \cdots
    +
    \parens*
      {
        \begin{array}{c|c|c|c}
          \vect 0 & \vect 0 & \cdots & \lambda_n e_n
        \end{array}
      }
    = \\ &=
    \parens*
      {
        \begin{array}{c|c|c|c}
          \lambda_1 e_1 & \lambda_2 e_2 & \cdots & \lambda_n e_n
        \end{array}
      }
    = \\ &=
    \parens*
      {
        \begin{array}{c|c|c|c}
          A e_1 & A e_2 & \cdots & A e_n
        \end{array}
      }
    = \\ &=
    A P.
  \end{balign*}

  \SufficiencySubProof Similar to the necessity proof except that \( P D = A P \) is the assumption and \( (\lambda_k, e_k) \) being eigenpairs is the conclusion.
\end{proof}

\begin{definition}\label{def:adjoint_operator}\mimprovised
  We say that the linear operator \( L^*: V \to U \) between \hyperref[def:inner_product_space]{inner product spaces} is \term{adjoint} to \( L: U \to V \) if for every \( x \in U \) and \( y \in V \) we have
  \begin{equation*}
    \inprod {Lx} y = \inprod x {L^* y}.
  \end{equation*}

  If \( U = V \) and \( L = L^* \), we say that the operator is \term{self-adjoint} or \term{symmetric}. In the case of a complex inner product space, we say that \( L \) is \enquote{\term{Hermitian}} instead of \enquote{symmetric}.

  Note that this definition holds for linear operators and the concept differs from the similar concepts for bilinear forms. The inner product itself is symmetric or Hermitian by definition.
\end{definition}

\begin{proposition}\label{thm:conjugate_transpose}
  The \hyperref[def:conjugate_transpose]{conjugate transpose} \( A^* \) of the matrix \( A \) corresponds to the \hyperref[def:adjoint_operator]{adjoint operator} of \( A \) (with respect to the \hyperref[def:inner_product_space]{dot product}).
\end{proposition}
\begin{proof}
  \begin{equation*}
    \inprod {Ax} {y}
    =
    (Ax)^* y
    =
    (x^* A^*) y
    =
    x^* (A^* y)
    =
    \inprod {x} {A^* y}.
  \end{equation*}
\end{proof}

\begin{lemma}\label{thm:hermitian_operator_eigenvalues_are_real}
  The \hyperref[def:eigenpair]{eigenvalues} of a \hyperref[def:adjoint_operator]{self-adjoint linear operator} are real.
\end{lemma}
\begin{proof}
  For every eigenpair \( (\lambda, x) \) of \( L: V \to V \) we have
  \begin{equation*}
    \lambda \inprod x x
    =
    \inprod {\lambda x} x
    =
    \inprod {L x} x
    =
    \inprod x {L x}
    =
    \inprod x {\lambda x}
    =
    \overline \lambda \inprod x x.
  \end{equation*}

  Since inner products are positive definite, \( \inprod x x \) is a positive real number and hence we can cancel it to obtain the equality \( \lambda = \overline \lambda \). Hence, if \( \lambda \) is an eigenvalue, it is a real number.
\end{proof}

\begin{lemma}\label{thm:hermitian_operator_eigenvectors_are_orthogonal}
  The \hyperref[def:eigenpair]{eigenvectors} of a \hyperref[def:adjoint_operator]{self-adjoint linear operator} corresponding to different eigenvalues are \hyperref[def:orthogonality]{orthogonal}.
\end{lemma}
\begin{proof}
  Let \( (\lambda, x) \) and \( (\mu, y) \) be two eigenpairs with \( \lambda \neq \mu \) and suppose that \( \lambda \neq 0 \). Then
  \begin{equation*}
    \lambda \inprod x y
    =
    \inprod {\lambda x} y
    =
    \inprod {A x} y
    =
    \inprod x {A y}
    =
    \inprod x {\mu y}
    =
    \overline \mu \inprod x y.
  \end{equation*}

  By \fullref{thm:hermitian_operator_eigenvalues_are_real}, \( \mu \) is a real number and hence \( \overline \mu = \mu \). Therefore,
  \begin{equation*}
    \lambda \inprod x y = \mu \inprod x y.
  \end{equation*}

  Since \( \lambda \neq 0 \) and \( \lambda \neq \mu \), it remains for \( \inprod x y \) to be zero. Hence, \( x \) and \( y \) are orthogonal.
\end{proof}

\begin{proposition}\label{thm:hermitian_matrix_is_diagonalizable}
  Every \hyperref[def:conjugate_transpose]{Hermitian matrix} is \hyperref[def:diagonalizable_matrix]{diagonalizable}, and its eigenvalues are real.
\end{proposition}
\begin{proof}
  Let \( A \) be a Hermitian \( n \times n \) matrix. By \fullref{thm:fundamental_theorem_of_algebra}, the characteristic polynomial has \( n \) roots, counting multiplicities. Let \( \lambda_1, \ldots, \lambda_l \) be eigenvalues and let \( m_1, \ldots, m_l \) be their geometric multiplicities. By \fullref{thm:hermitian_operator_eigenvalues_are_real}, \( \lambda_1, \ldots, \lambda_l \) are real numbers.

  Let \( e_1, \ldots, e_{m_1} \) be an orthonormal basis of the eigenspace of \( \lambda_1 \). Such a basis exists by \fullref{thm:finite_dimensional_orthonormal_basis_existence}. Let \( e_{m_1+1}, \ldots, e_{m_1+m_2} \) be an orthonormal basis of the eigenspace of \( \lambda_2 \). By \fullref{thm:hermitian_operator_eigenvectors_are_orthogonal}, the vectors \( e_1, \ldots, e_{m_1+m_2} \) are pairwise orthogonal. Via induction, we obtain an orthonormal basis \( e_1, \ldots, e_n \) of \( \BbbC^n \).

  By \fullref{thm:diagonalizable_matrix_iff_eigenvectors}, \( A \) is diagonalizable.
\end{proof}
